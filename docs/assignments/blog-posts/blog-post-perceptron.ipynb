{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Implementing the Perceptron Algorithm\n",
        "type: Blog Post\n",
        "date: '2024-03-28'\n",
        "description: |\n",
        "  In this blog post, you'll implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n",
        "objectives:\n",
        "  - Implementation\n",
        "  - Navigation\n",
        "  - Experimentation\n",
        "number-sections: false\n",
        "number-depth: 2\n",
        "publish: 'true'\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.hidden}\n",
        "$$\n",
        "\\newcommand{\\R}{\\mathbb{R}}\n",
        "\\newcommand{\\vx}{\\mathbf{x}}\n",
        "\\newcommand{\\vy}{\\mathbf{y}}\n",
        "\\newcommand{\\mX}{\\mathbf{X}}\n",
        "\\newcommand{\\vw}{\\mathbf{w}}\n",
        "\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n",
        "\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this blog post, you will complete an implementation of the perceptron algorithm and test it in several experiments. \n",
        "\n",
        "# Part A: Implement Perceptron\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "1. Please create a new blog post. In addition to the usual `.ipynb` file, please also create a script called `perceptron.py` *in the same directory*. This is the file in which you will implement the perceptron algorithm. \n",
        "2. Then, in your `.ipynb` notebook file, place the following in a Python code block underneath your metadata block: \n",
        "\n",
        "```python\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from perceptron import Perceptron, PerceptronOptimizer\n",
        "```\n",
        "\n",
        "The two commands beginning with `%` will cause your notebook to automatically re-read the contents of your `perceptron.py` file, so that changes you make in that file will be automatically and immediately reflected when you run cells in your blog notebook. \n",
        "\n",
        "## Implement the Perceptron Algorithm\n",
        "\n",
        "Now it is time to implement the perceptron algorithm. I am going to guide you through doing this in a relatively specific way that will help us generalize when we work with more complex algorithms. **Your implementation of the perceptron algorithm should be contained in the file `perceptron.py`. It is not part of your blog post, but you will link to it later.**\n",
        "\n",
        "### `LinearModel` and `Perceptron.loss()`\n",
        "\n",
        "If you haven't already, implement the methods of the `LinearModel` class as described in [this warmup](../../warmup-exercises.ipynb#sec-perceptron). Please also implement `Perceptron.loss()`. \n",
        "\n",
        "### `PerceptronOptimizer.step()`\n",
        "\n",
        "Now implement the `step()` method of `PerceptronOptimizer`. In this method, you should assume that `Perceptron.grad()` correctly returns the \"update\" part of the perceptron update. You'll implement `Perceptron.grad()` soon. In this function, do two things: \n",
        "\n",
        "1. Call `self.model.loss()`. We'll see why when we get to different classes of models. \n",
        "2. Then, call `self.model.grad()` and add the result to `self.model.w`. [Recall that `PerceptronOptimizer`'s `__init__` method saves a `Perceptron` object to the instance variable `self.model`.]{.aside} \n",
        "\n",
        "### `Perceptron.grad()`\n",
        "\n",
        "Finally, it is time to implement `Perceptron.grad()`. This is where the math comes in -- turning the math of the perceptron algorithm into code. In this part, you may assume that the argument `X` of `Perceptron.grad()` is a single row of the feature matrix. \n",
        "\n",
        "Inside `Perceptron.grad()`, you should do two things: \n",
        "\n",
        "1. Compute the score $s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle$. \n",
        "2. Return the vector $\\mathbb{1}\\left[s_i y_{i} < 0 \\right] y_{i} \\mathbf{x}_{i}$. \n",
        "\n",
        "## Check Your Implementation \n",
        "\n",
        "Your code is *probably working* when you can run the \"minimal training loop\" code from [this section](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#a-minimal-training-loop) of the notes and eventually achieve `loss = 0` on linearly separable data. **You should do this in your `.ipynb` file as part of your blog post.** You can use the functions supplied in those notes to generate and visualize the data (when the data is 2d). \n",
        "\n",
        "## Code Quality \n",
        "\n",
        "An excellent solution will be no more than 30 lines of code in total (across all three classes) and will contain no loops. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part B: Experiments\n",
        "\n",
        "Once you have completed your implementation and checked that it appears to be working on a simple example, please perform the following experiments and construct the requested visualizations.\n",
        "\n",
        "Please perform experiments (with visualizations) that illustrate the following claims: \n",
        "\n",
        "1. Using 2d data like the data in the example above, if the data is linearly separable then the perceptron algorithm converges to weight vector $\\vw$ describing a separating line (provided that the maximum number of iterations is large enough). \n",
        "    - Please show visualizations of the data, the separating line, and the evolution of the loss function during training.\n",
        "    - You are encouraged but not required to create a plot like [this one](https://www.philchodrow.prof/ml-notes/chapters/20-perceptron.html#fig-demonstration) from the lecture notes. Note that the code that generates this figure is supplied in a popout; you are free to use it with attribution. \n",
        "2. For 2d data, when the data is **not** linearly separable, the perceptron algorithm will not settle on a final value of $\\vw$, but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n",
        "    - Please show visualizations of the data, the decision boundary in the final iteration, and the evolution of the score over training. \n",
        "    - You will need to set a maximum number of iterations in your training loop in order to ensure that the algorithm terminates. 1000 iterations is plenty. \n",
        "3. The perceptron algorithm is also able to work in more than 2 dimensions! Show an example of running your algorithm on data with at least 5 features. You don't need to visualize the data or the separating line, but you should still show the evolution of the score over the training period. Include a comment on whether you believe that the data is linearly separable based on your observation of the score. \n",
        "\n",
        "# Part C (Optional): Minibatch Perceptron\n",
        "\n",
        "*This part is optional if you are satisfied with an M on this blog post. It is required for an E.*\n",
        "\n",
        "The *mini-batch perceptron algorithm* computes an update using $k$ points at once, rather than a single point. In each step $t$: \n",
        "\n",
        "1. Pick $k$ random indices $i_1,\\ldots,i_k$. \n",
        "2. Perform the update \n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{1}{k}\\sum_{\\ell = 1}^k \\mathbb{1}\\left[\\langle \\mathbf{w}, \\mathbf{x}_{i_k} \\rangle y_{i_k} < 0 \\right] y_{i_k} \\mathbf{x}_{i_k}\n",
        "$${#eq-batch-perceptron}\n",
        "\n",
        "@eq-batch-perceptron corresponds to computing $k$ perceptron increments simultaneously and averaging them before modifying $\\mathbf{w}$. \n",
        "\n",
        "To implement mini-batch updating, modify your `perceptron.grad()` method so that it accepts a submatrix of the feature matrix $\\mathbf{X}$. You should assume that this submatrix has size $k \\times p$. It is possible to perform this implementation in two lines and with no for-loops if you are careful. You'll then need to modify your main training loop so that a random submatrix of the feature matrix is passed to `PerceptronOptimizer.step()`.\n",
        "\n",
        "**Hint**: You can get a random submatrix of the feature matrix `X` and target vector `y` like this: \n",
        "\n",
        "```python \n",
        "k = 5\n",
        "ix = torch.randperm(X.size(0))[:k]\n",
        "print(X[ix,:])\n",
        "print(y[ix])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Minibatch Perceptron Experiments \n",
        "\n",
        "Please perform experiments and create visualizations to demonstrate the following: \n",
        "\n",
        "1. When `k = 1`, minibatch perceptron performs similarly to regular perceptron. \n",
        "2. When `k = 10`, minibatch perceptron can still find a separating line in 2d. \n",
        "3. When `k = n` (that is, the batch size is the size of the entire data set), minibatch perceptron can converge *even when the data is not linearly separable*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part D: Writing \n",
        "\n",
        "In crafting your blog post, please include the following components: \n",
        "\n",
        "- **At the very top of your blog post, a link to your source code (`perceptron.py`) on your GitHub repo.**\n",
        "- A brief walk-through of your implementation of `perceptron.grad()` and an explanation of how it correctly implements the math of the perceptron update. It is not necessary to walk the user through every single aspect of your solution class. \n",
        "- Full code and English descriptions for all the experiments you perform. \n",
        "- After your experiments, please address the following question: \n",
        "\n",
        "> What is the runtime complexity of a single iteration of the perceptron algorithm? Does the runtime complexity of a single iteration depend on the number of data points $n$? What about the number of features $p$?\n",
        "> If you implemented minibatch perceptron, what is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\n",
        "\n",
        "**Finally**, please add an introductory \"abstract\" paragraph describing the high-level purpose of the blog post and a concluding summary paragraph in which you reflect on your findings. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-0451",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
