{
  "cells": [
    {
      "cell_type": "raw",
      "id": "44379c72",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Implementing Logistic Regression\"\n",
        "type: \"Blog Post\"\n",
        "date: 2024-04-04\n",
        "description: |\n",
        "    In this blog post, you'll implement several first-order methods: optimization algorithms based on the gradients of functions. You'll implement simple gradient descent, a momentum method, and stochastic gradient descent, comparing their performance for training logistic regression. \n",
        "objectives: \n",
        "  - Theory\n",
        "  - Implementation\n",
        "  - Experimentation\n",
        "publish: \"true\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edeb7b07",
      "metadata": {},
      "source": [
        "::: {.hidden}\n",
        "$$\n",
        "\\newcommand{\\R}{\\mathbb{R}}\n",
        "\\newcommand{\\vx}{\\mathbf{x}}\n",
        "\\newcommand{\\vy}{\\mathbf{y}}\n",
        "\\newcommand{\\mX}{\\mathbf{X}}\n",
        "\\newcommand{\\vw}{\\mathbf{w}}\n",
        "\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n",
        "\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n",
        "\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "*The [expectations for all blog posts](../../blog-post-expectations.qmd) apply!* \n",
        "\n",
        "## Introduction\n",
        "\n",
        "[Recently](https://www.philchodrow.prof/ml-notes/chapters/23-gradient-descent.html), we introduced the gradient descent algorithm for solving the empirical risk minimization problem. We also calculated the gradient of the loss function for logistic regression.  \n",
        "\n",
        "In this blog post you will: \n",
        "\n",
        "1. Implement gradient descent for logistic regression in an object-oriented paradigm. \n",
        "2. Implement a key variant of gradient descent with *momentum* in order to achieve faster convergence. \n",
        "3. (Optionally), implement *Newton's Method* for even faster logistic regression. \n",
        "4. Perform experiments to test your implementations. \n",
        "\n",
        "\n",
        "## Part A: Implement Logistic Regression\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "1. Please create a new blog post. In addition to the usual `.ipynb` file, please also create a script called `logistic.py` *in the same directory*. This is the file in which you will implement logistic regression. \n",
        "2. Then, in your `.ipynb` notebook file, place the following in a Python code block underneath your metadata block: \n",
        "\n",
        "```python\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from logistic import LogisticRegression, GradientDescentOptimizer\n",
        "```\n",
        "\n",
        "If you complete the optional implementation of Newton's Method, you'll also need to import `NewtonOptimizer`. \n",
        "\n",
        "### Implement `LinearModel` and `LogisticRegression()`\n",
        "\n",
        "If you haven't already, implement the methods of the `LinearModel` class as described in [this warmup](../../warmup-exercises.ipynb#sec-perceptron). Then, define a new class called `LogisticRegression` which inherits from `LinearModel`. This class should have two methods: \n",
        "\n",
        "- `LogisticRegression.loss(X, y)` should compute the empirical risk $L(\\mathbf{w})$ using the logistic loss function [As usual, $s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$ and $\\sigma(s) = \\frac{1}{1 + e^{-s}}$.]{.aside}\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "The weight vector $\\mathbf{w}$ used for this calculation should be stored as an instance variable of the class. \n",
        "- `LogisticRegression.grad(X, y)` should compute the *gradient* of the empirical risk $L(\\mathbf{w})$. You can use [the formula](https://www.philchodrow.prof/ml-notes/chapters/23-gradient-descent.html#eq-logistic-gradient) for the gradient supplied in the lecture notes on gradient descent. \n",
        "\n",
        "For an **M**, you can implement `LogisticRegression.grad` using a `for`-loop. For an **E**, your solution should involve no explicit loops. While working on a solution that avoids loops, you might find it useful to at some point convert a tensor `v` with shape `(n,)` into a tensor `v_` with shape `(n,1)`. The code `v_ = v[:, None]` will perform this conversion for you. \n",
        "\n",
        "\n",
        "### Implement `GradientDescentOptimizer.step()`\n",
        "\n",
        "Next, implement a `GradientDescentOptimizer` class. For this project, we are going to implement *gradient descent with momentum*, also known as Spicy Gradient Descent. Let $\\mathbf{w}_k$ be the estimate of the weight vector at algorithmic step $k$. Gradient descent with momentum performs the update [This description of gradient descent with momentum is based on @hardtPatternsPredictionsActions2022.]{.aside}\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1}) \n",
        "\\end{aligned}\n",
        "$${#eq-gradient-momentum}\n",
        "\n",
        "Here, $\\alpha$ and $\\beta$ are *two* learning rate parameters. When $\\beta = 0$ we have \"regular\" gradient descent. In practice, a choice of $\\beta \\approx 0.9$ or so is common. Implementing the momentum term isn't too complex -- you'll just need to create a new instance variable to hold the *previous* value of $\\mathbf{w}$ as well as the current one. \n",
        "\n",
        "## Part B: Experiments\n",
        "\n",
        "### Experimental Data\n",
        "\n",
        "Here is some code to generate data for a classification problem. You can control the number of points by adjusting `n_points`, the number of features by adjusting `p_dims`, and the difficulty of the classification problem by adjusting the `noise` (higher noise is a harder problem). \n",
        "\n",
        "```python\n",
        "def classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n",
        "    \n",
        "    y = torch.arange(n_points) >= int(n_points/2)\n",
        "    y = 1.0*y\n",
        "    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n",
        "    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X, y = classification_data(noise = 0.5)\n",
        "```\n",
        "\n",
        "### How to Train Your Model\n",
        "\n",
        "Once you've correctly implemented logistic regression and gradient descent, you can do a gradient descent loop like this: \n",
        "\n",
        "```python\n",
        "LR = LogisticRegression() \n",
        "opt = GradientDescentOptimizer(LR)\n",
        "\n",
        "for _ in range(100):\n",
        "    # add other stuff to e.g. keep track of the loss over time. \n",
        "    opt.step(X, y, alpha = 0.1, beta = 0.9)\n",
        "```\n",
        "\n",
        "### Experiments\n",
        "\n",
        "Please perform experiments, with careful written explanations, that demonstrate the following statements: \n",
        "\n",
        "1. **Vanilla gradient descent**: When $p_dim = 2$, when $\\alpha$ is sufficiently small and $\\beta = 0$, gradient descent for logistic regression converges to a weight vector $\\mathbf{w}$ that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations). \n",
        "    - This is a good experiment to use to assess whether your implementation in Part A has bugs. \n",
        "2. **Benefits of momentum**: On the same data, gradient descent with momentum (e.g. $\\beta = 0.9$) *can* converge to the correct weight vector in fewer iterations than vanilla gradient descent (with $\\beta = 0$). Plot the loss over iterations for each method. You may need to experiment with the data and choice of $\\alpha$ in order to observe speedups due to momentum. \n",
        "3. **Overfitting**: Generate some data where `p_dim > n_points`. For example, `p_dim = 100` and `n_points = 50`. Do this **twice** with the exact same parameters. Call the first dataset  `X_train, y_train` and the second dataset `X_test, y_test`. Then, do an experiment in which you fit a logistic regression model to the data `X_train, y_train` and obtain 100% accuracy on this training data. What is the accuracy on the test data?  \n",
        "\n",
        "## Part C: Writing\n",
        "\n",
        "Please: \n",
        "\n",
        "1. Please include informative comments throughout your source code **and a thorough docstring** for each method in `LogisticRegression` and `GradientDescentOptimizer`. \n",
        "2. Please add careful expository writing throughout your blog post. You should describe each experiment and what it is intended to illustrate. You should also ensure that all your plots are legible and have appropriate axis labels and legends. \n",
        "3. At the beginning of your blog post, please place a link to your source code `logistic.py` on GitHub. After this link, please write an abstract paragraph describing the topic of your post and giving a brief overview of the experiments you performed. \n",
        "4. At the conclusion of your blog post, please write a discussion paragraph reminding the reader of what you did and what you learned while doing it. \n",
        "\n",
        "## Tips and Hints  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Most of the major math functions are shown in our [lecture on gradient descent](../../lecture-notes/gradient-descent.qmd). You're welcome to use any of these functions as you wish; please just incorporate comments in your code and blog post to cite where they came from.  \n",
        "\n",
        "If you compute the gradient using matrix-vector operations in `numpy` (recommended, no for-loops!), you may find it useful at some point to convert an `np.array()` of shape `(n,)` to an `np.array()` of shape `(n,1)` like this: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89db9d8",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "v = np.random.rand(5)\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c49dd5ed",
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "v[:,np.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52f47ed",
      "metadata": {},
      "source": [
        "You will find it convenient again to ensure that `X` contains a column of `1`s prior to any major computations. I defined this function: \n",
        "\n",
        "```python\n",
        "def pad(X):\n",
        "    return np.append(X, np.ones((X.shape[0], 1)), 1)\n",
        "```\n",
        "\n",
        "and called it at a few strategic places in my implementation. \n",
        " \n",
        "My complete implementation, including all the math functions, momentum, etc. but excluding comments, was about 100 lines of code. \n",
        "\n",
        "You're welcome to find creative ways to visualize your findings. You might also find it interesting to visualize the score (not just the loss). However, this is optional. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:ml-0451] *",
      "language": "python",
      "name": "conda-env-ml-0451-py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
