[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Readings in normal font should be completed and annotated ahead of lecture.\nReadings in italic provide optional additional depth on the material.\n\nAssignments are listed on the day when I suggest you begin working on them.\nReading sources:\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "schedule.html#week-2",
    "href": "schedule.html#week-2",
    "title": "Schedule",
    "section": "Week 2",
    "text": "Week 2\n\n\n\n\n\n\n\n\n\n\n\nTue\nFeb. 20\nLinear Score-Based Classification\n\n\n\nWe study a fundamental method for binary classification in which data points are assigned scores. Scores above a certain threshold are assigned to one class; scores below are assigned to another.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\nReading\nLinear Classifiers from MITx.\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nGraphing Decision Boundaries\nAssignments\nACTUAL REAL DUE DATE: Reflective Goal-Setting due 2/27\n\n\nThu\nFeb. 22\nStatistical Decision Theory and Automated Decision-Making\n\n\n\nWe discuss the theory of making automated decisions based on a score function. We go into detail on thresholding, error rates, and cost-based optimization.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\nReading\nPDSH: Introduction to Numpy\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nChoosing a Threshold\nAssignments\nBlog Post: Whose Costs?"
  },
  {
    "objectID": "schedule.html#week-3",
    "href": "schedule.html#week-3",
    "title": "Schedule",
    "section": "Week 3",
    "text": "Week 3\n\n\n\n\n\n\n\n\n\n\n\nTue\nFeb. 27\nAuditing Fairness\n\n\n\nWe introduce the topics of fairness and disparity in automated decision systems using a famous case study.\n\n\n\n\nLearning Objectives\nSocial Responsibility\nExperimentation\n\nReading\nBHN: Introduction\nMachine Bias by Julia Angwin et al. for ProPublica.\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nExperiencing (Un)Fairness\nAssignments\nReflective Goal-Setting due today\n\n\nThu\nFeb. 29\nStatistical Definitions of Fairness in Automated Decision-Making\n\n\n\nWe offer formal mathematical definitions of several natural intuitions of fairness, review how to assess them empirically on data in Python, and prove that two major definitions are incompatible with each other.\n\n\n\n\nLearning Objectives\nSocial Responsibility\nTheory\n\nReading\nBHN: Classification (ok to skip \"Relationships between criteria\" and below)\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nReading Check\nAssignments\nBlog Post: Bias Replication Study\nand/or\nBlog Post: Women in Data Science Conference"
  },
  {
    "objectID": "schedule.html#week-4",
    "href": "schedule.html#week-4",
    "title": "Schedule",
    "section": "Week 4",
    "text": "Week 4\n\n\n\n\n\n\n\n\n\n\n\nTue\nMar. 05\nNormative Theory of Fairness\n\n\n\nWe discuss some of the broad philosophical and political positions that underly the theory of fairness, and connect these positions to statistical definitions.\n\n\n\n\nLearning Objectives\nSocial Responsibility\n\nReading\nBHN: Relative Notions of Fairness\n\nNotes\nDiscussion guide shared in Slack\n\nWarmup\nCOMPAS and Equality of Opportunity\n\n\n\nThu\nMar. 07\nCritical Perspectives: Interrogate Your Task\n\n\n\nWe discuss several critical views that seek to move our attention beyond the fairness of algorithms and towards their role in sociotechnical systems. We center two questions: who benefits from a given data science task? What tasks could we approach instead if our aims were to uplift the oppressed?\n\n\n\n\nLearning Objectives\nSocial Responsibility\n\nReading\nData Feminism: The Power Chapter by Catherine D'Ignazio and Lauren Klein\n\"The Digital Poorhouse\" by Virginia Eubanks\n\"Studying Up: Reorienting the study of algorithmic fairness around issues of power\" by Barabas et al.\n\nNotes\nDiscussion guide shared in Slack\n\nWarmup\nPower, Data, and Studying Up\nAssignments\nBlog Post: Limitations of the Quantitative Approach"
  },
  {
    "objectID": "schedule.html#week-5",
    "href": "schedule.html#week-5",
    "title": "Schedule",
    "section": "Week 5",
    "text": "Week 5\n\n\n\n\n\n\n\n\n\n\n\nTue\nMar. 12\nCritical Perspectives: Interrogate Your Data\n\n\n\nWe discuss the importance of understanding the context of data when planning and executing data science, and effectively communicating this context when sharing our findings.\n\n\n\n\nLearning Objectives\nSocial Responsibility\n\nReading\nData Feminism: The Numbers Don't Speak For Themselves by Catherine D'Ignazio and Lauren Klein\nDatasheets for Datasets by Timnit Gebru et al.\n\nNotes\nDiscussion guide shared in Slack\n\nWarmup\nData Context and Data Sheets\n\n\n\nThu\nMar. 14\nIntroduction to Model Training: The Perceptron\n\n\n\nWe study the perceptron as an example of a linear model with a training algorithm. Our understanding of this algorithm and its shortcomings will form the foundation of our future explorations in empirical risk minimization.\n\n\n\n\nLearning Objectives\nTheory\n\nReading\nNo reading today, but please be ready to put some extra time into the warmup. It may be useful to review our lecture notes on score-based classification and decision theory when completing the warmup.\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nLinear Models, Perceptron, and Torch\nAssignments\nBlog Post: Implementing Perceptron"
  },
  {
    "objectID": "schedule.html#week-6",
    "href": "schedule.html#week-6",
    "title": "Schedule",
    "section": "Week 6",
    "text": "Week 6\n\n\n\n\n\n\n\n\n\n\n\nTue\nMar. 19\nSpring Break!\n\n\n\n\n\n\n\n\n\n\n\nWarmup\nTBD\n\n\n\nThu\nMar. 21\nSpring Break!\n\n\n\n\n\n\n\n\n\n\n\nWarmup\nTBD"
  },
  {
    "objectID": "schedule.html#week-7",
    "href": "schedule.html#week-7",
    "title": "Schedule",
    "section": "Week 7",
    "text": "Week 7\n\n\n\n\n\n\n\n\n\n\n\nTue\nMar. 26\nConvex Empirical Risk Minimization\n\n\n\nWe introduce the framework of convex empirical risk minimization, which offers a principled approach to overcoming the many limitations of the perceptron algorithm.\n\n\n\n\nLearning Objectives\nTheory\n\nReading\nConvexity Examples by Stephen D. Boyles, pages 1 - 7 (ok to stop when we start talking about gradients and Hessians).\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nPractice with Convex Functions\nAssignments\nACTUAL REAL DUE DATE: Mid-semester reflection due 04/02\n\n\nThu\nMar. 28\nGradient Descent\n\n\n\nWe study a method for finding the minima of convex functions using techniques from calculus and linear algebra.\n\n\n\n\nLearning Objectives\nTheory\n\nReading\nNo reading today, but please budget some extra time for the warmup.\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nA First Look at Gradient Descent\nAssignments\nBlog Post: Implementing Logistic Regression"
  },
  {
    "objectID": "schedule.html#week-8",
    "href": "schedule.html#week-8",
    "title": "Schedule",
    "section": "Week 8",
    "text": "Week 8\n\n\n\n\n\n\n\n\n\n\n\nTue\nApr. 02\nFeature Maps and Regularization\n\n\n\nWe re-introduce feature maps as a method for learning nonlinear decision boundaries, and add regularization to the empirical risk minimization problem in order to control the complexity of our learned models.\n\n\n\n\nLearning Objectives\nTheory\nExperimentation\n\nReading\nNo reading today -- please think hard about your project pitches!\n\nNotes\nLecture notes\nLive notes\n\nWarmup\nProject Pitches\nAssignments\nMid-semester reflection due today ,ACTUAL REAL DUE DATE: Project proposal due 4/9\n\n\nThu\nApr. 04\nLinear Regression\n\n\n\nWe introduce linear regression through the framework of convex empirical risk minimization.\n\n\n\n\nLearning Objectives\nTheory\n\nReading\nNo additional reading, but you may need to open up your linear algebra textbook in order to complete the warmup.\n\nNotes\nLinear Regression\nLive notes\n\nWarmup\nEigenvalues and Linear Systems"
  },
  {
    "objectID": "schedule.html#week-9",
    "href": "schedule.html#week-9",
    "title": "Schedule",
    "section": "Week 9",
    "text": "Week 9\n\n\n\n\n\n\n\n\n\n\n\nTue\nApr. 09\nKernel Methods\n\n\n\nWe introduce kernel methods as an alternative approach to the problem of fitting nonlinear models to data.\n\n\n\n\nLearning Objectives\nTheory\n\nReading\nClassification and K-Nearest Neighbours by Hiroshi Shimodaira for a course at the University of Edinburgh\n\nNotes\nTBD\nTBD\n\nWarmup\nIntroducing Kernels"
  },
  {
    "objectID": "schedule.html#finals-period",
    "href": "schedule.html#finals-period",
    "title": "Schedule",
    "section": "Finals Period",
    "text": "Finals Period\nDuring the reading and final exam period, you’ll meet with me 1-1 for about 15 minutes. The purpose of this meeting is to help us both reflect on your time in the course and agree on a final grade."
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Asking for Help",
    "section": "",
    "text": "Asking for help is a fundamental part of how you will learn in CSCI 0451. Do it often. Here is some wisdom on this topic from the Best Cat On the Internet:\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "help.html#dont-get-stuck",
    "href": "help.html#dont-get-stuck",
    "title": "Asking for Help",
    "section": "Don’t Get Stuck",
    "text": "Don’t Get Stuck\nWe want to productively challenge you, which is different from letting you get stuck. If you’ve spent more than 30 minutes without making any progress or change in your understanding, then that’s likely a sign that you should consult a new reading or other resource, ask for help from a classmate, a Course Assistant, or me."
  },
  {
    "objectID": "warmup-exercises.html",
    "href": "warmup-exercises.html",
    "title": "Warmup Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\vz}{\\mathbf{z}}\n\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "warmup-exercises.html#sec-score-based-classification",
    "href": "warmup-exercises.html#sec-score-based-classification",
    "title": "Warmup Exercises",
    "section": "Graphing Decision Boundaries",
    "text": "Graphing Decision Boundaries\n\nPart A\nSketch the line in \\(\\R^2\\) described by the equation \\[\n\\bracket{\\vw, \\vx}  =  b\\;,\n\\tag{1}\\]\nwhere \\(\\vw = \\paren{1, -\\frac{1}{2}}^T \\in \\R^2\\) and \\(b = \\frac{1}{2}\\). Here, \\(\\bracket{\\vw, \\vx} = \\sum_{i = 1}^p w_i x_i\\) is the inner product (or dot product) between the vectors \\(\\vw,\\vx \\in \\R^p\\).\n\n\nPart B\nYou can create numpy arrays like this: x = np.array([0.2, 4.3]). The syntax x@w can also be used as a convenient shorthand for np.dot(x, w).\nWrite a quick Python function called linear_classify(x, w, b). w and x should both be 1d numpy arrays of the same length, and b should be a scalar. The function np.dot(x, w) will compute the inner product of x and w. Argument b should be a scalar number. Your function should return 0 if if \\(\\bracket{\\vw, \\vx}  &lt;  b\\) and 1 if \\(\\bracket{\\vw, \\vx}  \\geq  b\\).\nVerify that your function works on a few simple examples.\n\n\nPart C\nThe function \\(\\phi\\) is an example of a feature map, which we will discuss soon.\nLet \\(\\phi:\\mathbb{R}^2\\rightarrow \\mathbb{R}^5\\) be the function\n\\[\n\\phi(\\mathbf{x}) = \\phi(x_1, x_2) = (x_1, x_2, x_1^2, x_2^2, x_1x_2)^T\\;.\n\\]\nMake a sketch of the curve in \\(\\R^2_+\\) (the nonnegative quadrant) defined by the equation\n\\[\n\\bracket{\\vw, \\phi(\\vx)} = b\\;,\n\\]\nwhere \\(\\vw = (0, 0, 1, \\frac{1}{4}, 0)^T\\) and \\(b = 1\\)."
  },
  {
    "objectID": "warmup-exercises.html#sec-penguins",
    "href": "warmup-exercises.html#sec-penguins",
    "title": "Warmup Exercises",
    "section": "Meet The Palmer Penguins!",
    "text": "Meet The Palmer Penguins!\n\n\n\nImage source: @allisonhorst. The Palmer Penguins data was originally collected by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020).\n\n\nOur data set for class today is the Palmer Penguins. This data set contains physiological measurements and species labels for several populations of Adelie, Chinstrap, and Gentoo penguins.\nOpen a fresh Jupyter notebook using the ml-0451 Anaconda Python kernel. In a code cell, paste and run the following code in order to acquire the data set.\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/palmer-penguins.csv\"\ndf = pd.read_csv(url)\n\nPart A\nUse a pandas summary table (df.groupby(...).aggregate(...)) to answer the following question: how does the mean mass of penguins vary by species and sex?\n\n\n\n\n\nImage source: @allisonhorst.\n\n\n\n\nPart B\nMake a scatterplot of culmen length against culmen depth, with the color of each point corresponding to the penguin species."
  },
  {
    "objectID": "warmup-exercises.html#sec-decision-theory",
    "href": "warmup-exercises.html#sec-decision-theory",
    "title": "Warmup Exercises",
    "section": "Choosing a Threshold",
    "text": "Choosing a Threshold\n\nPart A\nYou wake up feeling a bit of a tickle in your throat. You take a COVID rapid test. Consider two scenarios:\n\nThe test comes up positive and you quarantine for five days. As it turns out, your throat tickle disappears after 1 day, and a lab test reveals that you did not actually have COVID.\nThe test comes up negative. You figure you must just have a common cold. You carry on your day as usual. After 5 days, you decide to get a lab test, which reveals that you have been COVID+ for the last 5 days.\n\nIf it feels very difficult to come up with a single number to summarize the “cost” of each of these outcomes, that’s normal! In practice, designers of automated decision systems may need to assign comparable “costs” to injury, life, access to education, etc.\nUsing any assumptions that seem appropriate to you, assign a numerical cost to each of these scenarios. If it helps, you may assume that scenario (a) is 1 “unit of badness,” and that scenario (b) is \\(k\\) times as bad as scenario (a). What’s your suggested value of \\(k\\)?\nPlease write down your reasoning and your suggested value in a short paragraph.\n\n\nPart B\nLet’s now imagine that the rapid COVID test does not just give a yes/no answer, but actually a score describing the patient’s likelihood of COVID on a scale from 0 to 1. What score is high enough to merit you staying home, according to your costs from Part A? To answer this question, run the following code to create a simulated vector of cases (0 is COVID negative, 1 is COVID positive) and scores between 0 and 1.\nimport numpy as np\n\nNUM_CASES  = 1000\nPREVALENCE = 0.1\nNOISE      = 2\n\ncases  = 1*(np.random.rand(NUM_CASES) &lt; PREVALENCE)\nscores = np.exp(cases + NOISE*(np.random.rand(NUM_CASES))) / np.exp(NOISE+1)\nSuppose that the recommendation on the rapid test is that a score above \\(t\\) indicates a “positive” result and that you should quarantine.\n\nWrite a function which, given a candidate value of \\(t\\), computes the total “cost” of using that threshold. The cost is equal to the number of times scenario (a) occurs in this data set (from Part A), multiplied by the cost of scenario (a), plus the number of times scenario (b) occurs in this data set, multiplied by the cost of scenario (b).\nUsing a for-loop or any other technique, conduct a search to find the value of \\(t\\) that minimizes the total cost."
  },
  {
    "objectID": "warmup-exercises.html#sec-fairness",
    "href": "warmup-exercises.html#sec-fairness",
    "title": "Warmup Exercises",
    "section": "Experiencing (Un)Fairness",
    "text": "Experiencing (Un)Fairness\nIn completing this warmup, please keep in mind that you will be asked to share with your group and potentially with the class.\nTake 30 minutes to write two paragraphs.\nIn your first paragraph, please respond to the following prompt:\n\n\nYou can choose to discuss a decision that either unfairly harmed or unfairly benefited you.\n\nWhen was a time in your life in which you felt that you were the subject of an unfair decision? What was it about that decision that made it feel unfair, rather than just bad, disappointing, or surprising?\n\nIn your second paragraph, please respond to the following prompt:\n\nConsider Figure 4 in the Introduction of BHN. In this figure, there are blue dots and green dots, where the colors correspond to hypothetical demographic attributes. We as the reader can choose what the colors mean.\n\nSuggest one possible meaning for the blue and green dots in which you would say that the classifier depicted in the figure is unproblematic from the perspective of fairness.\nSuggest one possible meaning for the blue and green dots in which you would say that the classifier depicted in the figure is concerning from the perspective of fairness.\nWhat is the relevant difference between the two cases?\n\n\nPlease bring your paragraphs to class and be ready to share with your group."
  },
  {
    "objectID": "warmup-exercises.html#sec-bhn-comprehension",
    "href": "warmup-exercises.html#sec-bhn-comprehension",
    "title": "Warmup Exercises",
    "section": "Reading Check: BHN",
    "text": "Reading Check: BHN\nFor each of the following soundbites, please write two sentences describing how Barocas, Hardt, and Narayanan (2023) would respond. Please also include the number of the page on which you are basing your sentences in this PDF version of the book. You might find useful discussion of the point on multiple pages; it’s sufficient to list one.\nYou may need to check both the reading for today and the reading from the previous lecture.\n\n“Since the COMPAS algorithm didn’t use race as a predictor variable during training, it can’t be racially biased.”\n“For every decision-making task, it is possible to ethically deploy an appropriately trained and audited automated decision model.”\n“If two groups \\(a\\) and \\(b\\) are on average equally deserving of access to an opportunity, then the only requirement of fairness is that a decision-making system accepts members of group \\(a\\) at the same rate as members of group \\(b\\).”\n“Decision-making algorithms should have equal error rates between different groups.”\n“The data doesn’t lie, so the only fair approach to machine learning is to replicate patterns found in the data as accurately as possible.”"
  },
  {
    "objectID": "warmup-exercises.html#sec-perceptron",
    "href": "warmup-exercises.html#sec-perceptron",
    "title": "Warmup Exercises",
    "section": "Linear Models, Perceptron, and Torch",
    "text": "Linear Models, Perceptron, and Torch\nIn this warmup, you will introduce yourself to the model template code that we’ll use to implement several different machine learning models in this course. As you do so, you’ll use the Torch package (instead of Numpy) for numerical calculations.\nHere’s a function which generates the kind of data we are going to use with the perceptron, including a feature matrix and a set of predictor labels. Instead of np.arrays, these are now torch.Tensors. You can still think of them as arrays of numbers with most of the same operations. Most things that you have learned about np.arrays will still work for torch.Tensors, and in this warmup I will point out all the relevant differences.\n\nimport torch\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\nHere’s how it looks:\n\n\n\n\n\n\n\n\n\nTo define a classifier for this data, we are going to define three Python classes.\n\nThe LinearModel class is a template for several linear models that we will implement in this class. Recall that a linear model is a model that works by computing a score for each data point like \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\). This class has a single instance variable: the weight vector \\(\\mathbf{w}\\).\nThe Perceptron class inherits from LinearModel and describes the specific linear model we will use in lecture today.\nThe PerceptronOptimizer class will implement the specific learning algorithm that will improve the value of \\(\\mathbf{w}\\) in order to optimize an objective.\n\nHere are the three classes. I have written docstrings for the methods that we’ll implement in this warmup.\n\nimport torch\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        \"\"\"\n        Compute the scores for each data point in the feature matrix X. \n        The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;. \n\n        If self.w currently has value None, then it is necessary to first initialize self.w to a random value. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            s torch.Tensor: vector of scores. s.size() = (n,)\n        \"\"\"\n        if self.w is None: \n            self.w = torch.rand((X.size()[1]))\n\n        # your computation here: compute the vector of scores s\n        pass \n\n    def predict(self, X):\n        \"\"\"\n        Compute the predictions for each data point in the feature matrix X. The prediction for the ith data point is either 0 or 1. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            y_hat, torch.Tensor: vector predictions in {0.0, 1.0}. y_hat.size() = (n,)\n        \"\"\"\n        pass \n\nclass Perceptron(LinearModel):\n\n    def loss(self, X, y):\n        \"\"\"\n        Compute the misclassification rate. A point i is classified correctly if it holds that s_i*y_i_ &gt; 0, where y_i_ is the *modified label* that has values in {-1, 1} (rather than {0, 1}). \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n            y, torch.Tensor: the target vector.  y.size() = (n,). The possible labels for y are {0, 1}\n        \n        HINT: In order to use the math formulas in the lecture, you are going to need to construct a modified set of targets and predictions that have entries in {-1, 1} -- otherwise none of the formulas will work right! An easy to to make this conversion is: \n        \n        y_ = 2*y - 1\n        \"\"\"\n\n        # replace with your implementation\n        pass\n\n    def grad(self, X, y):\n        pass \n\nclass PerceptronOptimizer:\n\n    def __init__(self, model):\n        self.model = model \n    \n    def step(self, X, y):\n        \"\"\"\n        Compute one step of the perceptron update using the feature matrix X \n        and target vector y. \n        \"\"\"\n        pass\n\n\nPart A\nOpen a fresh Jupyter notebook with the ml-0451 kernel. Paste in the code that generates the data, as well as the three class definitions above.\nPlease implement LinearModel.score() and LinearModel.predict() according to their supplied docstrings.\nAn ideal solution will use no for-loops. It is possible to complete LinearModel.score() with one additional line of code and LinearModel.predict() with two lines of code.\n\n\nPart B\nPlease implement Perceptron.loss() according to its supplied docstring. It is possible to complete this function with two lines of code.\nHints\n\nTwo numbers \\(a\\) and \\(b\\) have the same sign iff \\(ab &gt; 0\\).\nIn torch, you can’t compute a.mean() if a is a boolean tensor. Instead, you need to cast a to a tensor of numerical values. One way to do this is by computing (1.0*a).mean().\n\n\n\nCheck\nOnce you have completed Parts A and B, the below code should run and the value of l should be 0.5. Paste it into a new cell in your notebook and run it to check.\np = Perceptron()\ns = p.score(X)\nl = p.loss(X, y)\nprint(l == 0.5)\nThere are several other methods in these classes that we will need to implement in order to have a functioning classification algorithm, but we won’t worry about those until later."
  },
  {
    "objectID": "warmup-exercises.html#sec-views-on-compas",
    "href": "warmup-exercises.html#sec-views-on-compas",
    "title": "Warmup Exercises",
    "section": "COMPAS and Equality of Opportunity",
    "text": "COMPAS and Equality of Opportunity\nRecently, we replicated ProPublica’s investigative reporting on the COMPAS algorithm and studied several competing statistical definitions of fairness.\nImagine that this article came out two weeks ago. What should we do about the situation reported by ProPublica? Please address this question from three distinct perspectives:\n\nThe perspective of someone who adheres to the narrow view of equality of opportunity.\nThe perspective of someone who adheres to the middle view of equality of opportunity.\nThe perspective of someone who adheres to the broad view of equality of opportunity.\n\n Write at least three sentences from each of these perspectives. Your response should, at minimum, address the following two questions:You may find it useful to remember from the readings that calibration, as discussed in BHN Ch. 4 is related to sufficiency, which is discussed in BHN Ch. 3 and also in our notes on statistical conceptions of fairness.\n\nIs the [narrow/middle/broad] view of equality of opportunity in fact violated, according to our previous data analysis?\nIf so, what change is obligated?\nWhose obligation is it to enact that change? Whose obligation is it to incur the costs of that change?\n\nPlease ground your sentences in our quantitative findings and our recent readings on statistical and normative notions of fairness. If you think the answer isn’t cut-and-dried, please do your best analysis and make a note about what’s complicated and why."
  },
  {
    "objectID": "warmup-exercises.html#sec-studying-up",
    "href": "warmup-exercises.html#sec-studying-up",
    "title": "Warmup Exercises",
    "section": "Power, Data, and Studying Up",
    "text": "Power, Data, and Studying Up\nFor each of the hypothetical scenarios below, please:\n\nBriefly analyze the scenario in terms of the four domains of the matrix of domination. Which of these domains are operating in the scenario provided? There are probably multiple, but not necessarily all four.\nSuggest a way in which data scientists could “study up” (Barabas et al. 2020) by collecting and analyzing data about the powerful or privileged actors in the scenario.\n\nA concise paragraph for each scenario is plenty.\n\nScenarios\n\nLandlords use an algorithm to predict “payment reliability” of prospective tenants in apartment complexes. The algorithm gives lower reliability scores to prospective tenants moving from predominantly Latinx/Hispanic neighborhoods than it does to prospective tenants moving from predominantly white neighborhoods.\nPolice use predictive policing algorithms that encourage them to allocate officers to neighborhoods that have historically had high arrest rates. A local politician tweets that these neighborhoods are “blighted.”\nMedical professionals use a health score to determine which patients should receive palliative care. The score includes an assessment of pain levels as one of its inputs. Doctors are known to systematically underestimate the pain levels of Black patients in comparison to white ones."
  },
  {
    "objectID": "warmup-exercises.html#sec-data-sheets",
    "href": "warmup-exercises.html#sec-data-sheets",
    "title": "Warmup Exercises",
    "section": "Data Context and Data Sheets",
    "text": "Data Context and Data Sheets\nThis warmup is based on Ch. 6 of D’Ignazio and Klein (2023) and Gebru et al. (2021).\nLet’s go back to our running example of the COMPAS data set. As you may remember, the data that feeds into a COMPAS risk score is collected by a screener using forms like this one. Such risk assessments are administered to people who have been arrested and charged with criminal activity.\n\nPart A\nUsing the writeup from ProPublica and any additional sources you are able to find, please respond to the following questions from Gebru et al. (2021) in order to construct a (very partial) datasheet for the COMPAS data set.\nYou may be able to answer many of these questions off the top of your head. For others, the answers might be a little blurry. For example, who created the COMPAS data set? Northpointe? The criminal penal system? ProPublica? In other cases, you might not know the answer – it’s fine to mark that and move on. Short responses are fine.\n\nMotivation\n\nFor what purpose was the dataset created?\nWho created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?\n\n\n\nComposition\n\nWhat do the instances (rows) that comprise the dataset represent?\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\n\nNote: this is another one that might take some thought.\n\nDoes the dataset identify any subpopulations (for example, by age, gender)?\nIs it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset?\nDoes the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\n\n\n\nCollection Process\n\nHow was the data associated with each instance acquired?\nWho was involved in the data collection process?\nDid the individuals in question consent to the collection and use of their data?\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\n\n\n\nUses\n\nHas the data set been used for any tasks already?\nWhat (other) tasks could the data set be used for?\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/ cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?\nAre there tasks for which the dataset should not be used?\n\n\n\n\nPart B\nD’Ignazio and Klein (2023) emphasize the importance of the context of the data when deciding what data science to do and how to interpret it. Consider the following hypothetical, out-of-context statement:\n\nThe COMPAS data set describes the the social background, thought patterns, criminal histories, and reoffenses for all criminal perpetrators in Broward County during the time period.\n\nIn roughly one paragraph, analyze this statement against what you know about the context of the data, including your datasheet and our previous discussions. What context is missing from this statement? Which aspects of this statement are incorrect? Why does it matter?"
  },
  {
    "objectID": "warmup-exercises.html#sec-convexity",
    "href": "warmup-exercises.html#sec-convexity",
    "title": "Warmup Exercises",
    "section": "Practice with Convex Functions",
    "text": "Practice with Convex Functions\n\nPart A\nPlease write a careful mathematical proof of the following statement. You may cite without proving any of the statements from the reading up to page 7. As suggested in the reading, Prop 2. (the second derivative test) is likely to be especially useful.\nClaim: Let \\(f:\\mathbb{R}\\rightarrow\\mathbb{R}\\) be any twice-differentiable and convex function. Then, the function \\(g(x) = f(ax + b)\\) is also convex for any constants \\(a\\) and \\(b\\).\n\n\nPart B\nThe logistic sigmoid \\(\\sigma\\) is the function \\(\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}\\) given by the formula \\[\n\\begin{aligned}\n    \\sigma(z) = \\frac{1}{1 + e^{-z}}\\;.\n\\end{aligned}\n\\]\n\nB.1\nUse matplotlib with either numpy or torch to make a graph of this function on the interval \\(z \\in [-1, 1]\\).\n\n\nB.2\nCompute \\(\\frac{d\\sigma(s_0)}{ds}\\). \\(\\frac{d\\sigma(s_0)}{ds}\\) is the first derivative of \\(\\sigma\\) evaluated at the point \\(s_0\\). You may need to recall things like the chain and quotient rules.\n\n\nB.3\nUsing your computation of \\(\\frac{d\\sigma(s_0)}{ds}\\) from the previous part, check that the following formula holds. You should do so by computing both sides of the equation and verifying that they are equal.\n\\[\n\\begin{aligned}\n    \\frac{d\\sigma(s_0)}{ds} = \\sigma(s_0)\\left(1 - \\sigma(s_0)\\right)\\;.\n\\end{aligned}\n\\]\n\n\nB.4\nUsing any methods from the reading, write a careful proof that the following two functions are convex:\n\n\\(f(s) = - \\log \\sigma(s)\\)\n\\(g(s) = - \\log (1 - \\sigma(s))\\)\n\n\n\nB.5\nLet \\(y \\in \\{0,1\\}\\). Explain why the function\n\\[\n\\ell(s, y) = - y \\log \\sigma(s) - (1  - y) \\log(1-\\sigma(s))\n\\]\nis convex as a function of \\(s\\)."
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent",
    "href": "warmup-exercises.html#sec-gradient-descent",
    "title": "Warmup Exercises",
    "section": "A First Look at Gradient Descent",
    "text": "A First Look at Gradient Descent\nLet \\(\\mathbb{R}^+\\) be the set of strictly positive real numbers. Let \\(a \\in \\mathbb{R}^+\\). Consider the function \\(g_a:\\mathbb{R}^+ \\rightarrow \\mathbb{R}\\) with formula Here and elsewhere in this class, \\(\\log\\) always refers to the natural log (base \\(e\\)), which you may have also seen written \\(\\ln\\).\n\\[\ng_a(x) = \\frac{1}{2}x^2 - a \\log x\\;.\n\\]\n\nPart A\nA critical point of \\(g_a\\) is a point \\(x_0\\) where \\(\\frac{dg_a(x_0)}{dx} = 0\\). Find the critical point of this function in the set \\(\\mathbb{R}^+\\). We’ll call this point \\(x_a\\).\n\n\nPart B\nUse the second derivative test to show that \\(g_a\\) is strictly convex on the set \\(\\mathbb{R}^+\\). It follows that the critical point \\(x_a\\) is a global minimum of \\(g_a\\).\n\n\nPart C\nImplement the following algorithm as a Python function:\nInputs: \\(a \\in \\mathbb{R}^+\\), tolerance \\(\\epsilon &gt; 0\\), learning rate \\(\\alpha\\), \\(\\mathrm{maxsteps} = 100\\).\n\nStart with an initial guess \\(x = a\\).\nSet \\(x' \\gets 2a\\) and \\(j \\gets 0\\).\nWhile \\(|x' - x| &gt; \\epsilon\\):\n\nif \\(j &gt; \\mathrm{maxsteps}\\)\n\nBreak\n\n\\(x \\gets x'\\)\n\\(x' \\gets x - \\alpha \\frac{dg_a(x)}{dx}\\);.\n\\(j \\gets j + 1\\).\n\nReturn \\(x\\).\n\nYou should use the formula for \\(\\frac{dg_a(x)}{dx}\\) that you found in Part A.\nTest your function like this:\nmystery_fun(a = 9, epsilon = 1e-8, alpha = 0.2)\nPlease show:\n\nOne setting of \\(\\alpha\\) for which your function returns a real number very close to the exact value of \\(x_a\\).\nOne setting of \\(\\alpha\\) for which your function fails to return a real number close to the exact value of \\(x_a\\) within the maximum number of steps.\n\n\n\nPart D\nIs it possible to compute the positive square root of a positive real number using only the operations of addition, subtraction, multiplication, and division?"
  },
  {
    "objectID": "warmup-exercises.html#sec-project-pitch",
    "href": "warmup-exercises.html#sec-project-pitch",
    "title": "Warmup Exercises",
    "section": "Project Pitches",
    "text": "Project Pitches\nPlease pitch a project idea in the #project-pitches channel on Slack. Your project idea will not necessarily be the final project you work on — indeed, many of you will work on someone else’s idea. That’s ok! I am still expecting everyone to make a pitch.\nFirst, read the course description of the project. Then, think about what you’d like to do!\nSecond, write your pitch! To write your pitch:\n\nWrite one or two sentences about the big picture: what problem you’d like to address and how machine learning fits in.\nIf your project needs data (almost all will), state your data plan. There are three ways:\n\nDescribe a data set to which you currently have access.\nLink to an online data set that is suitable for addressing your problem.\nDescribe a specific approach by which you will collect your own data.\n\nState what kind of problem your pitch involves. Is it a classification problem, a regression problem (predicting a quantitative outcome rather than a qualitative label), or an unsupervised problem like clustering?\nDescribe how you’ll judge whether your project is successful. What are you looking to have produced/achieved by the end of the semester?\nClose your pitch by letting us know: why are you excited about this topic? Pitches are due as your warmup assignment on Tuesday 4/2."
  },
  {
    "objectID": "warmup-exercises.html#sec-linear-systems-review",
    "href": "warmup-exercises.html#sec-linear-systems-review",
    "title": "Warmup Exercises",
    "section": "Eigenvalues and Linear Systems",
    "text": "Eigenvalues and Linear Systems\nConsider the system of linear equations expressed in matrix-vector notation as\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\tag{2}\\]\nfor some square matrix \\(\\mathbf{A} \\in \\mathbb{R}^{p \\times p}\\) and vector \\(\\mathbf{b} \\in \\mathbb{R}^p\\).\n\nPart A\nSuppose you are told that Equation 2 has a unique solution in \\(\\mathbf{x}\\) for every possible choice of \\(\\mathbf{b}\\in \\mathbb{R}^p\\).\nIs \\(\\mathbf{A}\\) invertible? How do you know? Find a statement of a theorem from linear algebra that supports your claim. It’s ok to look for the theorem online.\n\n\nPart B\nLet \\(\\alpha_1,\\ldots, \\alpha_p\\) be the eigenvalues of \\(\\mathbf{A}\\). Suppose that \\(\\alpha_j \\geq 0\\) for all \\(j\\), but that you don’t know anything else about \\(\\mathbf{A}\\).\n\nB.1\nCan you be sure that \\(\\mathbf{A}\\) invertible? Explain how you know (find a theorem and cite it), or construct a counterexample.\n\n\nB.2\nLet \\(\\lambda &gt; 0\\). Let \\(\\tilde{\\mathbf{A}} = \\mathbf{A} + \\lambda \\mathbf{I}\\). What are the eigenvalues of \\(\\tilde{\\mathbf{A}}\\)?\n\n\nB.3\nIs \\(\\tilde{\\mathbf{A}}\\) invertible? Explain how you know (find a theorem and cite it), or give a counterexample.\n\n\nB.4\nDoes the linear system \\(\\tilde{\\mathbf{A}}\\mathbf{x} = \\mathbf{b}\\) have zero, one, or infinitely many solutions? Does the value of \\(\\mathbf{b}\\) matter? Explain how you know (find a theorem and cite it)."
  },
  {
    "objectID": "warmup-exercises.html#sec-kernels",
    "href": "warmup-exercises.html#sec-kernels",
    "title": "Warmup Exercises",
    "section": "Introducing Kernels",
    "text": "Introducing Kernels\nSuppose we have a set of 1-dimensional labeled data. Unfold the code and copy it if you want to use it to generate your training data.\n\n\nCode\nimport torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\nn_points = 100\nx = torch.rand(n_points)\ny = 1*((x + 0.3*(torch.rand(n_points) - 0.5)) &gt; 0.5 )\n\ndef plot_1d_classification_data(x, y, ax):\n    \n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(x[ix], torch.zeros_like(x[ix]), s = 40,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.6, marker = markers[i], )\n    ax.set(xlabel = r\"$x$\")\n    \nfig, ax = plt.subplots(figsize = (10, 1))\nplot_1d_classification_data(x, y, ax)\n\n\n\n\n\n\n\n\n\nTo learn in this data set, we are going to try a very different kind of classifier than we’ve used before. This is a kernel classifier.\nFor the purposes of this warmup, our kernel classifier computes a score for a data point with feature \\(x\\) according to the formula\n\\[\ns = \\sum_{i = 1}^n ye^{-\\gamma (x - x_i )^2}\\;,\n\\tag{3}\\]\nwhere \\(\\gamma\\) is some tunable positive constant. Roughly, the intuition of this score function is that you get a higher score by being “close” (having low distance) to lots of points where \\(y = 1\\).\nYour warmup problem is to implement this score function. The code block below produces a range of possible values of \\(x\\):\n\nx_space = torch.linspace(0, 1, 1001)\n\nPlease write code to produce a vector \\(\\mathbf{s}\\) where the \\(i\\)th element of \\(s\\) is the score computed according to Equation 3. It is possible to do this as a one-liner of under 80 characters if you are sneaky about it.\nHint: you can compute a matrix of differences between every element of x_space and every element of the data x using the code below:\ndiffs = x_space[:, None] - x\nOnce you have computed the vector s, try running the code below to visualize the fit of your score to the training data. For \\(\\gamma = 100\\), I got something like this:\n\nfig, ax = plt.subplots(2, 1, figsize = (5, 4), height_ratios= (0.8, 0.2))\nax[0].plot(x_space, s, color = \"slategrey\")\nax[0].set(ylabel = \"Kernel score\")\nplot_1d_classification_data(x, y, ax[1])\n\n\n\n\n\n\n\n\nProduce versions of this plot for \\(\\gamma = 1\\), \\(\\gamma = 100\\), and \\(\\gamma = 10000\\). What do you observe?"
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent-2",
    "href": "warmup-exercises.html#sec-gradient-descent-2",
    "title": "Warmup Exercises",
    "section": "Gradient Descent (Again)",
    "text": "Gradient Descent (Again)\nConsider the function \\(f(w_0, w_1) = \\sin(w_0w_1)\\). You can define this function like this:\n\nimport numpy as np\ndef f(w):\n    return np.sin(w[0]*w[1])\n\nMathematically, the gradient of this function is\n\\[\\nabla f(w_0, w_1) = (w_1\\cos w_0w_1, w_0 \\cos w_0w_1)^T.\\]\n\nImplement a simple loop that uses gradient descent to find a minimum of this function.\n\nYou’ll have to choose the learning rate \\(\\alpha\\).\nThe np.cos() function will be useful for programming the gradient.\nIt’s not the fastest approach, but if you’re not show how to program the gradient you can always first implement it as a list of two floats, and then use np.array(my_list) to convert it into a numpy array.\nYou’ll also need to pick a random starting guess.\n\nFind two initial guesses for the parameter vector \\(\\vw\\) such that you get two different final minimizers (this is possible because \\(f\\) is not convex)."
  },
  {
    "objectID": "warmup-exercises.html#sec-overfitting",
    "href": "warmup-exercises.html#sec-overfitting",
    "title": "Warmup Exercises",
    "section": "Overfitting and the Scientific Method",
    "text": "Overfitting and the Scientific Method\n Image from Wikipedia.\nIn the scientific method, it is often emphasized that we need to formulate a hypothesis before performing an experiment. It’s fine for the hypothesis to be based on previous experiments. However, the scientific method never allows us to perform an experiment, formulate a hypothesis, and then say that the experiment supported the (new) hypothesis.\nWe can think of scientific theories as systems of thought that help us make predictions about new phenomena. With this in mind, please write a short paragraph explaining the importance of hypothesis-first science using the language of machine learning. In your explanation, please use the following vocabulary:\n\nTraining data.\nTraining accuracy.\nValidation/testing data.\nValidation/testing accuracy.\nOverfitting."
  },
  {
    "objectID": "warmup-exercises.html#sec-erm",
    "href": "warmup-exercises.html#sec-erm",
    "title": "Warmup Exercises",
    "section": "The Coin-Flipping Game",
    "text": "The Coin-Flipping Game\nLet’s play a game! Here is the setup:\nI have a coin with probability of heads equal to \\(p \\in [0,1]\\). I am going to ask you to pick a number \\(\\hat{p} \\in [0,1]\\). Then, I flip my coin.\nThis game is more fun for me than it is for you.\n\nIf my coin comes up heads, you give me \\(-\\log \\hat{p}\\) dollars.\nIf my coin comes up tails, you give me \\(-\\log (1-\\hat{p})\\) dollars.\n\n\nPart 1\nCompute the expected amount of money you will give me when we play this game in terms of \\(p\\) and \\(\\hat{p}\\). Call this quantity \\(R(\\hat{p}, p)\\). This is the risk of the guess \\(\\hat{p}\\).\n\n\nPart 2\nTake the derivative and set it equal to 0! Don’t forget to check that you’ve found a minimum of \\(R(\\hat{p}, p)\\) rather than a maximum or an inflection point.\nSuppose I tell you the value of \\(p\\). Write a mathematical proof to show that your best choice of \\(\\hat{p}\\) (the one that loses you the least money) is \\(\\hat{p} = p\\).\n\n\nPart 3\nNow suppose that I don’t tell you the true value of \\(p\\). Instead, I let you observe \\(n\\) coin flips before asking you to make your guess. Describe:\n\nA suggestion for choosing \\(\\hat{p}\\) based only on the results of the previous flips.\nA way to estimate the risk (expected amount of money lost) based only on the results of the previous flips.\n\nYour answer should depend on \\(\\hat{p}\\) but not on \\(p\\)!"
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates-2",
    "href": "warmup-exercises.html#sec-classification-rates-2",
    "title": "Warmup Exercises",
    "section": "Balancing Classification Rates",
    "text": "Balancing Classification Rates\nYou can do this first part just by copying and pasting lecture code. It doesn’t matter much how good your model is – just make sure you’re able to get predictions.\nUse the code from our recent lecture to download the Titanic data set as a Pandas data frame and train a model on the training data. Then download the test data. Compute y_pred, the vector of predictions of your model on the test data.\nThen, write a function that verifies eq. (2.6) in Alexandra Chouldechova’s paper “Fair Prediction with disparate impact.” Here’s what your function should do:\nThe positive predictive value is \\(\\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\).\n\nGiven vectors y_pred of predictions and y_test of actual labels, compute the False Negative Rate (FNR), False Positive Rate (FPR), prevalence \\(p\\), and positive predictive value (PPV).\nReturn as a tuple the lefthand side and righthand side of eq. (2.6) in Chouldechova.\nVerify that the two numbers are equal!"
  },
  {
    "objectID": "warmup-exercises.html#sec-limits-quantitative",
    "href": "warmup-exercises.html#sec-limits-quantitative",
    "title": "Warmup Exercises",
    "section": "Limits of The Quantitative Approach to Discrimination",
    "text": "Limits of The Quantitative Approach to Discrimination\nI’ll give you each a number in Slack. The numbers correspond to the following sections of Narayanan (2022). These are:\n\nThe null hypothesis allocates the burden of proof (p. 7-8)\nCompounding inequality is far below the radar of quantitative methods (p. 9-10)\nSnapshot datasets hide discrimination (p. 10-11)\nExplaining away discrimination (p. 12-13)\nWhat counts as evidence is a subjective choice (p. 5-7)\n\nFor your assigned section, please write a short paragraph (4-5 simple sentences is fine). You should:\n\nSummarize Narayanan’s key points in that section.\nIn one of the sentences, describe which aspects of the Uber case study (p. 13-16) reflect the ideas of the section you described.\n\nBring your paragraph in class and be ready to read it to your group."
  },
  {
    "objectID": "warmup-exercises.html#sec-vectorization",
    "href": "warmup-exercises.html#sec-vectorization",
    "title": "Warmup Exercises",
    "section": "Vectorization Brainstorm",
    "text": "Vectorization Brainstorm\nIn a recent lecture, we discussed methods of vectorizing text like the document-term matrix that use the bag of words assumption: the order of the words doesn’t matter!\nTake some time and propose an alternative approach to word-based text vectorization. Can you find a scheme that would give different vector representations to the following two sentences?\n\n“I love rabbits, not cats.” “I love cats, not rabbits.”\n\nYou don’t have to implement your vectorization, but please be prepared to write pseudocode for your group to show in detail how you would perform the vectorization."
  },
  {
    "objectID": "warmup-exercises.html#sec-compression",
    "href": "warmup-exercises.html#sec-compression",
    "title": "Warmup Exercises",
    "section": "Image Compression Factor of K-Means",
    "text": "Image Compression Factor of K-Means\nIn today’s reading on K-means clustering from the Python Data Science Handbook, Jake VanderPlas considers the use of K-means to reduce the number of distinct colors in an image (Example 2). I encourage you to run the code for this example while thinking about this warmup!\nGive an estimate of the compression factor: the reduction of information achieved when compressing an image using k-means clustering into \\(k\\) color clusters. The compression factor is the number of bits required to store the compressed image, divided by the number of bits required to store the original image. Both of these numbers can be computed asymptotically (i.e. with big-oh reasoning) in order to simplify the analysis.\nThere are multiple good ways to think about this question, and you’re welcome to choose one that makes sense to you as long as you carefully state your steps and assumptions. Here are a few points that I find helpful:\n\nBits in Original Image\n\nAn image with \\(n\\) rows and \\(m\\) columns has \\(nm\\) pixels.\nEach pixel has one of three RGB color channels (Red, Green, and Blue).\nEach color channel can be represented with 8 bits (which encode an integer between 0 and 255, denoting the color intensity in that channel).\n\n\n\nBits in Compressed Image\n\nIf I compress an image into just \\(k\\) distinct colors, then instead of storing the full RGB value for each pixel, I can just store enough bits to uniquely identify the cluster containing each pixel. How many bits do I need for this?\nI also need to store a dictionary (hash map) that associates color \\(j\\) (i.e. the centroid of the \\(j\\)th cluster of colors) to its RGB value.\n\n\n\nOptional Extra\nTry running the code above while varying the number of clusters. Do you think that a 16-color compression looks much better than an 8-color compression. Do you think the difference is good enough to justify approximately twice the storage? What about 32 colors vs. 16?"
  },
  {
    "objectID": "warmup-exercises.html#sec-intro-tensors",
    "href": "warmup-exercises.html#sec-intro-tensors",
    "title": "Warmup Exercises",
    "section": "Introducing Tensors",
    "text": "Introducing Tensors\nFirst, install PyTorch 2.0 into your ml-0451 Anaconda environment.\n\n\nThe best way to install PyTorch is is probably to run the following at the command line:\nconda activate ml-0451\npip3 install torch torchvision torchaudio\nThen, in a blank Jupyter notebook, copy, paste, and run each of the code blocks in the first section of the PyTorch tutorial.\nYou may need to do a little bit of exploring around the tutorials in order to come up with answers to these questions.\nFinally, write down a single-sentence answer to each of the following questions:\n\nIn what ways is a PyTorch tensor similar to a Numpy array?\nIn what ways is a PyTorch tensor different from a Numpy array?\nWhat is the primary motivation for the use of a specialized tensor data type, rather than an array, for deep learning?"
  },
  {
    "objectID": "warmup-exercises.html#sec-backprop",
    "href": "warmup-exercises.html#sec-backprop",
    "title": "Warmup Exercises",
    "section": "Efficient Differentiation",
    "text": "Efficient Differentiation\nThis exercise is based on a section of Chinmay Hegde’s notes on stochastic gradient descent and neural networks:\nConsider the following function: \\[\nL(w, b) = \\frac{1}{2} \\left(y - \\sigma(wx + b)\\right)^2 + \\frac{1}{2}\\lambda w^2\\;,\n\\]\nwhere \\(\\sigma(a) = \\frac{1}{1 + e^{-a}}\\).\nThis is the loss function that would be obtained when using a single feature \\(x\\) to predict \\(y\\), using the function \\(\\sigma(wx + b)\\) the predictor and measuring the quality of this predictor using the square-error loss function. Our aim is to compute the gradient of \\(L\\) with respect to \\(w\\) and \\(b\\), with a “ridge” regularization term \\(\\frac{1}{2}\\lambda w^2\\) that encourages the weight \\(w\\) to be small.\nI’ve used the property of the sigmoid that \\(\\sigma'(a) = \\sigma(a)(1-\\sigma(a))\\).\nThe gradient of \\(L\\) is \\(\\nabla L (w, b) = \\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}\\right)\\), where\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))x + \\lambda w \\\\\n\\frac{\\partial L}{\\partial b} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))\\;.\n\\end{aligned}\n\\tag{4}\\]\n\nWhat You Should Do\nAssume that each of the following operations cost one computational unit:\n\nMultiplying or dividing two scalar numbers.\nAdding or subtracting two scalar numbers.\nComputing an exponential like \\(e^a\\).\n\nUsing this assumption:\n\nDetermine the number of computational units (i.e. computational cost) of computing the gradient of \\(L\\) exactly as written in Equation 4, under the assumption that you are not allowed to store the values of any intermediate computations.\nNow determine the computational cost of computing the gradient of \\(L\\) under the assumption that you are allowed to store intermediate computations. Please describe both the number of computations and the number of floating point numbers that must be stored.\nFinally, determine the computational cost in terms of both steps and storage to compute \\(L\\) using the backpropagation algorithm (described for a very similar function in Hegde’s notes).\n\nCompare your results from each method."
  },
  {
    "objectID": "warmup-exercises.html#sec-convolutional-kernel",
    "href": "warmup-exercises.html#sec-convolutional-kernel",
    "title": "Warmup Exercises",
    "section": "Convolutional Kernels",
    "text": "Convolutional Kernels\nImplement kernel convolution. Your implementation should accept a 2d array X (think of X as representing a greyscale image) and a square convolutional kernel K. Your implementation should operate using pure numpy. You can use any zero-padding strategy, but you do need to explain what your strategy is when presenting.\nIt’s ok to use a for-loop to loop over pixels.\nHere’s an example image you can use:\n\nfrom PIL import Image\nimport urllib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nplt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nAfter implementing your function, you should be able to use it like this, replacing the implementation of scipy.signal with your own implementation. The result should look something like this:\n\nfrom scipy.signal import convolve2d\n\nkernel = np.array([[-1, -1, -1], \n                   [-1,  8, -1], \n                   [-1, -1, -1]])\n\nconvd = convolve2d(img, kernel)\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 8)\nplt.gca().axis(\"off\")"
  },
  {
    "objectID": "warmup-exercises.html#sec-project-check-in",
    "href": "warmup-exercises.html#sec-project-check-in",
    "title": "Warmup Exercises",
    "section": "Project Check-In",
    "text": "Project Check-In\nThis is a warmup activity that we will repeat weekly until the end of the semester.\nPrepare a 2-3 minute “presentation” of your project to your group. Your presentation can be informal and does not need to have any special visual aids. The primary expectation is that you are able to demonstrate some relevant functionality to your peers. If your project involves coding or data analysis, your relevant functionality might be as simple as accessing or preparing the data. You should plan to demonstrate additional functionality each week.\nIn other words, you should show your group something that works, regardless of how “big” it is.\nIf you are doing a project that does not involve implementation (such as a research essay), then you are still expected to offer an update. Your contributions could include describing the sources you’ve found or showing your group an outline of the argument that you will make.\nIt’s appropriate for each member of your project group to give the same presentation during warmup. Please note that you may not be in the same warmup group as your project partners. This means that:\n\nThe code you show needs to run on your laptop or in your compute instance (e.g. Google Colab).\nYou need to be ready to explain what is being shown, even if your project partners did much of the work."
  },
  {
    "objectID": "warmup-exercises.html#sec-transfer-learning",
    "href": "warmup-exercises.html#sec-transfer-learning",
    "title": "Warmup Exercises",
    "section": "What Needs To Be Learned?",
    "text": "What Needs To Be Learned?\nSuppose that you wanted to teach an individual to recognize English-language phishing emails. Write down a few features (based on the linked website, your own experience, or other sources) that you think would help someone classify an email as “phishing attempt or not” based on the text of the email.\nNow, imagine that you are going to sit down with your tutee to teach them how to recognize English-language phishing emails. Where would you start your instruction if…\n\nYour tutee was another member of your warmup group.\nYour tutee was a fluent English speaker but had never used email.\nYour tutee was a regular email user but spoke no English.\nYour tutee spoke no English and had never seen a computer.\n\nWhich of these four scenarios would require the most “learning effort?” Which would require the least?"
  },
  {
    "objectID": "warmup-exercises.html#sec-word-embedding",
    "href": "warmup-exercises.html#sec-word-embedding",
    "title": "Warmup Exercises",
    "section": "Word Embedding",
    "text": "Word Embedding\nTake out a sheet of paper and a pencil. Your goal is to place the following words on the sheet of paper in such a way that their location on the sheet is indicative of their relationships to each other. You can decide exactly how to do this. Should words with similar meanings be in the same part of the page? Should pairs of words with similar relationships have similar distances? Your approach is up to you, but please write it down along with your placements. Your words are:\n\nWoman\nStudent\nNurse\nDoctor\nMan\nProfessor\nModel\nComputer\nMachine\nProgrammer"
  },
  {
    "objectID": "warmup-exercises.html#sec-realistic-text",
    "href": "warmup-exercises.html#sec-realistic-text",
    "title": "Warmup Exercises",
    "section": "Realistic Text?",
    "text": "Realistic Text?\nIn our reading on The Unreasonable Effectiveness of Recurrent Neural Networks, there are a few examples of model output that is realistic but not real. Pick one of the examples, and write down as carefully as you are able what makes the generated text realistic. Then, describe what “tells” would tip off an attentive observer that the text isn’t real (generated intentionally by a human) after all.\nThe Shakespeare and Wikipedia examples might be the easiest ones to think about, but feel free to look at the \\(\\LaTeX\\) or Linux source code examples if you prefer."
  },
  {
    "objectID": "warmup-exercises.html#sec-mind-map",
    "href": "warmup-exercises.html#sec-mind-map",
    "title": "Warmup Exercises",
    "section": "Mind Map",
    "text": "Mind Map\nWow, we’ve covered a lot of ground in this class! Use a graphics program or a pen/paper to make a mind map describing some of our main theoretical and concepts. As a reminder, a mind-map is a graph in which the nodes are concepts and edges join related concepts. Please incorporate the following concepts as nodes:\n\nLoss function\nTarget\nPredictor\nModel\nRegression\nClassification\nEmpirical risk minimization\nGradient descent\nFeature map\nVectorization\nOverfitting\nTraining data\nValidation/testing data\nPerceptron\nLogistic regression\nNeural networks\nLinear regression\n\nAdditionally, please incorporate at least three other concepts of your own choosing.\n\nFlowcharts in Quarto\nSince mind-maps can be a little complicated to organize, you might find it easiest to work with some software. One optional possibility is actually included with Quarto: the Mermaid chart tool can render attractive diagrams that include labeled nodes and directed edges. Using a flowchart is probably the way. For example, inserting the following code into a special {mermaid} code block will produce the following diagram:\nflowchart TB\n    A(First concept) --&gt; B(Second concept)\n    B --is part&lt;br&gt; of--&gt; A\n    B--&gt;C(Third concept)\n    A--&gt;C\n    A & B & C --&gt;D(Fourth concept)\n\n\n\n\n\nflowchart TB\n    A(First concept) --&gt; B(Second concept)\n    B --is part&lt;br&gt; of--&gt; A\n    B--&gt;C(Third concept)\n    A--&gt;C\n    A & B & C --&gt;D(Fourth concept)\n\n\n\n\n\n\nFor the basics of using Mermaid with Quarto, see the Quarto docs. A benefit of this approach is that you don’t have to worry too much about positioning, and you can publish your mind map easily on your blog! On the other hand, this approach doesn’t give you much flexibility and makes it harder to be creative about incorporating complex relationships. Doing your mind map by hand or in any other software is entirely fine."
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates",
    "href": "warmup-exercises.html#sec-classification-rates",
    "title": "Warmup Exercises",
    "section": "Classification Rates",
    "text": "Classification Rates\n\nPart 1\nCOVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%.  On the other hand, the probability of a rapid test giving a positive result for an individual who truly does not have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. These numbers are mostly made-up.Example 2.3.1 of Murphy, page 46, has a good review of the relevant probability and the definition of each of the rates below.\nWrite a Python function called rate_summary that prints the following output, filling in the correct values for each of the specified rates:\ns = 0.8           # test sensitivity\nf = 0.02          # probability of positive test if no COVID\nprevalence = 0.05 # fraction of population infected\n\nrate_summary(s, f, current_infection)\nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\n\nPart 2\n\nSuppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? \nWhat if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?\n\nYou don’t necessarily need to use your function from the previous part in this part.\n\nPart 3\nIt’s all well and good to do the math, but what about when we actually have data? Write a function called rate_summary_2 that accepts two columns of a pandas.DataFrame (or equivalently two one-dimensional numpy.arrays of equal length). Call these y and y_pred. Assume that both y and y_pred are binary arrays (i.e. arrays of 0s and 1s). y represents the true outcome, whereas y_pred represents the prediction from an algorithm or test. Here’s an example of the kind of data we are thinking about:\n\nimport pandas as pd\n\nurl = \"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv\"\ndf = pd.read_csv(url)\n\ndf.head() # just for visualizing the first few rows\n\n\n\n\n\n\n\n\n\ny\ny_pred\n\n\n\n\n0\n0\n0\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n3\n0\n1\n\n\n4\n0\n0\n\n\n\n\n\n\n\n\nYou should be able to use your function like this:\n# y is the true label, y_pred is the prediction\nrate_summary_2(df[\"y\"], df[\"y_pred\"]) \nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\nHints\nAn excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following:\ndf[[\"y\"]] == df[[\"y_pred\"]]\ndf[[\"y\"]].sum(), df[[\"y\"]].sum()"
  },
  {
    "objectID": "slides/welcome.html#section",
    "href": "slides/welcome.html#section",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is the theory and practice of algorithmically learning patterns in data."
  },
  {
    "objectID": "slides/welcome.html#section-1",
    "href": "slides/welcome.html#section-1",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…automated consumer recommendations for content and shopping."
  },
  {
    "objectID": "slides/welcome.html#section-2",
    "href": "slides/welcome.html#section-2",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…generating realistic synthetic text, images, and code.\n\n\n\n\n\nAsk chatGPT to condemn itself in the tone of Shakespeare and it looks hilarious. pic.twitter.com/T785FbGmUX\n\n— Deqing Fu (@DeqingFu) December 5, 2022"
  },
  {
    "objectID": "slides/welcome.html#section-3",
    "href": "slides/welcome.html#section-3",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…predictions and recommendations for life-changing decisions: housing, healthcare, criminal justice."
  },
  {
    "objectID": "slides/welcome.html#section-4",
    "href": "slides/welcome.html#section-4",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…search engines, smart homes, computer vision, speech-to-text, scientific discovery, driver assistance systems…"
  },
  {
    "objectID": "slides/welcome.html#section-5",
    "href": "slides/welcome.html#section-5",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Can you list the times in which you interacted with a machine learning system yesterday?"
  },
  {
    "objectID": "slides/welcome.html#section-6",
    "href": "slides/welcome.html#section-6",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Big Messages\n \nThis class is about something that is already impacting your life, and is likely to do so more in the future.\nWe are going to grow in math, coding, technical writing, and critical awareness.\nThis class works by giving you opportunties to push yourself.\nThis class is fun and rewarding but not easy."
  },
  {
    "objectID": "slides/welcome.html#section-7",
    "href": "slides/welcome.html#section-7",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What are we going to learn in this class?"
  },
  {
    "objectID": "slides/welcome.html#section-8",
    "href": "slides/welcome.html#section-8",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "CSCI 0451 is….\nCoding\n\nNumerical array programming\nObject-oriented interfaces\nExperiments and visualization\n\nMath\n\nLinear algebra\nOptimization (\\(\\implies\\) calculus)\nA bit of probability\n\nReading, writing, discussion\n\nTechnical methods\nBias, fairness, and impact of ML"
  },
  {
    "objectID": "slides/welcome.html#section-9",
    "href": "slides/welcome.html#section-9",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 1957\n \n\n\n\n\n\n\n\nWhat We Are Actually Talking About\n\n\n\n\n\n\n\n\n\n\\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1}(y_i \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle &lt; 0)y_i \\mathbf{x}_i\\]"
  },
  {
    "objectID": "slides/welcome.html#section-10",
    "href": "slides/welcome.html#section-10",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 2022\n \n\n\n\n\nWhat We Are Actually Talking About\n\n\nxkcd"
  },
  {
    "objectID": "slides/welcome.html#section-11",
    "href": "slides/welcome.html#section-11",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "My Approach\nI want you to learn stuff in this class that is hard to learn from the internet.\n\n\nLR = LogisticRegression()\nLR.fit(predictors, target)\nLR.predict(new_predictors)\n\nWe are going to learn this workflow in a day, then do more interesting things.\n\n\n\n\nSpecial Focus: Disparity, Fairness, and Impact\nAutomated decision systems have a history reproducing structural privilege and oppression, especially in relation to race, gender, class, and sexuality.\nWhat does it mean for an automated decision system to be fair? This is a hard question which we will discuss from multiple perspectives."
  },
  {
    "objectID": "slides/welcome.html#section-12",
    "href": "slides/welcome.html#section-12",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Rough, tentative plan for the semester\n\n\n\n\nFundamentals of Prediction (~2 weeks)\n\nData science workflow.\nScore-based prediction, linear models, decision theory.\n\n\n\n\nFairness in Machine Learning (~2-3 weeks)\n\nLegitimacy of automated decision-systems\nFormal definitions of bias and fairness.\nLimitations of formal methods.\n\n\n\n\nAlgorithms (math) (~4 weeks)\n\nEmpirical risk minimization, convexity, optimization.\nControlling features: regularization and kernels.\n\n\n\n\nDeep Learning (~2 weeks)\n\nImage classification, text classification, word embedding.\nWe are not doing generative language models – take 457."
  },
  {
    "objectID": "slides/welcome.html#section-13",
    "href": "slides/welcome.html#section-13",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Ok…so what are we going to do?"
  },
  {
    "objectID": "slides/welcome.html#section-14",
    "href": "slides/welcome.html#section-14",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Most Days\n\n\n \nWarmup Activity\n\nComplete ahead of time.\nReinforces content from readings and connects them to lecture.\nPresent in groups of 5-6.\nRandom presenter presents to the group.\n\nLecture\n\nMath\nLive-coding + experiments\nYour questions and ideas!"
  },
  {
    "objectID": "slides/welcome.html#section-15",
    "href": "slides/welcome.html#section-15",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Activities and assignments\n\n\n\n\nBlog Posts\n\nSubstantial projects! Usually require &gt;5 hours.\nInvolves implementation, experiments, and discussion.\nPublished on your blog.\n\n\n\n\nDaily Warmup Activities\n\nRelatively quick when you’ve done the readings.\nOne (random) person each day will present to your team.\nConnects readings to lecture.\n\n\n\n\nProject\n\nIn groups of your choosing.\nWork on it throughout the semester, presentations in last week.\nWe’ll have activities etc. to help you pick a path."
  },
  {
    "objectID": "slides/welcome.html#section-16",
    "href": "slides/welcome.html#section-16",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Blog Posts\n\nPerform experiments in Jupyter notebooks.\nCreate figures, add expository prose, etc.\n(Sometimes) Implement algorithms in source (.py) files.\nRender your notebooks into a blog using the Quarto publishing engine.\nHost source code and rendered blog on GitHub."
  },
  {
    "objectID": "slides/welcome.html#section-18",
    "href": "slides/welcome.html#section-18",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Blog Post Feedback\n\nE: You have demonstrated excellent and thorough learning in this blog post. You should definitely move on.\nM: You have demonstrated learning in this blog post, but may have missed some opportunities. You could learn either by moving on or by revising this post.\nR: You have demonstrated some learning, but have missed some important ideas or techniques. I recommend that you focus your learning on revising this assignment.\n\nThese “grades” are always accompanied by written feedback on where to revise if applicable."
  },
  {
    "objectID": "slides/welcome.html#section-19",
    "href": "slides/welcome.html#section-19",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Readings and Warmups\nDo them!\nReadings should be completed ahead of time.\nNotes are for in-class.\nLet’s practice a warmup activity"
  },
  {
    "objectID": "slides/welcome.html#section-20",
    "href": "slides/welcome.html#section-20",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Your Affinity Vegetable\n \n1. Split into teams\n2. Go around and share your name and:\nIf you were a vegetable, which vegetable would you be and why?"
  },
  {
    "objectID": "slides/welcome.html#section-21",
    "href": "slides/welcome.html#section-21",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Your Affinity Vegetable\n \n3. Team leader: lead your team in finding a delicious dish that incorporates all of your vegetables.\nBe ready to share!"
  },
  {
    "objectID": "slides/welcome.html#section-22",
    "href": "slides/welcome.html#section-22",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading"
  },
  {
    "objectID": "slides/welcome.html#section-23",
    "href": "slides/welcome.html#section-23",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n\n\nInitialization:\n\n\nYou set goals for your learning and achievement (in week 2).\n\n\nMain Loop:\n\n\nYou attend class, participate in activities, and complete assignments.\n\nYou get feedback on your assignments from me and the TAs, and you revise.\n\n\nAt End Of Course:\n\n\nYou propose a letter grade that reflects your learning and achievement, and discuss it with me.\n\n\nThe feedback you get on individual assignments is data for your proposal. There is no formula."
  },
  {
    "objectID": "slides/welcome.html#section-24",
    "href": "slides/welcome.html#section-24",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n \n\n\n\n\n\n\n\n\n\nOpportunity\nChallenge\n\n\n\n\nNo points, no averages\nYou can focus on feedback and set your own goals.\nYou need to motivate based on your interest in the class\n\n\nResubmit assignments\nOne of the best ways to learn\nNeed to read feedback and prioritize time for revisions\n\n\nCan skip assignments\nNo busy work – work on what’s valuable to you.\nStill need to work enough to learn and meet your goals\n\n\nNo hard due-dates\nDon’t ask for extensions, take the time you need\nNeed to keep yourself on pace to achieve your goals"
  },
  {
    "objectID": "slides/welcome.html#section-25",
    "href": "slides/welcome.html#section-25",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What a Grade Sounds Like…\nA: I am ready to take the theory, techniques, and ideas of this course into my endeavours outside this classroom: future classes, projects, hobbies, career.\nB: With help or review, I might be able to take some of what I learned outside this classroom.\nC: I showed up and did stuff, but I don’t really see any ways to take what I learned outside this classroom.\nD-F: I didn’t really show up or do much.\n\n\n\nI am very likely to accept your proposed grade in the course if you EITHER:\n\nComplete most assignments to a high standard (including revisions) OR\nWork for ~10 productive hours per week outside of class OR\nDo some of the assignments I give you and also some other things (that you propose) that are relevant to the course learning goals."
  },
  {
    "objectID": "slides/welcome.html#section-26",
    "href": "slides/welcome.html#section-26",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What is something that makes you feel excited or empowered about collaborative grading?\nWhat is something that makes you feel nervous or confused about collaborative grading?"
  },
  {
    "objectID": "slides/welcome.html#section-27",
    "href": "slides/welcome.html#section-27",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "The Wisdom Of Those Who Have Gone Before\nStay on top of the blog posts and do the daily warmups. also go to office hours if you are confused, Phil is helpful and there will likely be CS0451 peers there to talk through assignments with.\nReview after each class using lecture notes so that you have a solid understanding of the concepts taught in class.\nget to know quarto blogs and watch threeblueonebrown essence of linear algebra on Youtube to review some ideas\nBe realistic in your goal setting and focus on what you want to get out of the course.\nFocus on learning and growing instead of the grade. Be curious and think hard."
  },
  {
    "objectID": "slides/welcome.html#section-28",
    "href": "slides/welcome.html#section-28",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Based on what you know about the course so far, what are some ways that success might look like for you?"
  },
  {
    "objectID": "assignments/process/end-of-course.html",
    "href": "assignments/process/end-of-course.html",
    "title": "End-Of-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice (I recommend either VSCode or JupyterLab).\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Canvas.\nIn the The Data section, replace the blanks with brief responses.\nIn the What you Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, move on to propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451.\nSubmit the notebook as a PDF on Canvas.\nMake an appointment with me (use the link posted on Canvas) in which we will discuss your learning in the course and your final grade.\n\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf end-of-course.ipynb\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/process/end-of-course.html#the-data",
    "href": "assignments/process/end-of-course.html#the-data",
    "title": "End-Of-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often did you attend class,” good answers include “almost always,” “I missed three times,” “about 75% of the time,” “not as often as I wanted,” etc.\n\nPresence in Class\n\nHow often did you attend class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often did you take notes on the core readings ahead of the class period? ____\nHow often were you prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times did you actually present the daily warm-up to your team? ____\nHow many times did you ask your team for help while presenting the daily warm-up? ____\nHow often did you learn something new from a teammate’s presentation of the daily warm-up? ____\nHow often did you help a teammate during the daily warm-up presentation? ____\nDid you contribute a question for our guest speaker? ____\n\n\n\nPresence Outside of Class\n\nHow often did you attend Student Hours or Peer Help? ____\nHow often did you ask for or receive help from your fellow students? ____\nDid you regularly participate in a study group outside class? ____\nHow often did you post questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts did you submit? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: ____\nRevisions useful: ____\nRevisions encouraged: ____\nIncomplete: ____\n\nRoughly how many hours per week did you spend on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/end-of-course.html#what-you-learned",
    "href": "assignments/process/end-of-course.html#what-you-learned",
    "title": "End-Of-Course Reflection",
    "section": "What You Learned",
    "text": "What You Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what did you do in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/end-of-course.html#reflecting-on-goals",
    "href": "assignments/process/end-of-course.html#reflecting-on-goals",
    "title": "End-Of-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways did you meet your goals from the beginning of the course? Be specific: explain what the goal was and what you did to meet it.\nIn what ways did you not meet your goals from the beginning of the course? Be specific: explain what the goal was and what the gap was between what you aspired to and what happened.\nIf there’s any context you want to share about how you fared relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you learned, how you participated, or what you achieved in CSCI 0451?\n[your response here]"
  },
  {
    "objectID": "assignments/process/ideas.html",
    "href": "assignments/process/ideas.html",
    "title": "Ideas for Goal-Setting",
    "section": "",
    "text": "If you’re not sure what goals you might want to set for yourself when setting your goals for the course, here are a few ideas to help you get started. Don’t limit yourself to just these ideas! They are just here to show you some possibilities and get your creative juices flowing. Note that these are not requirements and you do not have to do all of these in order to demonstrate learning in the course.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/process/ideas.html#blog-posts",
    "href": "assignments/process/ideas.html#blog-posts",
    "title": "Ideas for Goal-Setting",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nSubmit a blog post in most weeks during the semester.\nFor each of the learning objectives “Theory”, “Implementation”, “Navigation”, “Experimentation,” and “Social Responsibility,” complete at least two blog posts that cover each objective.\n\nNote: many blog posts address multiple objectives.\n\nSubmit the first draft of no more than two blog posts after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by implementing more complex algorithms, discussing more advanced theory, or performing experiments significantly beyond what is requested.\nPropose and complete a blog post on a topic not covered in lecture, especially one related to your areas of focused interest.\nPropose and complete an additional blog post on a topic related to algorithmic bias and social responsibility."
  },
  {
    "objectID": "assignments/process/ideas.html#course-presence",
    "href": "assignments/process/ideas.html#course-presence",
    "title": "Ideas for Goal-Setting",
    "section": "Course Presence",
    "text": "Course Presence\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\n“Pass” at most once when asked to lead the warmup activity for my group.\nAsk questions or make suggestions for the warmup presenter in most weeks.\nPropose questions ahead of time for our guest speaker.\nOrganize a study group outside of class time to work on blog posts or other course work.\nAttend a study group outside of class time.\nFrequently attend Peer Help or Student Hours (after preparing questions and working examples).\nRegularly post questions or answers on the course Slack workspace."
  },
  {
    "objectID": "assignments/process/ideas.html#project",
    "href": "assignments/process/ideas.html#project",
    "title": "Ideas for Goal-Setting",
    "section": "Project",
    "text": "Project\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nImplement algorithms or write automated checks of algorithms written by teammates.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nLead creation of the final project presentation.\nTake the lead in delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "assignments/math-pre-assessment/math-pre-assessment.html",
    "href": "assignments/math-pre-assessment/math-pre-assessment.html",
    "title": "Math Self-Assessment",
    "section": "",
    "text": "This is an ungraded self-assessment. The purpose is for you to assess your comfort with some of the key mathematical ideas we will draw on in this course. Please do the following:\n\nPrint out this webpage or save it as a PDF.\nRead each problem.\nOn paper or a tablet, draw a little face beside each one to indicate your level of comfort:\n\n😊: you are confident and can do this problem without needing to look anything up.\n😑: you believe you could figure this problem out, but you would need to consult a text first.\n\n😓: you feel stuck and aren’t sure how to begin this problem.\n\nRecommended but optional: actually do the problems.\nSubmit your version of the assignment with little faces on Canvas.\n\nIt is ok if some of this content is new to you. See how far you can get and make a note of what you may still need to learn or practice. We will not need most of these ideas until Week 5 or so, so you have time to brush up on any topics for which you feel uncomfortable.\n\nThis resource from Stanford’s CS246 contains most of the linear algebra that you’ll need for the course.\n\n\n\nThe logistic sigmoid \\(\\sigma\\) is the function \\(\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}\\) given by the formula \\[\n\\begin{aligned}\n    \\sigma(w) = \\frac{1}{1 + e^{-w}}\\;.\n\\end{aligned}\n\\]\n\n\nMake a sketch of this function.\n\n\n\n\nCompute \\(\\frac{d\\sigma(w_0)}{dw}\\). \\(\\frac{d\\sigma(w_0)}{dw}\\) is the first derivative of \\(\\sigma\\) evaluated at the point \\(w_0\\).\n\n\n\n\nUsing your computation of \\(\\frac{d\\sigma(w_0)}{dw}\\) from the previous part, check that the following formula holds. You should do so by computing both sides of the equation and verifying that they are equal.\n\\[\n\\begin{aligned}\n    \\frac{d\\sigma(w_0)}{dw} = \\sigma(w_0)\\left(1 - \\sigma(w_0)\\right)\\;.\n\\end{aligned}\n\\]\n\n\n\n\nCompute \\(\\frac{d^2\\sigma(w_0)}{dw^2}\\). It is recommended to use your results from the previous two parts. \\(\\frac{d^2\\sigma(w_0)}{dw^2}\\) is the second derivative of \\(\\sigma\\) evaluated at the point \\(w_0\\).\n\n\n\n\nProvided that the function in question has sufficiently many derivatives, the Taylor expansion can be used to approximate that function near a point. For example, we might find it useful to estimate \\(\\sigma(w_0 + \\delta)\\), where \\(\\delta\\) is a small number.\nCompute an estimate of \\(\\sigma(w_0 + \\delta)\\) using the second-order Taylor expansion of \\(\\sigma\\), assuming that \\(\\delta\\) is sufficiently small.\n\n\n\n\nThe error in this approximation has scaling \\(O(g(\\delta))\\) for some function \\(g\\). Please say as much as you are able about the function \\(g\\).\n\n\n\n\n\nLet \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a function that accepts a vector \\(\\mathbf{w} \\in \\mathbb{R}^p\\) as its argument and returns a scalar \\(y \\in \\mathbb{R}\\). Let’s write \\(\\mathbf{w} = (w_1,\\ldots,w_p)^T\\).\n\n\n\n\n\n\nPartial Derivative\n\n\n\nThe partial derivative of \\(f\\) at point \\(\\mathbf{w}\\) with respect to component \\(w_i\\) is\n\\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial w_i}(\\mathbf{w}) = \\lim_{h \\rightarrow 0} \\frac{f(\\mathbf{w} + h \\mathbf{e}_i) - f(\\mathbf{w})}{h}\\;,\n\\end{aligned}\n\\]\nprovided that this limit exists. Here, \\(\\mathbf{e}_i\\) is the \\(i\\)th standard basis vector.\n\n\nThere is an easy “recipe” for computing partial derivatives: treat \\(f\\) like a single-variable function in which every variable except \\(w_i\\) is held constant, then take the usual single-variable derivative with respect to \\(w_i\\).\n\n\n\n\n\n\nExample\n\n\n\nLet \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) be given by the formula\n\\[\n\\begin{aligned}\n    f(w_1, w_2, w_3) = \\sin(w_1 w_2) + w_1 + e^{2w_3}\\;.\n\\end{aligned}\n\\]\nThe partial derivatives of \\(f\\) evaluated at a generic point \\((w_1,w_2,w_3)^T\\) can be obtained by holding all variables except one constant, and then applying standard rules from single-variable calculus:\n\\[\n\\begin{aligned}\n   \\frac{\\partial f}{\\partial w_1} &= w_2 \\cos(w_1 w_2) + 1 \\\\\n   \\frac{\\partial f}{\\partial w_2} &= w_1 \\cos(w_1 w_2)\\\\\n   \\frac{\\partial f}{\\partial w_3} &= 2e^{w_3}\\;.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nGradient\n\n\n\nThe vector of all partial derivatives of a function \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) at a point \\(\\mathbf{w}\\) is called the gradient \\(\\nabla f (\\mathbf{w})\\):\n\\[\n\\begin{aligned}\n\\nabla f(\\mathbf{w}) = \\left(\\frac{\\partial f(\\mathbf{w})}{\\partial w_1}, \\frac{\\partial f(\\mathbf{w})}{\\partial w_2},\\ldots,\\frac{\\partial f(\\mathbf{w})}{\\partial w_p} \\right)^T\\;.\n\\end{aligned}\n\\]\n\n\n\n\nLet \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) be given by the formula\n\\[\n\\begin{aligned}\n    f(w_1, w_2, w_3) = w_1^2 + e^{w_1w_2} + \\cos(3w_3)\\;.\n\\end{aligned}\n\\]\nCompute \\(\\nabla f(w_1,w_2,w_3)\\).\n\n\n\n\nIt might seem strange to use the variable \\(\\mathbf{w}\\) as the variable of differentiation while treating \\(\\mathbf{x}\\) as a constant. There will be a reason for it when we get to algorithms!\nLet \\(\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^p\\), and let \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle = \\sum_{i = 1}^p x_iw_i\\) be the Euclidean inner product between \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\).  Let \\(f_{\\mathbf{x}}(\\mathbf{w}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\). Here, we are treating \\(f_{\\mathbf{x}}(\\mathbf{w})\\) as a function of \\(\\mathbf{w}\\), holding \\(\\mathbf{x}\\) constant.The operation \\(\\langle \\mathbf{w},\\mathbf{x}\\rangle\\) is also called the dot product and is sometimes written \\(\\mathbf{w}\\cdot \\mathbf{x}\\) or \\(\\mathbf{w}^T\\mathbf{x}\\).\nLet \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w})\\) be the gradient with respect to the variables in \\(\\mathbf{w}\\), holding \\(\\mathbf{x}\\) constant. Write a careful calculation showing that \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w}) = \\mathbf{x}\\).\n\n\n\n\nLet \\(f_\\mathbf{x}(\\mathbf{w}) = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}\\rangle)\\). Compute \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w})\\). You are likely to find the results of previous parts to be helpful.\nHint: chain rule.\n\n\n\n\nLet \\(\\lVert \\mathbf{w} \\rVert_2 = \\sqrt{\\sum_{i = 1}^p w_i^2}\\) be the Euclidean norm of \\(\\mathbf{w}\\). Let \\(f(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2\\). Compute \\(\\nabla_\\mathbf{w} f(\\mathbf{w})\\). Express your answer in terms of the vector \\(\\mathbf{x}\\), but not any of the individual entries \\(x_i\\).\n\n\n\n\n\n\n\n\n\n\nMixed Partial Derivatives\n\n\n\nLet \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a function. The mixed partial derivative \\(\\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j}\\) is defined as\n\\[\n\\begin{aligned}\n    \\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j} = \\frac{\\partial}{\\partial w_j}\\frac{\\partial f(\\mathbf{w})}{\\partial w_i }\\;.\n\\end{aligned}\n\\]\nThat is, to compute a mixed partial derivative, first compute \\(\\frac{\\partial f(\\mathbf{w})}{\\partial w_i }\\) and then differentiate that with respect to \\(w_j\\).\n\n\nCompute \\(\\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j}\\) for the function \\(f_\\mathbf{x}(\\mathbf{w}) = \\sigma(\\langle \\mathbf{x}, \\mathbf{w}\\rangle)\\), assuming that \\(1 \\leq i,j \\leq p\\). Please consider two cases: \\(i \\neq j\\) and \\(i = j\\).\n\n\n\n\n\nSuppose that a randomly-selected population of 1000 people is tested for COVID-19 using a rapid test (which is occasionally wrong), as well as a very thorough lab test which we’ll imagine is always right. The results are summarized in the following table:\n\n\n\n\nCOVID+\nCOVID-\n\n\n\n\nRapid +\n102\n31\n\n\nRapid -\n65\n802\n\n\n\nWe are going to use these results to estimate the reliability of the rapid test.\n\n\nThe accuracy of the rapid test is the fraction of the time in which the rapid test “got the right answer.” What is the accuracy of the rapid test according to these results?\n\n\n\n\nThe true positive rate is the proportion of all COVID+ people who received a Rapid+ result. What is the true positive rate of the rapid test?\n\n\n\n\nThe false positive rate is the proportion of all COVID- people who received a Rapid+ result. What is the false positive rate of the rapid test?\n\n\n\n\nThe true negative rate is the proportion of all COVID- people who received a Rapid- result. The false negative rate is the proportion of all COVID+ people who received a Rapid- result. What are the true and false negative rates of the rapid test?\n\n\n\n\nSuppose that you took the rapid test and received a positive result. What is your estimate of the probability that you are truly COVID+?\n\n\n\n\nSuppose that you took this rapid test and received a negative result. What is your estimate of the probability that you are truly COVID+?\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/math-pre-assessment/math-pre-assessment.html#part-a-single-variable-differentiation",
    "href": "assignments/math-pre-assessment/math-pre-assessment.html#part-a-single-variable-differentiation",
    "title": "Math Self-Assessment",
    "section": "",
    "text": "The logistic sigmoid \\(\\sigma\\) is the function \\(\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}\\) given by the formula \\[\n\\begin{aligned}\n    \\sigma(w) = \\frac{1}{1 + e^{-w}}\\;.\n\\end{aligned}\n\\]\n\n\nMake a sketch of this function.\n\n\n\n\nCompute \\(\\frac{d\\sigma(w_0)}{dw}\\). \\(\\frac{d\\sigma(w_0)}{dw}\\) is the first derivative of \\(\\sigma\\) evaluated at the point \\(w_0\\).\n\n\n\n\nUsing your computation of \\(\\frac{d\\sigma(w_0)}{dw}\\) from the previous part, check that the following formula holds. You should do so by computing both sides of the equation and verifying that they are equal.\n\\[\n\\begin{aligned}\n    \\frac{d\\sigma(w_0)}{dw} = \\sigma(w_0)\\left(1 - \\sigma(w_0)\\right)\\;.\n\\end{aligned}\n\\]\n\n\n\n\nCompute \\(\\frac{d^2\\sigma(w_0)}{dw^2}\\). It is recommended to use your results from the previous two parts. \\(\\frac{d^2\\sigma(w_0)}{dw^2}\\) is the second derivative of \\(\\sigma\\) evaluated at the point \\(w_0\\).\n\n\n\n\nProvided that the function in question has sufficiently many derivatives, the Taylor expansion can be used to approximate that function near a point. For example, we might find it useful to estimate \\(\\sigma(w_0 + \\delta)\\), where \\(\\delta\\) is a small number.\nCompute an estimate of \\(\\sigma(w_0 + \\delta)\\) using the second-order Taylor expansion of \\(\\sigma\\), assuming that \\(\\delta\\) is sufficiently small.\n\n\n\n\nThe error in this approximation has scaling \\(O(g(\\delta))\\) for some function \\(g\\). Please say as much as you are able about the function \\(g\\)."
  },
  {
    "objectID": "assignments/math-pre-assessment/math-pre-assessment.html#part-b-multivariable-differentiation",
    "href": "assignments/math-pre-assessment/math-pre-assessment.html#part-b-multivariable-differentiation",
    "title": "Math Self-Assessment",
    "section": "",
    "text": "Let \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a function that accepts a vector \\(\\mathbf{w} \\in \\mathbb{R}^p\\) as its argument and returns a scalar \\(y \\in \\mathbb{R}\\). Let’s write \\(\\mathbf{w} = (w_1,\\ldots,w_p)^T\\).\n\n\n\n\n\n\nPartial Derivative\n\n\n\nThe partial derivative of \\(f\\) at point \\(\\mathbf{w}\\) with respect to component \\(w_i\\) is\n\\[\n\\begin{aligned}\n    \\frac{\\partial f}{\\partial w_i}(\\mathbf{w}) = \\lim_{h \\rightarrow 0} \\frac{f(\\mathbf{w} + h \\mathbf{e}_i) - f(\\mathbf{w})}{h}\\;,\n\\end{aligned}\n\\]\nprovided that this limit exists. Here, \\(\\mathbf{e}_i\\) is the \\(i\\)th standard basis vector.\n\n\nThere is an easy “recipe” for computing partial derivatives: treat \\(f\\) like a single-variable function in which every variable except \\(w_i\\) is held constant, then take the usual single-variable derivative with respect to \\(w_i\\).\n\n\n\n\n\n\nExample\n\n\n\nLet \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) be given by the formula\n\\[\n\\begin{aligned}\n    f(w_1, w_2, w_3) = \\sin(w_1 w_2) + w_1 + e^{2w_3}\\;.\n\\end{aligned}\n\\]\nThe partial derivatives of \\(f\\) evaluated at a generic point \\((w_1,w_2,w_3)^T\\) can be obtained by holding all variables except one constant, and then applying standard rules from single-variable calculus:\n\\[\n\\begin{aligned}\n   \\frac{\\partial f}{\\partial w_1} &= w_2 \\cos(w_1 w_2) + 1 \\\\\n   \\frac{\\partial f}{\\partial w_2} &= w_1 \\cos(w_1 w_2)\\\\\n   \\frac{\\partial f}{\\partial w_3} &= 2e^{w_3}\\;.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nGradient\n\n\n\nThe vector of all partial derivatives of a function \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) at a point \\(\\mathbf{w}\\) is called the gradient \\(\\nabla f (\\mathbf{w})\\):\n\\[\n\\begin{aligned}\n\\nabla f(\\mathbf{w}) = \\left(\\frac{\\partial f(\\mathbf{w})}{\\partial w_1}, \\frac{\\partial f(\\mathbf{w})}{\\partial w_2},\\ldots,\\frac{\\partial f(\\mathbf{w})}{\\partial w_p} \\right)^T\\;.\n\\end{aligned}\n\\]\n\n\n\n\nLet \\(f:\\mathbb{R}^3 \\rightarrow \\mathbb{R}\\) be given by the formula\n\\[\n\\begin{aligned}\n    f(w_1, w_2, w_3) = w_1^2 + e^{w_1w_2} + \\cos(3w_3)\\;.\n\\end{aligned}\n\\]\nCompute \\(\\nabla f(w_1,w_2,w_3)\\).\n\n\n\n\nIt might seem strange to use the variable \\(\\mathbf{w}\\) as the variable of differentiation while treating \\(\\mathbf{x}\\) as a constant. There will be a reason for it when we get to algorithms!\nLet \\(\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^p\\), and let \\(\\langle \\mathbf{w}, \\mathbf{x} \\rangle = \\sum_{i = 1}^p x_iw_i\\) be the Euclidean inner product between \\(\\mathbf{w}\\) and \\(\\mathbf{x}\\).  Let \\(f_{\\mathbf{x}}(\\mathbf{w}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\). Here, we are treating \\(f_{\\mathbf{x}}(\\mathbf{w})\\) as a function of \\(\\mathbf{w}\\), holding \\(\\mathbf{x}\\) constant.The operation \\(\\langle \\mathbf{w},\\mathbf{x}\\rangle\\) is also called the dot product and is sometimes written \\(\\mathbf{w}\\cdot \\mathbf{x}\\) or \\(\\mathbf{w}^T\\mathbf{x}\\).\nLet \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w})\\) be the gradient with respect to the variables in \\(\\mathbf{w}\\), holding \\(\\mathbf{x}\\) constant. Write a careful calculation showing that \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w}) = \\mathbf{x}\\).\n\n\n\n\nLet \\(f_\\mathbf{x}(\\mathbf{w}) = \\sigma(\\langle \\mathbf{w}, \\mathbf{x}\\rangle)\\). Compute \\(\\nabla_\\mathbf{w}f_{\\mathbf{x}}(\\mathbf{w})\\). You are likely to find the results of previous parts to be helpful.\nHint: chain rule.\n\n\n\n\nLet \\(\\lVert \\mathbf{w} \\rVert_2 = \\sqrt{\\sum_{i = 1}^p w_i^2}\\) be the Euclidean norm of \\(\\mathbf{w}\\). Let \\(f(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2\\). Compute \\(\\nabla_\\mathbf{w} f(\\mathbf{w})\\). Express your answer in terms of the vector \\(\\mathbf{x}\\), but not any of the individual entries \\(x_i\\).\n\n\n\n\n\n\n\n\n\n\nMixed Partial Derivatives\n\n\n\nLet \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a function. The mixed partial derivative \\(\\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j}\\) is defined as\n\\[\n\\begin{aligned}\n    \\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j} = \\frac{\\partial}{\\partial w_j}\\frac{\\partial f(\\mathbf{w})}{\\partial w_i }\\;.\n\\end{aligned}\n\\]\nThat is, to compute a mixed partial derivative, first compute \\(\\frac{\\partial f(\\mathbf{w})}{\\partial w_i }\\) and then differentiate that with respect to \\(w_j\\).\n\n\nCompute \\(\\frac{\\partial^2 f(\\mathbf{w})}{\\partial w_i \\partial w_j}\\) for the function \\(f_\\mathbf{x}(\\mathbf{w}) = \\sigma(\\langle \\mathbf{x}, \\mathbf{w}\\rangle)\\), assuming that \\(1 \\leq i,j \\leq p\\). Please consider two cases: \\(i \\neq j\\) and \\(i = j\\)."
  },
  {
    "objectID": "assignments/math-pre-assessment/math-pre-assessment.html#part-c-computing-probabilities-from-contingency-tables",
    "href": "assignments/math-pre-assessment/math-pre-assessment.html#part-c-computing-probabilities-from-contingency-tables",
    "title": "Math Self-Assessment",
    "section": "",
    "text": "Suppose that a randomly-selected population of 1000 people is tested for COVID-19 using a rapid test (which is occasionally wrong), as well as a very thorough lab test which we’ll imagine is always right. The results are summarized in the following table:\n\n\n\n\nCOVID+\nCOVID-\n\n\n\n\nRapid +\n102\n31\n\n\nRapid -\n65\n802\n\n\n\nWe are going to use these results to estimate the reliability of the rapid test.\n\n\nThe accuracy of the rapid test is the fraction of the time in which the rapid test “got the right answer.” What is the accuracy of the rapid test according to these results?\n\n\n\n\nThe true positive rate is the proportion of all COVID+ people who received a Rapid+ result. What is the true positive rate of the rapid test?\n\n\n\n\nThe false positive rate is the proportion of all COVID- people who received a Rapid+ result. What is the false positive rate of the rapid test?\n\n\n\n\nThe true negative rate is the proportion of all COVID- people who received a Rapid- result. The false negative rate is the proportion of all COVID+ people who received a Rapid- result. What are the true and false negative rates of the rapid test?\n\n\n\n\nSuppose that you took the rapid test and received a positive result. What is your estimate of the probability that you are truly COVID+?\n\n\n\n\nSuppose that you took this rapid test and received a negative result. What is your estimate of the probability that you are truly COVID+?"
  },
  {
    "objectID": "assignments/project/project-presentation.html",
    "href": "assignments/project/project-presentation.html",
    "title": "Project Presentation",
    "section": "",
    "text": "Your project presentation will be a brief presentation followed by a very short Q&A session. What’s in your presentation is largely up to you. Here are your main constraints:\n\nYou have no more than 4 minutes. I will give you a “one minute left” signal at the 3-minute mark, and give you a hard cutoff after 4 minutes have passed.\nYour presentation must be in the form of a Google Slides presentation. Your presentation must be uploaded before the presentation day to this Google Drive folder. It is also fine for your presentation to involve a live demonstration of your software deliverable if you would like; however, the main presentation should be in your slides.\nIt’s fine to delegate parts of your presentation to different group members. That said, if your group is split across sections A and B, you need to ensure that you can give a complete presentation of your project in both section A and section B. \n\nFor example, if one group member is in Section A and two group members are in Section B, then the group member in Section A needs to do a complete presentation on their own.Beyond this, your presentation is up to you. Please feel welcome to be creative and ambitious in showcasing your work!\n\n\n\n  © Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html",
    "href": "assignments/blog-posts/blog-post-linear-regression.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#implement-linear-regression-two-ways",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#implement-linear-regression-two-ways",
    "title": "Implementing Linear Regression",
    "section": "1 Implement Linear Regression Two Ways",
    "text": "1 Implement Linear Regression Two Ways\nTo start this blog post, please implement least-squares linear regression in two ways.\n\nFirst, use the analytical formula for the optimal weight vector \\(\\hat{\\mathbf{w}}\\) from the lecture notes. This formula requires matrix inversion and several matrix multiplications.\nNext, use the formula for the gradient of the loss function to implement gradient descent for linear regression. You can still pass in max_iter and alpha (the learning rate) as parameters to the fit method. \n\nImplementing stochastic gradient descent would be nice thing to do but not required. Only if you want to go above and beyond!In addition to the fit method, your implementation should include a score method (see below) and a predict method (just return \\(\\mathbf{X}\\mathbf{w}\\)).\nIt’s fine for you to either define separate methods like fit_analytic and fit_gradient for these methods. It’s also fine to define a single fit method with a method argument to determine which algorithm is used.\nAs usual, place your implementation in a source file where you will be able to implement it.\n\nThe Score\nLet \\(\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^ny_i\\). Then, the score we’ll use is the so-called coefficient of determination, which is\n\\[\nc = 1 - \\frac{\\sum_{i = 1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^n (\\bar{y} - y_i)^2}\\;.\n\\]\nThe coefficient of determination is always no larger than 1, with a higher value indicating better predictive performance. It can be arbitrarily negative for very bad models. Note that the numerator in the fraction is just \\(L(\\mathbf{w})\\), so making the loss small makes the coefficient of determination large.\n\n\nEfficient Gradient Descent\nFor gradient descent, please implement a score_history so that you can visualize the value of the score over epochs.\nThe formula for the gradient is \\[\n\\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\] However, you should resist the urge to compute this formula “from scratch” at every iteration. The reason is that the matrix multiplication \\(\\mathbf{X}^T\\mathbf{X}\\) has time-complexity \\(O(np^2)\\), where \\(n\\) is the number of data points and \\(p\\) is the number of features. Similarly, the matrix-vector product \\(\\mathbf{X}^T\\mathbf{y}\\) has time-complexity \\(O(np)\\). Both of these can be pretty expensive if you have a lot of data points! Fortunately, they don’t depend on the current value of \\(w\\), so you can actually just precompute them:\n\nOnce during the fit method, compute \\(\\mathbf{P}= \\mathbf{X}^T \\mathbf{X}\\) and \\(\\mathbf{q}= \\mathbf{X}^T \\mathbf{y}\\).\nThe gradient is then \\(\\nabla L(\\mathbf{w}) = 2(\\mathbf{P}\\mathbf{w}- \\mathbf{q})\\).\n\nComputing \\(\\mathbf{P}\\mathbf{w}\\) requires only \\(O(p^2)\\) steps. In other words, precomputing \\(\\mathbf{P}\\) and \\(\\mathbf{q}\\) eliminates the dependence of the runtime on the number of data points – not bad!"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#demo",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#demo",
    "title": "Implementing Linear Regression",
    "section": "2 Demo",
    "text": "2 Demo\nThe following function will create both testing and validation data that you can use to test your implementation:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nHere’s an example of how to use the function to generate data. Unfortunately, it’s only possible to easily visualize this problem when p_features = 1.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nOnce you’ve impmlemented your solution, you should be able to use it like this:\n\nfrom solutions.linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.183\nValidation score = 0.1574\n\n\nThe estimated weight vector \\(\\mathbf{w}\\) is\n\nLR.w\n\narray([0.31679445, 1.08950597])\n\n\nI can get the same value for \\(\\mathbf{w}\\) using gradient descent (it would be even closer if we allowed more iterations).\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_iter = 1e2)\nLR2.w\n\n(2,)\n11.328469647280597\n5.236717682699775\n4.329889482032084\n4.012454684147247\n3.7532710960961264\n3.5131876870105243\n3.2886182249064757\n3.078414055199832\n2.8816465505910274\n2.697456168844806\n2.5250389538925284\n2.363642380127672\n2.2125620250558753\n2.0711384919646654\n1.9387545317693446\n1.8148323489901614\n1.6988310799383664\n1.590244432093386\n1.4885984743673286\n1.3934495686123411\n1.3043824333430065\n1.2210083312222988\n1.142963372400955\n1.069906926304447\n1.0015201349362628\n0.9375045212085436\n0.8775806862259342\n0.8214870898371666\n0.7689789091317749\n0.719826969900038\n0.6738167463923196\n0.6307474250121007\n0.5904310278566007\n0.5526915922789675\n0.5173644028918645\n0.4842952726600383\n0.45333986994442044\n0.42436308856040666\n0.39723845810120845\n0.37184759195230777\n0.3480796705879372\n0.32583095789455535\n0.3050043484101287\n0.2855089435031292\n0.26725965464158385\n0.25017683202051866\n0.23418591692696017\n0.21921711632524907\n0.2052050982422807\n0.19208870662338354\n0.17981069441402237\n0.16831747370264602\n0.15755888183383046\n0.1474879624713472\n0.13806076065513045\n0.12923613095797595\n0.12097555790459999\n0.11324298786914323\n0.10600467171760167\n0.09922901750827567\n0.09288645260737073\n0.0869492946180047\n0.08139163055914499\n0.0761892037673134\n0.07131930802737269\n0.06676068847021657\n0.062493448805007\n0.05849896448096553\n0.05475980139972026\n0.051259639823399365\n0.04798320314649763\n0.04491619122043182\n0.04204521793992856\n0.03935775281881115\n0.036842066300137755\n0.03448717856206151\n0.032282811595914215\n0.03021934434739268\n0.02828777072508872\n0.026479660292977047\n0.024787121475399344\n0.02320276711402693\n0.021719682226188593\n0.02033139382422293\n0.019031842663739848\n0.017815356797920807\n0.016676626821934472\n0.015610682699918022\n0.014612872072974804\n0.013678839953758766\n0.012804509718958525\n0.011986065316754802\n0.011219934611367261\n0.010502773792455393\n0.009831452780809252\n0.009203041567052343\n0.008614797423449762\n0.008064152933172926\n0.0075487047847031235\n0.007066203282447441\n\n\narray([0.31771408, 1.08903144])\n\n\nI can also see how the score changed over time. Because we’re not using stochastic gradient descent, the score should increase monotonically in each iteration.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\n\n\nYour implementation is likely correct when you are able to reproduce results that are similar to these (although small differences are no problem)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#experiments",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "3 Experiments",
    "text": "3 Experiments\nOnce you’ve demonstrated the behavior above, perform an experiment in which you allow p_features, the number of features used, to increase, while holding n_train, the number of training points, constant. Try increasing p_features all the way to n_train - 1. What happens to the training score? What happens to the validation score? I’d suggest showing these results on a nice plot in which the horizontal axis is the number of features, the vertical axis is the score, and the training/validation scores are shown in different colors.\nOptional: Relate your findings when p_features = n_train-1 to the existence of a solution of the equation \\(\\mathbf{X}\\mathbf{w}= \\mathbf{y}\\). What do you know about the rank of \\(\\mathbf{X}\\)? Remember that the number of columns in \\(\\mathbf{X}\\) is actually p_features + 1, since it’s still necessary to pad with a column of 1s.\nWhen discussing your findings, make sure to connect them to the idea of overfitting."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#lasso-regularization",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "4 LASSO Regularization",
    "text": "4 LASSO Regularization\nThe LASSO algorithm uses a modified loss function with a regularization term:\n\\[\nL(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;.\n\\]\nHere, \\(\\mathbf{w}'\\) is the vector composed of all the entries of \\(\\mathbf{w}\\) excluding the very last entry. The 1-norm is defined as\n\\[\n\\lVert \\mathbf{w}' \\rVert_1 = \\sum_{j = 1}^{p-1} \\lvert w_j \\rvert\\;.\n\\]\nThe effect of the regularizing term is to make the entries of the weight vector \\(\\mathbf{w}\\) small. In fact, LASSO has a nice property: it tends to force entries of the weight vector to be exactly zero.  This is very desirable in so-called overparameterized problems, when the number of features \\(p\\) is larger than the number of data points \\(n\\).The reason we exclude the final entry of \\(\\mathbf{w}\\) is that it is not desirable to penalize the weight corresponding to the constant feature in \\(\\mathbf{X}\\).\nImplementing LASSO involves some more complicated mathematical optimization than we will discuss in this course, so instead we’ll use the implementation in scikit-learn. You can import it like this:\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nHere, alpha controls the strength of the regularization (it’s not related to the learning rate in gradient descent). Let’s fit this model on some data and check the coefficients:\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\nThe score on the validation set is high, which might be different from what you found with pure linear regression.\n\nL.score(X_val, y_val)\n\n0.6410338391919257\n\n\n\nWhat You Should Do\nReplicate the same experiment you did with linear regression, increasing the number of features up to or even past n_train - 1, using LASSO instead of linear regression. You might want to experiment with a few values of the regularization strength alpha. Comment on how your validation score compares to standard linear regression when the number of features used is large."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#optional-bikeshare-data-set",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#optional-bikeshare-data-set",
    "title": "Implementing Linear Regression",
    "section": "5 Optional: Bikeshare Data Set",
    "text": "5 Optional: Bikeshare Data Set\nThe following code will download and save a data set related to the Capital Bikeshare system in Washington DC. We use the aggregated version graciously provided by the authors of the following paper:\n\nFanaee-T, Hadi, and Gama, Joao, “Event labeling combining ensemble detectors and background knowledge”, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.\n\nThis data set includes information about the season and time of year; the weather; and the count of bicycle users on each day for two years (year 0 is 2011, year 1 is 2012). This level of information gives us considerable ability to model phenomena in the data.\nFor more on what the entries in each column means, you can consult the data dictionary here (under “Attribute Information”).\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n2011-01-01\n1\n0\n1\n0\n6\n0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n331\n654\n985\n\n\n1\n2\n2011-01-02\n1\n0\n1\n0\n0\n0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n131\n670\n801\n\n\n2\n3\n2011-01-03\n1\n0\n1\n0\n1\n1\n1\n0.196364\n0.189405\n0.437273\n0.248309\n120\n1229\n1349\n\n\n3\n4\n2011-01-04\n1\n0\n1\n0\n2\n1\n1\n0.200000\n0.212122\n0.590435\n0.160296\n108\n1454\n1562\n\n\n4\n5\n2011-01-05\n1\n0\n1\n0\n3\n1\n1\n0.226957\n0.229270\n0.436957\n0.186900\n82\n1518\n1600\n\n\n\n\n\n\n\n\nOur aim for this case study is to plot daily usage by casual users (as opposed to registered users). The total number of casual users each day is given by the casual column, Let’s plot this over time:\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\n\n\n\n\nFor this prediction task, it’s handy to work with a smaller subset of the columns, and to transform the mnth column into dummy variables.\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n\n\n\n\ncasual\nweathersit\nworkingday\nyr\ntemp\nhum\nwindspeed\nholiday\nmnth_2\nmnth_3\nmnth_4\nmnth_5\nmnth_6\nmnth_7\nmnth_8\nmnth_9\nmnth_10\nmnth_11\nmnth_12\n\n\n\n\n0\n331\n2\n0\n0\n0.344167\n0.805833\n0.160446\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n131\n2\n0\n0\n0.363478\n0.696087\n0.248539\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n120\n1\n1\n0\n0.196364\n0.437273\n0.248309\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n108\n1\n1\n0\n0.200000\n0.590435\n0.160296\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n82\n1\n1\n0\n0.226957\n0.436957\n0.186900\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n247\n2\n1\n1\n0.254167\n0.652917\n0.350133\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n727\n644\n2\n1\n1\n0.253333\n0.590000\n0.155471\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n728\n159\n2\n0\n1\n0.253333\n0.752917\n0.124383\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n729\n364\n1\n0\n1\n0.255833\n0.483333\n0.350754\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n730\n439\n2\n1\n1\n0.215833\n0.577500\n0.154846\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n731 rows × 19 columns\n\n\n\n\nNow we can do a train-test split.\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\nTrain an instance of your LinearRegression class on the bikeshare training data. Then:\n\nScore your model on the test set.\nCompute the predictions for each day and visualize them in comparison to the actual ridership on the test set.\nCompare the entries w of your model to the corresponding entry of X_train.columns in order to see which features your model found to contribute to ridership. Positive coefficients suggest that the corresponding feature contributes to ridership. Can you find effects corresponding to nice weather? Summer months? Holidays? Weekends?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "The expectations for all blog posts apply!\nThis blog post is a replication study. In this post, you will read a famous scholarly paper on racial bias in a healthcare recommender system; replicate its primary findings; and discuss your results.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#preparation",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#preparation",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Preparation",
    "text": "Preparation\nTo prepare to complete this blog post, you need to do three things:\nFirst, read this journal article\n\nObermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.\n\nSecond, watch the concise high-level discussion of this paper in the video “Are We Automating Racism?”. I recommend the entire video, but the discussion of this paper runs from approximately 14:45 to 17:51. The discussion features Dr. Ruha Benjamin, a prominent critical scholar of technology and society. She is the author of the well-known book Race After Technology, as well as several others.\nThird, reread the article in the context of Dr. Benjamin’s discussion.\nNow, proceed on to the next parts."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#part-a-data-access",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#part-a-data-access",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part A: Data Access",
    "text": "Part A: Data Access\nIn order to protect patient privacy, the authors did not share the “real” data used in their study. Instead, they created a randomized version of the data that preserves many of the same patterns and trends. You can use the code below to access the data.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nPoke around the data a bit. You should consult the code repository provided by the authors, especially the data dictionary, in order to interpret the meaning of the rows and columns.\nA few of the columns are especially important:\n\nrisk_score_t is the algorithm’s risk score assigned to a given patient.\ncost_t is the patient’s medical costs in the study period.\nrace is the patient’s self-reported race. The authors filtered the data to include only white and black patients.\ngagne_sum_t is the total number of chronic illnesses presented by the patient during the study period."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#part-b-reproduce-fig.-1",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#part-b-reproduce-fig.-1",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part B: Reproduce Fig. 1",
    "text": "Part B: Reproduce Fig. 1\nPlease make a plot in which you reproduce Fig. 1A of Obermeyer et al. (2019). Your figure is not required to be exactly the same, but it must visualize risk score percentiles against mean number of active chronic conditions within that percentile. For example, I thought I would try spliting out male and female patients. I also think the figure looks a bit nicer with axes inverted. So, my result looked like this:\n\n\n\n\n\n\n\n\n\nPlease make sure to appropriately label all axes, legends, and plot facets.\nInclude a brief discussion of the meaning of this plot. Suppose that Patient A is Black, that Patient B is White, and that both Patient A and Patient B have exactly the same chronic illnesses. Are Patient A and Patient B equally likely to be referred to the high-risk care management program?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#part-c-reproduce-fig.-3",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#part-c-reproduce-fig.-3",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part C: Reproduce Fig. 3",
    "text": "Part C: Reproduce Fig. 3\nThe authors write that\n\n…there are many opportunities for a wedge to creep in between needing health care and receiving health care—and crucially, we find that wedge to be correlated with race…\n\nTo support their argument, they produce Figure 3. This figure shows how total medical expenditures are correlated with the risk score and with the number of chronic health conditions. Please produce a version of Figure 3, including both panels. For the horizontal axis in the second panel, please use the number of chronic conditions. It is not necessary to draw threshold values in the same way as the authors. Here’s an example output.\nNotes:\n\nIn my version of the first panel of Figure 3, I plot the mean expenditure within each risk score percentile.\nAlthough not explicitly labeled, I believe that the righthand panel of Fig. 3 in Obermeyer et al. (2019) has percentiles in the number of chronic conditions on the horizontal axis. I’m uncertain how they computed these, considering that in our sample data set the maximum number of chronic conditions is 17. This implies that either the original data set has patients with more chronic conditions or the authors did something to arbitrarily break ties. To avoid dealing with this, please simply use the number of chronic illnesses for your horizontal axis. Here’s an example of my solution:\n\n\n\n\n\n\nI found it helpful to use plt.subplots to create a figure with two panels, and then sns.scatterplot to populate each of the panels.\n\n\n\n\nComment on your findings. You may find it useful to note that the vast majority of patients in this data set have 5 or fewer chronic conditions, as you will confirm in Part D."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#part-d-modeling-cost-disparity",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#part-d-modeling-cost-disparity",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part D: Modeling Cost Disparity",
    "text": "Part D: Modeling Cost Disparity\nYou’ll notice that there is a relatively stable pattern of disparity in the cost incurred by Black and white patients with 5 or fewer chronic conditions, but that this pattern begins to swing wildly in one direction or another as the number of active chronic conditions increases. This is likely a reflection of the smaller number of data points as the number of conditions increases, which exacerbates the wild variability of healthcare costs.\nLet’s see if we can quantify the disparity that we observe in patients that have 5 or fewer active chronic conditions.\n\nData Prep\n\nFirst, determine the percentage of the patients in the data with 5 or fewer chronic conditions. Does this percentage justify the choice to focus on these patients?\nCreate a new column of the data set which is just the logarithm of the cost. This is called a log-transform. We’ll use this as our target variable. Log transforms are common when the target variable varies widely across several orders of magnitude. Because \\(\\log(0)\\) is undefined, you should subset the data so that patients who incurred $0 in medical costs are removed.\nCreate a dummy (one-hot encoded) column for the qualitative race variable in which 0 means that the patient is white and 1 means that the patient is Black.\nSeparate the data into predictor variables X and target variable y (the log-cost). For predictor variables, just use the dummy columns for race and the number of active chronic conditions.\n\n\n\nModeling\nYour figure from the previous part suggests that the relationship between the number of chronic conditions and the cost might be nonlinear. For this reason, we are going to fit a linear regression model with polynomial features in the number of active chronic conditions in order to to account for the nonlinearity. Here’s a mathematical description of our model: This is a relatively “casual” approach to modeling and should be viewed as exploratory rather than statistically confirmatory.\n\\[\n\\begin{aligned}\n  \\log \\mathrm{cost} \\approx w_b \\times (\\text{patient is Black}) + \\mathrm{intercept} + \\sum_{i = 1}^k w_k\\times (\\text{gagne sum})^k\\;.\n\\end{aligned}\n\\]\nOur primary statistical interest is in the value of the coefficient \\(w_b\\). In the context of a log-transformed linear model, \\(w_b\\) has the following interpretation: the number \\(e^{w_b}\\) is an estimate of the percentage of cost incurred by a Black patient in comparison to an equally sick white patient.\nTo estimate \\(w_b\\), we can use a linear regression model, which is supplied by scikit-learn. How many polynomial features should we use? A simple way to figure this out is cross-validation. Here’s a function that will construct data sets with polynomial features of various sizes:\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nUse this function to loop through polynomial features of varying degrees. For each possible degree, construct a LinearRegression model and compute its score on the expanded data using cross-validation. From your results, determine a reasonable value of the polynomial degree that appears to led to the best predictions on this data set. There may be several reasonable choices.\nOnce you have determined a degree that seems reasonable for this data set, construct a copy of the data with the correct number of polynomial features and fit one last linear regression model.\nYou can access the coefficients of the linear regression model as LR.coef_. Determine which of these coefficients corresponds to Black race (the coefficients are in the same order as the variables in the data frame used for training). Finally, compute \\(e^{w_\\mathrm{b}}\\). What is your estimate of the cost incurred by Black patients as a percentage of white patients. Does it roughly support the argument of Obermeyer et al. (2019)?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-dissecting-bias.html#part-e-abstract-and-discussion",
    "href": "assignments/blog-posts/blog-post-dissecting-bias.html#part-e-abstract-and-discussion",
    "title": "Replication Study: Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Part E: Abstract and Discussion",
    "text": "Part E: Abstract and Discussion\nPlease add an introductory “abstract” section to your blog post describing the high-level aims of of your analysis and an overview of your findings. The abstract should be no more than one paragraph. Then, add a closing “discussion” section of your blog post in which you summarize your findings and describe what you learned from the process of completing this post. In your discussion, please address the following question:\n\nOf the formal statistical discrimination criteria discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023), which of these criteria best describe the purported bias of the algorithm studied by Obermeyer et al. (2019)? What aspects of the study support your answer?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html",
    "href": "assignments/blog-posts/blog-post-music-classification.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "Content warning: this blog post involves the use of song lyric data, some of which may be obscene or offensive.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#introduction",
    "href": "assignments/blog-posts/blog-post-music-classification.html#introduction",
    "title": "Deep Music Genre Classification",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe code below will allow you to download and load into memory a Pandas data frame containing information on 28,000 musical tracks produced between the years 1950 and 2019.\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\nI accessed the data on Kaggle here. The data was originally collected from Spotify by researchers who published in the following data publication:\n\nMoura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; França, Mardônio (2020), “Music Dataset: Lyrics and Metadata from 1950 to 2019”, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3\n\nHere’s an excerpt of the data:\n\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\npop\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\npop\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\npop\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\npop\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\npop\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nIn this blog post, your task is to use Torch to predict the genre of the track based on the track’s lyrics and engineered features. The lyrics are contained in the lyrics column. Here is a list of the engineered features that you may additionally find useful.\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']      \n\nThese features were engineered by teams at Spotify to describe attributes of the tracks."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-music-classification.html#what-you-should-do",
    "title": "Deep Music Genre Classification",
    "section": "2 What You Should Do",
    "text": "2 What You Should Do\nCreate at least three neural networks with Torch and train them.\n\nYour first network should use only the lyrics to perform the classification task. You are welcome to use any technique for this, and it’s ok for your solution to closely resemble the methods from our lecture on text classification.\nYour second network should use only the engineered features to perform the classification task. Don’t overthink this one: a few fully-connected layers are fine.\nYour third network should use both the lyrics and the engineered features to perform the classification task.\nFinally, visualize the word embedding learned by your model and comment on any interesting results you notice."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#code-specifications",
    "href": "assignments/blog-posts/blog-post-music-classification.html#code-specifications",
    "title": "Deep Music Genre Classification",
    "section": "3 Code Specifications",
    "text": "3 Code Specifications\n\nPlease implement exactly one data loader class for your networks. This class should return batches of data containing both the text features and the engineered features.\nPlease implement exactly one function for your training loop. You can pass this function arguments that state whether the model being trained should use only the text features, only the engineered features, or both.\n\nYou can use simple array slicing to access only one set of features from a batch containing both features.\n\nPlease perform a train-validation split and compare each of your three models on validation data. Again, please implement only one function for your evaluation loop, which can use the same mechanism as the training loop to determine which part of the data should be passed to the model."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#writing-specifications",
    "href": "assignments/blog-posts/blog-post-music-classification.html#writing-specifications",
    "title": "Deep Music Genre Classification",
    "section": "4 Writing Specifications",
    "text": "4 Writing Specifications\nPlease include:\n\nAn introductory paragraph describing the purpose of the post.\nA closing paragraph summarizing your results and what you learned from the process.\nCommentary throughout describing the design of your models, how you implemented utility functions, and how to interpret the visualizations and numerical results you produce."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#hints",
    "href": "assignments/blog-posts/blog-post-music-classification.html#hints",
    "title": "Deep Music Genre Classification",
    "section": "5 Hints",
    "text": "5 Hints\n\nArchitecture for Third Network\nFor the third network, I recommend that you:\n\nSeparate the input data into the text features and engineered features (this can be done in either the model or the data loader).\nIn the forward method of your model, process the text features and the engineered features through separate pipelines. Then, use the torch.cat function to combine them, and finally pass the result through one or two fully-connected layers before the output. Here’s a rough outline:\n\nclass CombinedNet(nn.Module):\n    \n    def __init__(self):\n        # ...\n    \n    def forward(self, x):\n        # separate x into x_1 (text features) and x_2 (engineered features)\n        \n        # text pipeline: try embedding! \n        # x_1 = ...\n\n        # engineered features: fully-connected Linear layers are fine\n        # x_2 = ...\n\n        # ensure that both x_1 and x_2 are 2-d tensors, flattening if necessary\n        # then, combine them with: \n        x = torch.cat(x_1, x_2, 1)\n        # pass x through a couple more fully-connected layers and return output\n\n\nWorking with Lyrics\nThere are a few things that I found it helpful to do when working with the song lyrics in order to make the task manageable. That said, I strongly suspect that it is possible to significantly improve on my solution.\nFirst, I chose only the most common tokens in the data by including only tokens that appeared at least 50 times. I did this using the min_freq argument of build_vocab_from_iterator:\nvocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"&lt;unk&gt;\"], min_freq = 50)\nSecond, I incorporated some nn.Dropout layers in between several stages of my model, with the dropout probability equal to 0.2.\nFinally, I took the average across tokens for each embedding dimension, using tensor.mean():\n\n    def forward(self, x):\n        # ...\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = x.mean(axis = 1)\n        # ..."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#expected-accuracy",
    "href": "assignments/blog-posts/blog-post-music-classification.html#expected-accuracy",
    "title": "Deep Music Genre Classification",
    "section": "6 Expected Accuracy",
    "text": "6 Expected Accuracy\nPlease compute the base rate for your problem. While it’s possible to achieve relatively high accuracy classification on this data set, you should consider your three models to be successful if they all consistently score above the base rate after training, even if the improvement is not large. Please make sure to comment on the performance of each model, especially on the performance of the third model compared to the other two."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#if-its-not-working-too-slow",
    "href": "assignments/blog-posts/blog-post-music-classification.html#if-its-not-working-too-slow",
    "title": "Deep Music Genre Classification",
    "section": "7 If It’s Not Working / Too Slow",
    "text": "7 If It’s Not Working / Too Slow\n\nCome chat with me in OH!\nWork in Colab and make sure that you are using the GPU.\nIt’s ok to reduce the size of your data by restricting to a smaller number of genres, e.g. 3."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-music-classification.html#optional-extras",
    "href": "assignments/blog-posts/blog-post-music-classification.html#optional-extras",
    "title": "Deep Music Genre Classification",
    "section": "8 Optional Extras",
    "text": "8 Optional Extras\nOptionally, I encourage you to create some interesting visualizations that might highlight differences between genres in terms of some of the engineered features, perhaps over time. You’re welcome to pose and address your own questions here. A few that I am wondering about are:\n\nHas pop music gotten more danceable over time in this sample, according to Spotify’s definition of danceability?\nDoes blues music tend to have more sadness than other genres? Does pop or rock have more energy?\nAre acousticness and instrumentalness similar features? Can you find any patterns in when they disagree?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html",
    "href": "assignments/blog-posts/blog-post-WIDS.html",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "",
    "text": "The expectations for all blog posts apply!\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html#introduction",
    "href": "assignments/blog-posts/blog-post-WIDS.html#introduction",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Introduction",
    "text": "Introduction\nThe Middlebury Women in Data Science (WiDS) Conference will take place on March 4th, 2024. This blog post is an opportunity for you to attend the conference, reflect on your learning, and contextualize the conference against the broad landscape of gender disparities in data science and STEM more generally."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html#part-1-why-spotlight-women-in-data-science",
    "href": "assignments/blog-posts/blog-post-WIDS.html#part-1-why-spotlight-women-in-data-science",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?\nWhy is it important to have events that specifically shine a spotlight on the accomplishments of women in data science? To address this question, we’ll read some sections of the report Solving the Equation: The Variables for Women’s Success in Engineering and Computing written by Corbett and Hill (2015) for the American Association of University Women. Please read the following:\n\nExecutive Summary (pages 2-5)\nWhy So Few? (pages 34-46)\nWomen in Engineering and Computing (pages 8-16)\nWhat Can We Do? (pages 98-105)\n\nOnce you’ve completed your reading, write three paragraphs:\n\nWhy is it a problem that women are underrepresented in computing, math, and engineering? For whom is it a problem?\nHow is the representation and status of women in computing today different from the 1950s and 1960s? What are some of the forces that brought on this change?\nWhich of the barriers and unequal challenges described in the section “Why So Few?” can be eroded by events that spotlight the achievement of women in STEM?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html#part-2-attend-wids",
    "href": "assignments/blog-posts/blog-post-WIDS.html#part-2-attend-wids",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 2: Attend WiDS",
    "text": "Part 2: Attend WiDS\nAttend the conference and take notes, especially on Dr. Brown’s keynote."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html#part-3-report",
    "href": "assignments/blog-posts/blog-post-WIDS.html#part-3-report",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 3: Report!",
    "text": "Part 3: Report!\n\nFor each of the three lightning talks, please write a paragraph describing who spoke, what they spoke about, their main argument, and what you learned.\nPlease write two paragraphs in which you report on Dr. Brown’s keynote presentation, including her topic, her primary argument, and what you learned."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-WIDS.html#part-4-transitions-abstract-and-reflection",
    "href": "assignments/blog-posts/blog-post-WIDS.html#part-4-transitions-abstract-and-reflection",
    "title": "The Women in Data Science (WiDS) Conference at Middlebury College",
    "section": "Part 4: Transitions, Abstract, and Reflection",
    "text": "Part 4: Transitions, Abstract, and Reflection\nPlease add transition sentences and paragraphs in your writing so that your blog post reads like a single cohesive essay on the status of women in computing and the role of conferences like WiDS in supporting change. Use markdown section headers to delineate sections and transition between them.\nThen, add an introductory “abstract” section to your blog post describing the high-level structure of the post and your biggest takeaways. Please also add a reflection paragraph at the bottom describing what you learned from completing this blog post and what you hope to learn next."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#kernel-logistic-regression",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#kernel-logistic-regression",
    "title": "Kernel Logistic Regression",
    "section": "1.1 Kernel Logistic Regression",
    "text": "1.1 Kernel Logistic Regression\nIn the kernel logistic regression problem, we instead solve empirical risk minimization with modified features. The empirical risk is now\n\\[\nL_k(\\mathbf{v}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{v}, \\boldsymbol{\\kappa}(\\mathbf{x}_i) \\rangle, y_i)\\;,\n\\tag{1}\\]\nwhere \\(\\mathbf{v}\\in \\mathbb{R}^n\\) (not  \\(\\mathbb{R}^p\\)). The modified feature vector \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\) has entries\n\\[\n\\boldsymbol{\\kappa}(\\mathbf{x}_i) = \\left( \\begin{matrix}\n    k(\\mathbf{x}_1, \\mathbf{x}_i) \\\\\n    k(\\mathbf{x}_2, \\mathbf{x}_i) \\\\\n    \\vdots \\\\\n    k(\\mathbf{x}_n, \\mathbf{x}_i)\n\\end{matrix}\\right)\\;.\n\\]\nHere, \\(k:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) is the kernel function. Kernel functions need to satisfy some special mathematical properties. We’re not going to code them up; instead we’re going to use some built-in functions from scikit-learn to handle the kernel functions for us.\nOnce the model has been trained and an optimal \\(\\hat{\\mathbf{v}}\\) has been obtained, one can then make a prediction using the formula\n\\[\n\\hat{y} = \\langle \\hat{\\mathbf{v}}, \\boldsymbol{\\kappa}(\\mathbf{x}) \\rangle\\;.\n\\]\nIf it is desired to return a 0-1 label instead of a real number, one can return \\(\\mathbb{1}[\\hat{y} &gt; 0]\\)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#implement-kernel-logistic-regression",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#implement-kernel-logistic-regression",
    "title": "Kernel Logistic Regression",
    "section": "2.1 Implement Kernel Logistic Regression",
    "text": "2.1 Implement Kernel Logistic Regression\nImplement a Python class called KernelLogisticRegression. You’ll be able to use it like the example in the previous section. Your class should implement the following methods:\nIf you’re not sure how to use **kwargs in Python functions and methods, you might want to check this resource.\n\n__init__(self, kernel, **kernel_kwargs) should accept a kernel function and a set of named keyword arguments called kernel_kwargs. All the __init__() method should do is to save these items as instance variables called\n\nself.kernel\nself.kernel_kwargs\n\nfit(self, X, y) will again be the method that learns the optimal parameters \\(\\hat{v}\\). The fit method is going to look a little different this time:\n\nFirst, pad X to make sure that X contains a column of 1s. Here’s a function to do this:\n\n    def pad(X):\n        return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nSave X as an instance variable called self.X_train.\nCompute the kernel matrix of X with itself. If you implemented __init__() correct, this can be done with the call\n\nkm = self.kernel(X_, X_, **self.kernel_kwargs)\n\nMinimize the empirical risk Equation 1. You might find it useful to define a separate function for computing the empirical risk. Note that the predictor is still an inner product, just with a different parameter vector \\(\\mathbf{v}\\) and a different matrix column \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\). This means that, if you’re careful, you can compute the entire empirical risk using just one matrix multiplication!\n\nHowever you find it, save the resulting optimal value of \\(\\mathbf{v}\\) as self.v.\nYou should still use the logistic loss for \\(\\ell\\).\nYou will probably need to choose a random initial \\(\\mathbf{v}\\). Don’t forget that \\(\\mathbf{v}\\) should have length equal to the number of data points, not the number of features.\n\nIf you’ve already implemented gradient descent for logistic regression in this blog post, then it’s not too hard to adapt your method to kernel logistic regression. However, it’s also fine to use the function scipy.optimize.minimize() as demonstrated in this lecture.\n\npredict(self, X) should accept a new feature matrix and return binary labels \\(\\{0,1\\}\\). For each row of \\(\\mathbf{X}\\), the prediction is obtained using the formula \\(\\mathbb{1}[\\langle \\hat{\\mathbf{v}}, \\boldsymbol{\\kappa}(\\mathbf{x}) \\rangle]\\). To do this:\n\nCompute the kernel matrix between self.X_train and the new feature input X. Each column of this matrix is \\(\\boldsymbol{\\kappa}(\\mathbf{x}_j)\\) for some \\(j\\).\nCompute inner products of the form \\(\\langle \\mathbf{v}, \\boldsymbol{\\kappa}(\\mathbf{x}_j) \\rangle\\). If the user supplies a matrix X with multiple columns, you should be able to compute all the predictions at once. This can be done efficiently using matrix multiplication.\nFinally, return a binary vector \\(\\hat{\\mathbf{y}}\\) whose \\(j\\)th entry is \\(\\hat{y}_j = \\mathbb{1}[\\langle \\mathbf{v}, \\mathbf{x}_j \\rangle &gt; 0]\\).\n\nscore(self, X, y) computes the accuracy of the model predictions on the feature matrix X with labels y.\n\nYou can assume that the user will always only call predict and score after calling fit. If you’d like, you’re welcome to add warnings or handle other cases in which the user may be less cooperative and attempt to call one of those methods first.\nMy complete implementation of kernel logistic regression was about 50 lines of code, excluding comments.\nDocstrings are not expected for this blog post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#experiments",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#experiments",
    "title": "Kernel Logistic Regression",
    "section": "2.2 Experiments",
    "text": "2.2 Experiments\n\nBasic Check\nOnce you’re done, you’ll be able to import and and use your function like this.\nfrom kernel_logistic import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nHere, the rbf_kernel is the kernel function and gamma is a parameter to that kernel function that says how “wiggly” the decision boundary should be. Larger gamma means a more wiggly decision boundary.\nYour implementation is likely correct when you can generate new synthetic versions of the data set above (just call make_moons again) and achieve accuracy consistently at or above 90%. To check that, you can just run the code block above a few times.\n\n\nChoosing gamma\nWhen we choose a very large value of gamma, we can achieve a very wiggly decision boundary with very good accuracy on the training data. For example:\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\n\n\n\n\nHere, our classifier draws a little orange blob around each orange data point: points very nearby are classified as orange while other points are classified as blue. This is sufficient to achieve 100% accuracy on the training data. But this doesn’t generalize: generate some new data and we’ll see much worse performance:\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\n\n\n\n\nWhoops! Not so good. We say that the validation or testing accuracy of the classifier is quite low. Cases in which the validation accuracy is low even though the training accuracy is high are classic instances of overfitting.\n Design an experiment in which you fit your model for several different values of gamma. Show accuracy on both training data (the data on which the model was fit) and testing data (data generated from the same settings but which the model has never seen before). Please show your findings in the form of an attractive visualization with clear labels and a clear message.My suggestion is to choose gamma in 10**np.arange(-5, 6)\n\n\nVary the Noise\nRepeat your experiment with at least two other values of the noise parameter to make_moons. The noise determines how spread out the two crescents of points are. Do your findings suggest that the best value of gamma depends much on the amount of noise?\n\n\nTry Other Problem Geometries\nUse the make_circles function to generate some concentric circles instead of crescents. Show a few examples with varying amounts of noise. Can you find some values of gamma that look like they lead to good learning performance for this data set? Here’s an example of a fairly successful classifier: both the points and the accuracy are computed on unseen test data.\n\n\n0.995"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#blog-post",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#blog-post",
    "title": "Kernel Logistic Regression",
    "section": "2.3 Blog Post",
    "text": "2.3 Blog Post\nYour blog post should describe your approach to your code and written descriptions of your experiments.\n\nPlease include a walk-through for your user of how you computed the empirical loss.\nPlease make sure that your figures are appropriately labeled and described.\nPlease make sure to include a link to the GitHub page containing your source code at the very beginning of the blog post.\n\nIn case you’re curious, it’s possible to add formal captions to your figures in Quarto. This makes things look a little fancier, but is not required!\nOnce you’re happy with how things look, render your blog, push it to GitHub, and submit a link to the URL of your blog post on Canvas."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html",
    "href": "assignments/blog-posts/blog-post-image-processing.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "$$\n$$\nThis is a two-part blog post on linear algebra methods for unsupervised learning with two kinds of data: images and graphs. You can be a little bit choosy in which parts of this blog post you want to do. Doing either of the two main parts is fine. You can also choose to do just one of the main parts and its corresponding “optional extras.”\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#application-to-images",
    "href": "assignments/blog-posts/blog-post-image-processing.html#application-to-images",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Application to Images",
    "text": "Application to Images\nThe following function will read an image for you from a URL and save it as a numpy array.\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nI will choose an image of Maru. Maru is a Cat On The Internet who is famous for doing stuff like this:\n\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\nMy image is an RGB color image (I suggest you find an RGB image as well). In the code below, I’ll convert it to greyscale.\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n\n\n\n\n\n\n\nMy grey_img is now a simple (but large) matrix:\n\ngrey_img.shape\n\n(413, 640)\n\n\nThis means that I can use my SVD pipeline to construct approximations of my image. This task is called image compression and it is an important problem for storing large quantities of images on computers that may have small amounts of storage."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "What You Should Do",
    "text": "What You Should Do\nIn your blog post:\n\nAccess your favorite RGB image from the internet by its URL, download it, and convert it to greyscale using the workflow shown above.\nWrite a function called svd_reconstruct that reconstructs an image from its singular value decomposition. Your function should have two arguments: the image to reconstruct, and the number k of singular values to use.\nPerform an experiment in which you reconstruct your image with several different values of k. Your choice of k should go up at least until you can’t distinguish the reconstructed image from the original by eye. As part of your experiment, you should determine the amount of storage needed for your reconstruction as a fraction of the amount of storage needed for the original image.\n\nHint: An \\(m\\times n\\) greyscale image needs \\(mn\\) pixels (numbers) to represent it. How many numbers must be stored to reconstruct this image with k components using the SVD?\n\n\nHere’s an example of output from your experiment:\n\nfrom solutions.images import svd_reconstruct, svd_experiment\nsvd_experiment(grey_img)\n\n\n\n\n\n\n\n\nIn your blog post, include all your implementation code. Use comments and surrounding text to discuss your solution, and comment on your findings."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#optional-extras",
    "href": "assignments/blog-posts/blog-post-image-processing.html#optional-extras",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Optional Extras",
    "text": "Optional Extras\nImplement and demonstrate one or more of the following functionalities in your svd_reconstruct function:\n\nAllow the user to specify a desired compression factor and select the number of components k to use based on this selection.\nAllow the user to specify a desired threshold epsilon for the singular values. Then, only components for which the corresponding singular value is larger than epsilon are used."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#introduction",
    "href": "assignments/blog-posts/blog-post-image-processing.html#introduction",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Introduction",
    "text": "Introduction\nIn lecture, we discussed the Laplacian spectral clustering algorithm as a method for finding interesting clusters in point cloud data sets by operating on a graph. Spectral clustering doesn’t only work on point clouds, however; we can use it on any data that we can represent as a graph, like social networks.\nHere’s a famous social network:\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\n\n\n\nFigure 1: The karate club graph\n\n\n\n\n\nThis graph is often called the “Karate Club Graph.” Each node (blue dot) represents an individual member of a karate club. Edges between them are measurements of social ties by the researcher Zachary; informally, you can think of two connected nodes as having interacted in a friendly social setting.\nBUT: things didn’t stay friendly for long! The reason that this data set is so famous is that it provides a relatively pure case study of the process of graph fission. In this case, the karate club studied eventually broke into two separate clubs after a conflict between the instructor (“Mr. Hi”) and the club president (“Officer”). Node 0 is Mr. Hi himself, and Node 33 is the club president. This information is present as a node attribute in the data:\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nWe can draw the graph with this data like this, using the node_color attribute to control the node colors.\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\n\n\nFigure 2: The karate club graph with two labeled clubs after fission\n\n\n\n\n\nThe fundamental question of community detection is: can we predict divisions like this based only on the social ties? That is: could we have looked at Figure 1 (NOT Figure 2) and made a guess that the club might split on approximately these lines?\nA bit more abstractly, the community detection problem is to divide an observed graph into interpretable or important components, often called “communities” or “clusters.” There are lots of algorithms for this problem, but one of them is spectral clustering! We can extract an adjacency matrix for the graph like this:\n\nA = nx.adjacency_matrix(G).toarray()"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do-1",
    "href": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do-1",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "What You Should Do",
    "text": "What You Should Do\n\nImplement a function called spectral_clustering that accepts a graph G as an argument and returns a vector of binary labels that split the graph. Show your implementation in your blog post, and include comments describing each of the steps. Your implementation should be relatively short (10 lines is more than enough), but you might need to do a little research into functions like np.linalg.eig in order to better understand how they represent eigenvectors in their return value.\nShow a plot of the graph like the ones above, using the labels that you found with your algorithm.\nDiscuss the extent to which the labels found by your algorithm match the actual club division."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#optional-extras-1",
    "href": "assignments/blog-posts/blog-post-image-processing.html#optional-extras-1",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Optional Extras",
    "text": "Optional Extras\n\nImplement multiway spectral clustering. In multiway spectral clustering, our aim is to split the graph into \\(k\\) pieces, where \\(k &gt; 2\\). Then, demonstrate multiway spectral clustering on the karate club network, or any other network data you can find. Do the results look reasonable to you by eye?\n\nThere are several approaches to this problem. Here’s one way:\n\nRetrieve the \\(k\\) eigenvectors corresponding to the eigenvalues smallest in magnitude.\nTreat these \\(k\\) eigenvectors as defining a matrix \\(\\mathbf{U} \\in \\mathbb{R}^{n\\times k}\\).\nPerform k-means clustering on this matrix with \\(k\\) centroids, and return the corresponding labeling.\n\n\nPropose a measure of similarity between two categorical labelings \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Your measure should be 1 if \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\) split the graph into exactly the same clusters, should be no smaller than 0 in any scenario. Note that a challenge for this problem is that your measure should be permutation invariant: if \\(\\mathbf{z}_1\\) is the same as \\(\\mathbf{z}_2\\) except with the 0s and 1s swapped, they still correspond to the same clustering and should be considered identical. Then, implement your measure and use it quantitatively compare the clusters found by spectral clustering with the true club split.\n\nThis problem is harder than it sounds! But there are lots of good approaches, so please feel free to be creative."
  },
  {
    "objectID": "assignments/blog-posts/solutions/deep-music.html",
    "href": "assignments/blog-posts/solutions/deep-music.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\n\n\n  © Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "$$\n$$\nThe folktables package allows you to download and neatly organize data from the American Community Survey’s Public Use Microdata Sample (PUMS). You can install it in your ml-0451 environment by running the following two commands in your terminal:\nYou can learn more about the folktables package, including documentation and examples, on the package’s GitHub page.\nIn this blog post, you’ll fit a classifier using data from folktables and perform a bias audit for the algorithm.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#using-folktables",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#using-folktables",
    "title": "Auditing Allocative Bias",
    "section": "1 Using folktables",
    "text": "1 Using folktables\nThe first thing to do is to download some data! Here’s an illustration of downloading a complete set of PUMS data for the state of Alabama.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000049\n6\n1\n1600\n3\n1\n1013097\n75\n19\n...\n140\n74\n73\n7\n76\n75\n80\n74\n7\n72\n\n\n1\nP\n2018GQ0000058\n6\n1\n1900\n3\n1\n1013097\n75\n18\n...\n76\n78\n7\n76\n80\n78\n7\n147\n150\n75\n\n\n2\nP\n2018GQ0000219\n6\n1\n2000\n3\n1\n1013097\n118\n53\n...\n117\n121\n123\n205\n208\n218\n120\n19\n123\n18\n\n\n3\nP\n2018GQ0000246\n6\n1\n2400\n3\n1\n1013097\n43\n28\n...\n43\n76\n79\n77\n80\n44\n46\n82\n81\n8\n\n\n4\nP\n2018GQ0000251\n6\n1\n2701\n3\n1\n1013097\n16\n25\n...\n4\n2\n29\n17\n15\n28\n17\n30\n15\n1\n\n\n\n\n5 rows × 286 columns\n\n\n\n\nThere are approximately 48,000 rows of PUMS data in this data frame. Each one corresponds to an individual citizen of the given STATE who filled out the 2018 edition of the PUMS survey. You’ll notice that there are a lot of columns. In the modeling tasks we’ll use here, we’re only going to focus on a relatively small number of features. Here are all the possible features I suggest you use:\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n19\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n18\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n2\n6.0\n\n\n2\n53\n17.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n28\n19.0\n5\n16\n2\nNaN\n1\n1.0\n2.0\n1\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n25\n12.0\n5\n16\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n1.0\n2\n1\n6.0\n\n\n\n\n\n\n\n\nFor documentation on what these features mean, you can consult the appendix of the paper that introduced the package.\nFor a few examples:\n\nESR is employment status (1 if employed, 0 if not)\nRAC1P is race (1 for White Alone, 2 for Black/African American alone, 3 and above for other self-identified racial groups)\nSEX is binary sex (1 for male, 2 for female)\nDEAR, DEYE, and DREM relate to certain disability statuses.\n\nLet’s consider the following task: we are going to\n\nTrain a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race, and\nPerform a bias audit of our algorithm to determine whether it displays racial bias.\n\nFirst, let’s subset the features we want to use:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we can construct a BasicProblem that expresses our wish to use these features to predict employment status ESR, using the race RAC1P as the group label. I recommend you mostly don’t touch the target_transform, preprocess, and postprocess columns. You can find examples of constructing problems in the folktables source code if you really want to carefully customize your problem.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nThe result is now a feature matrix features, a label vector label, and a group label vector group, in convenient format with which we can work.\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)\n\n\nBefore we touch the data any more, we should perform a train-test split:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nNow we are ready to create a model and train it on the training data:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\nWe can then extract predictions on the test set like this:\n\ny_hat = model.predict(X_test)\n\nThe overall accuracy in predicting whether someone is employed is:\n\n(y_hat == y_test).mean()\n\n0.7842193386354123\n\n\nThe accuracy for white individuals is\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7838255977496483\n\n\nThe accuracy for Black individuals is\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7838630806845965\n\n\nWe can also calculate confusion matrices, false positive rates, false negative rates, positive predictive values, prevalences, and lots of other information using tools we’ve already seen."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#what-you-should-do",
    "title": "Auditing Allocative Bias",
    "section": "2 What You Should Do",
    "text": "2 What You Should Do\n\nChoose Your Problem\nChoose a prediction problem (target variable), a list of features, and a choice of group with respect to which to evaluate bias. I would suggest one of the following two possibilities:\n\n(What we just illustrated): predict employment status on the basis of demographics excluding race, and audit for racial bias.\nPredict whether income is over $50K on the basis of demographics excluding sex, and audit for gender bias.\n\n You can also pick the state from which you would like to pull your data.Do not audit for racial bias in VT, as we didn’t have enough Black individuals fill out the PUMS survey. 😬\nFinally, you should choose a machine learning model. While you can use a model like logistic regression that you’ve previously implemented, my suggestion is to use one out of the box from scikit-learn. Some simple classifiers with good performance are:\n\nsklearn.linear_model.LogisticRegression\nsklearn.svm.SVC (support vector machine)\nsklearn.tree.DecisionTreeClassifier (decision tree)\nsklearn.ensemble.RandomForestClassifier (random forest)\n\n\n\nBasic Descriptives\nUse simple descriptive analysis to address the following questions. You’ll likely find it easiest to address these problems when working with a data frame. Here’s some code to turn your training data back into a data frame for easy analysis:\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nUsing this data frame, answer the following questions:\n\nHow many individuals are in the data?\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\nOf these individuals, how many are in each of the groups?\nIn each group, what proportion of individuals have target label equal to 1?\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group labe. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n\n\n\nTrain Your Model\nTrain your model on the training data. Please incorporate a tunable model complexity and use cross-validation in order to select a good choice for the model complexity. Some possibilities:\n\nUse polynomial features with LogisticRegression.\nTune the regularization parameter C in SVC.\nTune the max_depth of in DecisionTreeClassifier and in RandomForestClassifier.\n\n\n\nAudit Your Model\nThen, perform an audit in which you address the following questions (all on test data):\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\nWhat is the positive predictive value (PPV) of your model?\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model?\n\n\n\nBy-Group Measures\n\nWhat is the accuracy of your model on each subgroup?\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup?\n\n\n\nBias Measures\nSee (chouldechova2017fair?) for definitions of these terms. For calibration, you can think of the score as having only two values, 0 and 1.\n\nIs your model approximately calibrated?\nDoes your model satisfy approximate error rate balance?\nDoes your model satisfy statistical parity?\n\n\n\n\nConcluding Discussion\nIn a few paragraphs, discuss the following questions:\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#optional-extras",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#optional-extras",
    "title": "Auditing Allocative Bias",
    "section": "3 Optional Extras",
    "text": "3 Optional Extras\n\nIntersectional Bias?\nAs an optional component of your bias audit, you could consider checking for intersectional bias in your model. For example, is the FNR significantly higher for Black women than it is for Black men or white women?\nTo address this question, you’ll likely find it is easier to work with a data frame again.\n\nimport pandas as pd\ndf = pd.DataFrame(X_test, columns = features_to_use)\ndf[\"group\"] = group_test\ndf[\"label\"] = y_test\n\n\n\nFeasible FNR and FPR Rates\nAs an optional component of your bias audit, you could reproduce Figure 5 in (chouldechova2017fair?) (link). This figure uses Eq. (2.6), fixing the prevalence (proportion of true positive labels) \\(p\\) for each group, as well as a desired PPV that should be the same across both groups. With these numbers fixed, eq. (2.6) then defines a line of feasible FNR and FPR rates, which you could plot. Don’t worry about reproducing the shaded regions unless you really want to."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-limits-of-quantitative.html",
    "href": "assignments/blog-posts/blog-post-limits-of-quantitative.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "The expectations for all blog posts apply!\n\nWhat You Should Do\nQuantitative methods for assessing discrimination and bias include techniques like:\n\nFormal (mathematical) definitions of bias and fairness in terms.\nAudits of machine learning algorithms, including things like confusion matrices and false positive rates.\nStatistical tests of significance for effects related to race, gender, or other protected attributes.\n\nIn a recent speech, Narayanan (2022, 25) asserts that\n\n“currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good.”\n\nIn a carefully-structured essay of approximately 1,500-2,000 words, engage this claim in conversation with the following scholarly sources:\n\nNarayanan’s speech.\nFairness and Machine Learning: Barocas, Hardt, and Narayanan (2023).\nData Feminism: D’Ignazio and Klein (2023).\n3 additional scholarly sources of your choosing.\n\nYour essay should include:\n\nA careful explanation of Narayanan’s position.\nA careful explanation of the uses or benefits of quantitative methods, as described in one of your scholarly sources.\n\nPlease include at least one example of a beneficial study of an algorithm or decision-process using quantitative techniques. Include a careful discussion of which quantitative notion(s) of fairness or lack of discrimination was/were used in the example, in both the technical language of Chapter 3 and the moral language of Chapter 4 of Barocas, Hardt, and Narayanan (2023).\n\n\nA careful explanation of one of the limitations or drawbacks of quantitative methods, described in Narayanan’s speech or one of your scholarly sources.\n\nPlease include at least one example of a limited, misleading, or otherwise disappinting study of an algorithm or decision-process using quantitative techniques. Include a careful discussion of which quantitative notion(s) of fairness or lack of discrimination was/were used in the example, in both the technical language of Chapter 3 and the moral language of Chapter 4 of Barocas, Hardt, and Narayanan (2023).\n\n\nAppropriate supporting points from your other scholarly sources.\nAn argument in which you stake out a position on Narayanan’s claim of view. Do you agree? Disagree? Agree with qualifications? Which ones? Why?\n\n\n\nReferences in Quarto\nAppropriately formatted citations are a fundamental aspect of scholarly writing. A fundamental aspect of technical scholarly writing is learning to manage references using automated tools such as Quarto. Managing references in Quarto is very easy once you have followed the setup below! Blog posts that do not use Quarto’s citation system will receive at most Ms.\nTo manage references in Quarto, you need to create a .bib file (you can call it refs.bib). This file should live in the same directory as your blog post. Your .bib file is essentially a database of document information. Here’s an example of a a refs.bib file:\n@book{hardtPatternsPredictionsActions2022,\n  title = {Patterns, {{Predictions}}, and {{Actions}}},\n  author = {Hardt, Moritz and Recht, Benjamin},\n  year = {2022},\n  publisher = {{Princeton University Press}},\n  isbn = {978-0-691-23372-7},\n  langid = {english}\n}\n\n@book{barocasFairnessMachineLearning2023,\n  title = {Fairness and Machine Learning: Limitations and Opportunities},\n  shorttitle = {Fairness and Machine Learning},\n  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},\n  year = {2023},\n  publisher = {{The MIT Press}},\n  address = {{Cambridge, Massachusetts}}\n}\n\n@misc{narayanan2022limits,\n  author       = {Narayanan, Arvind},\n  howpublished = {Speech},\n  title        = {The limits of the quantitative approach to discrimination},\n  year         = {2022}\n}\nThe simplest way to get entries for your references is to look them up on Google Scholar.\n\nSearch for the document you want.\nClick the “Cite” link underneath and choose “Bibtex” from the options at the bottom.\nCopy and paste the contents of the new page to your refs.bib file.\n\nOnce you’ve assembled your references, add the following line to your document metadata (the stuff in the top cell of your Jupyter notebook)\nbibliography: refs.bib\nOnce you’ve followed these steps, you’re ready to cite! You can reference your documents using the @ symbol and their bibliographic key, which is the first entry for each document in the refs.bib file. For example, typing\n@barocasFairnessMachineLearning2023\nresults in the reference\nBarocas, Hardt, and Narayanan (2023)\nas well as an entry in the “References” section at the end of your blog post, as illustrated below.\nFor more on how to handle citations in Quarto, check the Quarto documentation.\n\n\n\n\n\n  © Phil Chodrow, 2024References\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. Cambridge, Massachusetts: The MIT Press.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2023. Data Feminism. First MIT Press paperback edition. Cambridge, Massachusetts: The Mit Press.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-newton.html",
    "href": "assignments/blog-posts/blog-post-newton.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "The expectations for all blog posts apply!\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-newton.html#introduction",
    "href": "assignments/blog-posts/blog-post-newton.html#introduction",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nThis is an advanced blog post that requires you to have previously completed our blog post on implementing logistic regression. If you haven’t implemented logistic regression yet, go do that first.\n\nPart A: Implement NewtonOptimizer\nNewton’s Method is a second-order optimization technique. This means that it requires information about the second derivatives of the loss function \\(L\\) as well as the first derivatives. Here’s how Newton’s method works:\n\nWe compute the usual gradient \\(\\nabla L(\\mathbf{w})\\). Recall that the gradient is the vector of first derivatives of \\(L\\).\nWe also compute the Hessian matrix, which is the matrix of second derivatives of \\(L\\). For logistic regression, the Hessian is the matrix \\(\\mathbf{H}(\\mathbf{w}) \\in \\mathbb{R}^{p \\times p}\\) with entries \\[\n\\begin{aligned}\nh_{ij}(\\mathbf{w}) = \\sum_{k = 1}^n x_{ki}x_{kj}\\sigma(s_k)(1-\\sigma(s_k))\\;.\n\\end{aligned}\n\\] The ideal way to compute this matrix is to use the formula \\(\\mathbf{H}(\\mathbf{w}) = \\mathbf{X}^T\\mathbf{D}(\\mathbf{w})\\mathbf{X}\\), where \\(\\mathbf{D}\\) is the diagonal matrix with entries \\(d_{kk}(\\mathbf{w}) = \\sigma(s_k)(1-\\sigma(s_k))\\).\nOnce we know how to calculate the gradient and the Hessian, we repeat the update \\[\n\\begin{aligned}\nw \\gets w - \\alpha \\mathbf{H}(\\mathbf{w})^{-1} \\nabla L (\\mathbf{w})\\;.\n\\end{aligned}\n\\] until convergence. Here, \\(\\alpha &gt; 0\\) is a learning rate and \\(\\mathbf{H}(\\mathbf{w})^{-1}\\) is the matrix inverse of the Hessian matrix.\n\nPlease implement a NewtonOptimizer class that can use Newton’s method to estimate \\(\\mathbf{w}\\) for a LogisticRegression model as implemented in our logistic regression blog post. To do so, you’ll also need to extend your LogisticRegression implementation with a new method hessian that computes \\(\\mathbf{H}(\\mathbf{w})\\).\nTo check your implementation, make sure that, on the same data set, for sufficiently small \\(\\alpha\\), Newton’s method converges to the same result that you would get using standard gradient descent.\n\n\nPart B: Perform Experiments\nPerform experiments and include visualizations to show that:\n\nWhen \\(\\alpha\\) is chosen appropriately, Newton’s method converges to the correct choice of \\(\\mathbf{w}\\).\nUnder at least some circumstances, Newton’s method can converge much faster than standard gradient descent, in the sense of decreasing the empirical risk.\nIf \\(\\alpha\\) is too large, Newton’s method fails to converge.\n\n\n\nPart C: Operation Counting\nIn high-dimensional optimization, the number of features \\(p\\) can be very large. This can be a problem for Newton’s method, because the operation of inverting a \\(p\\times p\\) matrix requires \\(O(p^\\gamma)\\) operations for some \\(2 \\leq \\gamma &lt;3\\)  Multiplying the gradient by the Hessian also requires \\(O(p^2)\\) operations.Surprisingly, the exact value of \\(\\gamma\\) is not known. Recent estimates have proven that \\(\\gamma &lt; 2.372\\), while many researchers believe that the true value of \\(\\gamma\\) is \\(\\gamma = 2\\).\nAssume that it costs \\(c\\) computational units to compute the loss \\(L\\), \\(2c\\) units to compute the gradient \\(\\nabla L\\), and \\(pc\\) units to compute the Hessian  Suppose further that it costs \\(k_1p^\\gamma\\) units to invert a \\(p\\times p\\) matrix and \\(k_2p^{2}\\) units to perform the matrix-vector multiplication required by Newton’s method.These assumptions are simplistic but based on standard theory.\nFinally, suppose that Newton’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps, while gradient descent converges to an adequate solution in \\(t_\\mathrm{gd}\\) steps.\nUnder these assumptions, write expressions describing the total computational costs of Newton’s method as compared to gradient descent. How much smaller must \\(t_\\mathrm{nm}\\) be than \\(t_\\mathrm{gd}\\) in order to ensure that Newton’s method will require fewer computational units to complete? When \\(p\\) becomes very large, is using Newton’s method ever going to pay off?\n\n\nPart D: Writing and Finishing Touches\n\nAdd a link to your source code NewtonOptimizer on GitHub at the very top of your blog post.\nAdd expository writing throughout your post, with special focus on carefully describing the purpose of your experiments and what the results mean. Please also make sure that all plots are appropriately labeled with axis labels and legends.\nAdd an “abstract” paragraph in which you describe the overall topic and aims of your blog post.\nAdd a concluding paragraph in which you reflect on what you achieved and what you learned."
  },
  {
    "objectID": "blog-post-expectations.html",
    "href": "blog-post-expectations.html",
    "title": "Expectations for Blog Posts",
    "section": "",
    "text": "The following expectations apply to all submitted blog posts in CSCI 0451.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "blog-post-expectations.html#writing",
    "href": "blog-post-expectations.html#writing",
    "title": "Expectations for Blog Posts",
    "section": "Writing",
    "text": "Writing\nBlog posts are computational essays.\n\nPlease write in thoughtfully composed English paragraphs.\n\nProvided that your intended meaning is unambiguous, spelling and grammar will not be major feedback items and will not be the determining factor about whether revisions are encouraged or expected.\n\nYou should explain all code displayed in your blog post, in such a way that your reader can understand what you are doing.\n\nA corollary is that you should break large code blocks up into smaller ones so that you can explain your approach.\n\nPlease make sure that your writing also responds to all prompts in the body of the blog post assignment.\n\nIn this course, writing is a fundamental part of the learning process. Blog posts that have fully correct technical solutions but which are missing major written components will generally receive feedback of an M (which means “I think you could learn more on this assignment”) rather than an E.\nTip: Quarto offers some neat functionality for marking up your code. Code comments can also be helpful, but do not replace text."
  },
  {
    "objectID": "blog-post-expectations.html#code",
    "href": "blog-post-expectations.html#code",
    "title": "Expectations for Blog Posts",
    "section": "Code",
    "text": "Code\nYour code should be careful, concise, efficient Python.\n\nUnless otherwise stated by the prompt, all your code can be included in your blog post itself (it is not necessary to create a separate module).\nWhenever possible, for-loops should be minimized in favor of efficient vectorized operations in numpy and pandas."
  },
  {
    "objectID": "blog-post-expectations.html#computational-outputs",
    "href": "blog-post-expectations.html#computational-outputs",
    "title": "Expectations for Blog Posts",
    "section": "Computational Outputs",
    "text": "Computational Outputs\n\nAll plots must have labeled axes and legends when appropriate.\nUse of captions and cross-references is encouraged.\nWhen showing a visualization, every aspect of that visualiation should be discussed in the accompanying text. If an aspect of a visualization is not sufficiently important to include in your text, please remove it!"
  },
  {
    "objectID": "blog-post-expectations.html#text-formatting",
    "href": "blog-post-expectations.html#text-formatting",
    "title": "Expectations for Blog Posts",
    "section": "Text Formatting",
    "text": "Text Formatting\n\nMathematical content should be typeset using Quarto’s included MathJax syntax. “alpha x y” is not appropriately typeset; instead, typing $\\alpha \\times y$ produces the mathematics \\(\\alpha \\times y\\).\nAll code should be in code blocks (Quarto will usually take care of this for you). Inline code can be rendered using backticks (`code here`), which results in text that looks like this."
  },
  {
    "objectID": "blog-post-expectations.html#submission-instructions",
    "href": "blog-post-expectations.html#submission-instructions",
    "title": "Expectations for Blog Posts",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nSimply submit a URL to your online blog post in Canvas!\nTo submit a revision, just include another link to your blog post as a comment on the Canvas submission. (Unfortunately your revised posts don’t show up as revised in Canvas). The reason for submitting a comment rather than a new submission is so that it is easy for us all to see the feedback that you were previously given and are revising in response to."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Index of Assignments",
    "section": "",
    "text": "Process Reflections\n\n\n\n    \n        \n            \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            Math Self-Assessment\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 19, 2023\n                    \n                \n                \n                    \n                        We check in on our comfort with mathematical techniques that we'll use later in the course.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Math\n                            \n                          \n                      \n                  \n            \n        \n    \n          \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            Reflective Goal-Setting\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 27, 2024\n                    \n                \n                \n                    \n                        We plan our goals for learning, engagement, and achievement over the course of the semester.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Reflection\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            Mid-Course Reflection\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 2, 2024\n                    \n                \n                \n                    \n                        We reflect on our learning, engagement, and achievement in the first part of the semester.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Reflection\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            End-Of-Course Reflection\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        May 15, 2024\n                    \n                \n                \n                    \n                        We reflect on our learning, engagement, and achievement over the course of the semester.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Reflection\n                            \n                          \n                      \n                  \n            \n        \n    \n\n\n\nNo matching items\n\n\n\n\nBlog Posts\n\n\n\n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Classifying Palmer Penguins\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 22, 2024\n                    \n                \n                \n                    \n                        In this blog post, you'll work through a complete example of the standard machine learning workflow. Your primary goal is to determine the smallest number of measurements necessary to confidently determine the species of a penguin.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Experimentation\n                             \n                                | \n                            \n                        \n                            Navigation\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            'Optimal' Decision-Making\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 29, 2024\n                    \n                \n                \n                    \n                        In this blog post, you will further study optimal decision-making in the context of the the credit-risk prediction problem. Your aim is to (a)determine a score function and threshold that maximize profit for the bank under more realistic assumptions and (b) assess which populations of prospective borrowers are most impacted by your proposed policy.\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Theory\n                             \n                                | \n                            \n                        \n                            Social Responsibility\n                             \n                                | \n                            \n                        \n                            Navigation\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Replication Study: *Dissecting racial bias in an algorithm used to manage the health of populations*\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 8, 2024\n                    \n                \n                \n                    \n                        In this post, you will read a famous scholarly paper on racial bias in a healthcare recommender system; replicate its primary findings; and discuss your results\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Social Responsibility\n                             \n                                | \n                            \n                        \n                            Navigation\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Limits of the Quantitative Approach to Bias and Fairness\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 14, 2024\n                    \n                \n                \n                    \n                        This blog post is actually an essay -- no math or coding is involved. In this blog post, you'll discuss the limits of the quantitative approach to bias and fairness in allocative decision-making.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Social Responsibility\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            The Women in Data Science (WiDS) Conference at Middlebury College\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 14, 2024\n                    \n                \n                \n                    \n                        This blog post involves attending, reporting on, and reflecting on the Women in Data Science (WiDS) Conference at Middlebury College on March 4th, 2024.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Social Responsibility\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Implementing the Perceptron Algorithm\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 28, 2024\n                    \n                \n                \n                    \n                        In this blog post, you'll implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Implementation\n                             \n                                | \n                            \n                        \n                            Navigation\n                             \n                                | \n                            \n                        \n                            Experimentation\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Implementing Logistic Regression\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 4, 2024\n                    \n                \n                \n                    \n                        In this blog post, you'll implement a generalized form of gradient descent for logistic regression.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Theory\n                             \n                                | \n                            \n                        \n                            Implementation\n                             \n                                | \n                            \n                        \n                            Experimentation\n                            \n                          \n                      \n                  \n            \n        \n    \n        \n            \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Newton's Method for Logistic Regression\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 11, 2024\n                    \n                \n                \n                    \n                        In this blog post, you'll implement Newton's method for even faster logistic regression.\n\n                    \n                \n                \n            \n                \n                    \n                        Learning Objectives: \n                        \n                        \n                            Theory\n                             \n                                | \n                            \n                        \n                            Implementation\n                             \n                                | \n                            \n                        \n                            Experimentation\n                            \n                          \n                      \n                  \n            \n        \n    \n          \n            \n        \n    \n          \n            \n        \n    \n\n\n\nNo matching items\n\n\n\n\nProject\n\n\n\n\n\n\n\nProject\nProject Presentation\nBest By:  \nMay 10, 2023\n\n\nYour project presentation is a brief oral description of what you achieved in your project.\n\n\nLearning Objectives:  Project\n\n\n\nProject\nProject Blog Post\nBest By:  \nMay 14, 2023\n\n\nYour project blog post is a written description of what you achieved in your project and a reflection on what you learned.\n\n\nLearning Objectives:  Project\n\n\n\nProject\nProject Proposal\nBest By:  \nApr 9, 2024\n\n\n\n\n\nLearning Objectives:  Project\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n  © Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-erm.html",
    "href": "assignments/blog-posts/blog-post-erm.html",
    "title": "Empirical Risk Minimization For Classification",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n\n\n1 Introduction\nLet’s do empirical risk minimization!\n\n\n\n\n  © Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html",
    "href": "assignments/blog-posts/blog-post-perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\nThe expectations for all blog posts apply!\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#getting-started",
    "href": "assignments/blog-posts/blog-post-perceptron.html#getting-started",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Getting Started",
    "text": "Getting Started\n\nPlease create a new blog post. In addition to the usual .ipynb file, please also create a script called perceptron.py in the same directory. This is the file in which you will implement the perceptron algorithm.\nThen, in your .ipynb notebook file, place the following in a Python code block underneath your metadata block:\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nThe two commands beginning with % will cause your notebook to automatically re-read the contents of your perceptron.py file, so that changes you make in that file will be automatically and immediately reflected when you run cells in your blog notebook."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#implement-the-perceptron-algorithm",
    "href": "assignments/blog-posts/blog-post-perceptron.html#implement-the-perceptron-algorithm",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Implement the Perceptron Algorithm",
    "text": "Implement the Perceptron Algorithm\nNow it is time to implement the perceptron algorithm. I am going to guide you through doing this in a relatively specific way that will help us generalize when we work with more complex algorithms. Your implementation of the perceptron algorithm should be contained in the file perceptron.py. It is not part of your blog post, but you will link to it later.\n\nLinearModel and Perceptron.loss()\nIf you haven’t already, implement the methods of the LinearModel class as described in this warmup. Please also implement Perceptron.loss().\n\n\nPerceptronOptimizer.step()\nNow implement the step() method of PerceptronOptimizer. In this method, you should assume that Perceptron.grad() correctly returns the “update” part of the perceptron update. You’ll implement Perceptron.grad() soon. In this function, do two things:\n\nCall self.model.loss(). We’ll see why when we get to different classes of models.\nThen, call self.model.grad() and add the result to self.model.w. \n\nRecall that PerceptronOptimizer’s __init__ method saves a Perceptron object to the instance variable self.model.\n\nPerceptron.grad()\nFinally, it is time to implement Perceptron.grad(). This is where the math comes in – turning the math of the perceptron algorithm into code. In this part, you may assume that the argument X of Perceptron.grad() is a single row of the feature matrix.\nInside Perceptron.grad(), you should do two things:\n\nCompute the score \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i\\rangle\\).\nReturn the vector \\(\\mathbb{1}\\left[s_i y_{i} &lt; 0 \\right] y_{i} \\mathbf{x}_{i}\\)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#check-your-implementation",
    "href": "assignments/blog-posts/blog-post-perceptron.html#check-your-implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Check Your Implementation",
    "text": "Check Your Implementation\nYour code is probably working when you can run the “minimal training loop” code from this section of the notes and eventually achieve loss = 0 on linearly separable data. You should do this in your .ipynb file as part of your blog post. You can use the functions supplied in those notes to generate and visualize the data (when the data is 2d)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#code-quality",
    "href": "assignments/blog-posts/blog-post-perceptron.html#code-quality",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Code Quality",
    "text": "Code Quality\nAn excellent solution will be no more than 30 lines of code in total (across all three classes) and will contain no loops."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#minibatch-perceptron-experiments",
    "href": "assignments/blog-posts/blog-post-perceptron.html#minibatch-perceptron-experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Minibatch Perceptron Experiments",
    "text": "Minibatch Perceptron Experiments\nPlease perform experiments and create visualizations to demonstrate the following:\n\nWhen k = 1, minibatch perceptron performs similarly to regular perceptron.\nWhen k = 10, minibatch perceptron can still find a separating line in 2d.\nWhen k = n (that is, the batch size is the size of the entire data set), minibatch perceptron can converge even when the data is not linearly separable, provided that the learning rate \\(\\alpha\\) is small enough."
  },
  {
    "objectID": "assignments/blog-posts/solutions/logistic/experiments.html",
    "href": "assignments/blog-posts/solutions/logistic/experiments.html",
    "title": "",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\n\nfrom logistic import LogisticRegression, gradient, stochastic_gradient\n\n# for step in [gradient, stochastic_gradient]:\n\n# LR = LogisticRegression()\n# LR.fit_stochastic(X, y, k = 1, max_iter = 10000)\n# plt.plot(LR.history, label = \"stochastic gradient\")\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = True, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = False, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05)\nplt.plot(LR.history, label = \"gradient\")\nplt.loglog()\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nw = LR.w\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  © Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-adam.html",
    "href": "assignments/blog-posts/blog-post-adam.html",
    "title": "Optimization with Adam",
    "section": "",
    "text": "This is a relatively theoretical blog post on which I am intentionally giving you relatively little guidance. The purpose is to offer a challenge for students who have already done most of the previous blog posts. If you have already completed most of the previous posts, then I encourage you to give this one a try! Otherwise, I suggest that you instead focus on completing some of the prior posts.\nThe Adam optimization algorithm is a mainstay of modern deep learning. You can think of Adam as fancy gradient descent. It still uses gradient information, but processes that information in a more complex way that often produces state-of-the-art performance in modern large-scale tasks.\nAdam was introduced by (kingma2014adam?), in a paper which has been cited over 140,000 times.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-adam.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-adam.html#what-you-should-do",
    "title": "Optimization with Adam",
    "section": "1 What You Should Do",
    "text": "1 What You Should Do\n\nRead the Adam Paper\nStart by reading the paper that introduced Adam. You can focus on sections 1, 2, 3, and 6.1, skipping the others. It’s ok for your first reading to be relatively quick; you’ll want to go back through the sections multiple times as you do your implementation and experiments.\n\n\nImplement Adam for Logistic Regression\nExpand your implementation of logistic regression to include a version of the fit method that uses the Adam algorithm (Algorithm 1). If you implemented gradient as a separate function, you can just use that function within your implementation of Adam.\nThe authors of the paper frame their algorithm as minimizing a stochastic (random) objective function \\(f\\). This is a fancy mathematical way to talk about stochastic batch gradient descent. When they talk about evaluating/differentiating the “stochastic function” \\(f\\), you can instead think of “evaluating/differentiating \\(f\\) on a subset of the data,” just like we did in gradient descent for logistic regression. So, you can think of \\(g_t\\) in their algorithm as the gradient evaluated on a batch of the data. You can still follow the standard structure of stochastic gradient descent in which you loop through the entire data set in one epoch, reshuffle the batches, and do it again.\nYour implementation should allow the user to pass five arguments:\n\nbatch_size, the batch size for computing gradients. This works in the same way as it did in our initial implementation of stochastic gradient descent, and you can use much of the same code.\nalpha, the step-size.\nbeta_1 and beta_2, the moment estimate decay rates.\nw_0, the initial guess for the weight vector. Personally I would suggest giving this argument a default None value and, in the case that the user does not pass w_0, initialize it randomly with the correct dimensions.\n\n\n\nPerform a Basic Experiment\nRedo some of the simple experiments from implementation of logistic regression. Compare Adam optimization to standard stochastic gradient descent with a few different parameter choices. Please measure both the number of epochs and the actual amount of time required to achieve convergence.\n\n\nPerform a Digits Experiment\nWe saw an example of loading and manipulating the digits data set in the reading on k-means.\nLoad the digits dataset from scikit-learn. Then, do one of two things:\n\nFilter the digits data set so that it only contains data with two class labels (e.g. 4s vs. 8s).\nOptional challenge: extend your implementation of logistic regression to handle multiple class labels. Doing this by hand is quite challenging and I only recommend it if you are feeling very ambitious. You might be able to get some inspiration from this implementation, though please don’t copy code and acknowledge the author if you find her example helpful.\n\nThen, do a version of the experiment in Section 6.1 of the Adam paper in which you again compare the efficiency of your Adam implementation against standard stochastic gradient descent in terms of both epochs and elapsed time.\n\n\nPerform One More Experiment\nFind a data set (any data set that interests you) on which you can perform classification with your Adam model. I recommend that the number of features not be very large (maybe no larger than 100). Then, again compare the performance of Adam to standard stochastic gradient descent on the data you found.\n\n\nWrite Your Post\nIn your written blog post, please:\n\nShow your Adam implementation. Include comments corresponding to the comments in Alg. 1 of the paper so that the reader understands which lines of code correspond to which lines of math.\nDescribe and discuss the findings from your experiments.\nDon’t forget to label your axes!!\nInclude an introductory section describing the purpose of the blog post and a summary section reflecting on your findings. In your summary paragraph, please include answers to the following questions:\n\nIn the authors’ assessment, what aspects of the Adam algorithm help it to converge so efficiently? (You don’t need to go deep into the math but you should be able to identify a qualitative features from their paper).\nDid you observe highly efficient performance in your experiments when compared to other methods? If not, can you offer a hypothesis about why?\nHow did the experience of implementing this algorithm compare to the experience of implementing standard stochastic gradient descent?\n\n\nFinally, don’t forget to label your axes!!"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "The expectations for all blog posts apply!\nIn two sets of lecture notes, we studied the theory of making (binary) decisions based on a linear score function. We illustrated the theory in the context of credit-risk prediction, in which we attempted to identify prospective borrowers who are most likely to default on loans. For the purposes of illustrating the theory, our analysis in those lecture notes was very simple. In particular, we used only two features, and did not make a major attempt to find an optimal vector of weights \\(\\mathbf{w}\\).\nYou have two aims in this blog post:\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-a-grab-the-data",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-a-grab-the-data",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part A: Grab the Data",
    "text": "Part A: Grab the Data\nStart by downloading the training data:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nThe columns in this data are:\n\nperson_age, the age of the prospective borrower.\nperson_income, the income of the prospective borrower at time of application.\n\nperson_home_ownership, the home ownership status of the prospective borrower at time of application. Possible values are MORTGAGE, OWN, RENT, and OTHER.\nperson_emp_length, the length of most recent employment for the prospective borrower, in years.\nloan_intent, the purpose of the loan request.\nloan_grade, a composite measure of the likelihood of the borrower to repay the loan. You may not use this column in your score function in Part B.\nloan_amnt, the amount of the loan.\nloan_int_rate, the annual interest rate on the loan.\nloan_status, whether or not the borrower defaulted on the loan. This is the target variable.\nloan_percent_income, the amount of the loan as a percentage of the prospective borrower’s personal income.\ncb_person_default_on_file, whether the prospective borrower has previously defaulted on a loan in the records of a credit bureau.\ncb_person_cred_hist_length, the length of credit history of the prospective borrower."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-b-explore-the-data",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-b-explore-the-data",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part B: Explore The Data",
    "text": "Part B: Explore The Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\n\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-c-build-a-model",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-c-build-a-model",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part C: Build a Model",
    "text": "Part C: Build a Model\nPlease use any technique to construct a score function and threshold for predicting whether a prospective borrower is likely to default on a given loan. You may use all the features in the data except loan_grade (and the target variable loan_status), and you may choose any subset of these. There are several valid ways to approach this modeling task:\n\nChoose features and estimate entries of a weight vector \\(\\mathbf{w}\\) by hand (this is allowed but not recommended).\n(Recommended): Choose your features, estimate new ones if needed, and fit a score-based machine learning model to the data. My suggestion is LogisticRegression. Once you have fit a logistic regression model, the weight vector \\(\\mathbf{w}\\) is stored as the attribute model.coef_.\n\nI suggest that you try several combinations of features, possibly including some which you create, and test out which combinations work best with cross-validation."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-d-find-a-threshold",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-d-find-a-threshold",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part D: Find a Threshold",
    "text": "Part D: Find a Threshold\nOnce you have a weight vector \\(\\mathbf{w}\\), it is time to choose a threshold \\(t\\). To choose a threshold that maximizes profit for the bank, we need to make some assumptions about how the bank makes and loses money on loans. Let’s use the following (simplified) modeling assumptions:\n\nIf the loan is repaid in full, the profit for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt. This formula assumes that the profit earned by the bank on a 10-year loan is equal to 25% of the interest rate each year, with the other 75% of the interest going to things like salaries for the people who manage the bank. It is extremely simplistic and does not account for inflation, amortization over time, opportunity costs, etc.\nIf the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt. This formula corresponds to the same profit-earning mechanism as above, but assumes that the borrower defaults three years into the loan and that the bank loses 70% of the principal.\n\nThese modeling assumptions are extremely simplistic! You may deviate from these assumptions if you have relevant prior knowledge to inform your approach!!\nBased on your assumptions, determine the threshold \\(t\\) which optimizes profit for the bank on the training set. Explain your approach, including labeled visualizations where appropriate, and include a final estimate of the bank’s expected profit per borrower on the training set."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-e-evaluate-your-model-from-the-banks-perspective",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part E: Evaluate Your Model from the Bank’s Perspective",
    "text": "Part E: Evaluate Your Model from the Bank’s Perspective\nOnly after you have finalized your weight vector \\(\\mathbf{w}\\) and threshold \\(t\\), evaluate your automated decision-process on the test set:\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nWhat is the expected profit per borrower on the test set? Is it similar to your profit on the training set?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-f-evaluate-your-model-from-the-borrowers-perspective",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part F: Evaluate Your Model From the Borrower’s Perspective",
    "text": "Part F: Evaluate Your Model From the Borrower’s Perspective\nNow evaluate your model from the (aggregate) perspective of the prospective borrowers. Please quantitatively address the following questions, using the predictions of your model on the test data:\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-g-write-and-reflect",
    "href": "assignments/blog-posts/blog-post-cost-of-profit-maximization.html#part-g-write-and-reflect",
    "title": "‘Optimal’ Decision-Making",
    "section": "Part G: Write and Reflect",
    "text": "Part G: Write and Reflect\nWrite a brief introductory paragraph for your blog post describing the overall purpose, methodology, and findings of your study. Then, write a concluding discussion describing what you found and what you learned through from this blog post.\nPlease include one paragraph discussing the following questions:\n\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\n\nYou are free to define “fairness” in a way that makes sense to you, but please write down your definition as part of your discussion."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-guest-speaker.html",
    "href": "assignments/blog-posts/blog-post-guest-speaker.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "This is an unusual blog post:\n\nThis blog post has two parts. You can do both of them in the same post, but you need to submit each one on Canvas.\nThe first part, in which you create questions for our guest speaker, has a hard deadline of Wednesday, April 19th. Posts with a first part submitted after Wednesday, April 19th cannot be used as evidence of learning or course engagement.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-guest-speaker.html#tldr",
    "href": "assignments/blog-posts/blog-post-guest-speaker.html#tldr",
    "title": "Learning from Timnit Gebru",
    "section": "tl;dr",
    "text": "tl;dr\n\nRead about Dr. Gebru.\nWatch Dr. Gebru’s recorded talk.\nPropose a question for Dr. Gebru based on her talk.\n\nSubmit a blog post with these proposed questions by April 19th.\n\nAttend Dr. Gebru’s talk at Middlebury on April 24th.\n\nI will record the talk for those who are not able to attend. You must let me know in advance that you will not be able to attend Dr. Gebru’s talk and why.\n\nWrite a reflection on Dr. Gebru’s talk.\n\nSubmit your blog post again with this reflection."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-guest-speaker.html#part-1-questions-for-dr.-gebru",
    "href": "assignments/blog-posts/blog-post-guest-speaker.html#part-1-questions-for-dr.-gebru",
    "title": "Learning from Timnit Gebru",
    "section": "Part 1: Questions for Dr. Gebru",
    "text": "Part 1: Questions for Dr. Gebru\nIn class on April 24th, we will have an opportunity to do Q&A with Dr. Gebru about her recent work in AI and tech ethics. The purpose of Part 1 is to help you get ready for this conversation by learning about Dr. Gebru, getting a quick view of her recent work, and proposing questions for our conversation with her.\n\nRead about Dr. Gebru\nTake 30 minutes to do a little bit of research about Dr. Gebru. Starting with her Wikipedia page is a good start.\nIn your blog, write an introductory paragraph in which you explain what is happening at Midd (Dr. Gebru is virtually visiting our class and giving a talk to the campus). Include in this paragraph an explanation of who Dr. Gebru is and why she is such a recognized voice in artificial intelligence and its ethical implications.\n\n\nWatch Dr. Gebru’s Talk\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition 2020. Watch the recording of this talk.\nIn your blog, write another few paragraphs summarizing the primary points raised by Dr. Gebru. Conclude this section of your blog with a tl;dr to your friend: in one sentence, what is the thing that everyone needs to understand about computer vision as it’s used today?There is no “one thing” that Dr. Gebru discusses; your tl;dr should be based on what you learned from her talk in combination with your judgment.\n\n\nPropose Questions\nReflect carefully and propose a question that you’d like to ask Dr. Gebru. Take some time for this.\n\nYour question should not be answerable directly from her talk.\nYour queston should not be easily answerable with a Google search (try it first).\n\nIn your blog, write your proposed question.\n\n\nSubmit Your Blog Post\nRender your post, push it online, and submit it on Canvas like usual. I will compile your questions and make selections for our discussion with Dr. Gebru. If I choose your question to ask, then I’ll let you know and you’ll be called on to ask it during our conversation with her."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-guest-speaker.html#part-2-dr.-gebrus-talk-at-middlebury",
    "href": "assignments/blog-posts/blog-post-guest-speaker.html#part-2-dr.-gebrus-talk-at-middlebury",
    "title": "Learning from Timnit Gebru",
    "section": "Part 2: Dr. Gebru’s Talk “At” Middlebury",
    "text": "Part 2: Dr. Gebru’s Talk “At” Middlebury\nIn the evening of April 24th, at 7pm in Hillcrest 103, Dr. Gebru will give a talk on “Eugenics and the Promise of Utopia through Artificial General Intelligence.” To complete this part of the blog post, you must attend this talk. I will record the talk for students who are not able to attend. If you are not able to attend, you must inform me ahead of time and state the reason. I know, I don’t like to micromanage your time like this, but it’s a matter of showing respect to an important and accomplished speaker.\n\nAttend and Take Some Notes on the Talk\nBring a pencil and paper or a device and take some notes for yourself on Dr. Gebru’s talk.\n\n\nReflect on Dr. Gebru’s Talk\nIn your blog, write a few paragraphs in which you summarize Dr. Gebru’s argument. In the last of these paragraphs, include some of your own perspective. Do you agree? Disagree? What do you feel people should learn from Dr. Gebru’s discussion?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-guest-speaker.html#part-3-reflect-on-the-process",
    "href": "assignments/blog-posts/blog-post-guest-speaker.html#part-3-reflect-on-the-process",
    "title": "Learning from Timnit Gebru",
    "section": "Part 3: Reflect on the Process",
    "text": "Part 3: Reflect on the Process\nIn your blog, close out with a final paragraph reflecting on your time interacting with Dr. Gebru and her work. What did you learn from the experience? What did you find fun, exciting, or empowering? What did you find frustrating, challenging, or discouraging? What are you curious about now?\nFinally, render your blog post and submit it again on Canvas."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html",
    "href": "assignments/blog-posts/blog-post-penguins.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The expectations for all blog posts apply!\nThe Palmer Penguins data set is a data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. It was first published by Gorman, Williams, and Fraser (2014) and was nicely packaged and released for use in the data science community by Horst, Hill, and Gorman (2020). The data contains physiological measurements for a number of individuals from each of three species of penguin:\nYou can access the (training) data like this:\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\nHere’s how the data looks:\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#your-challenge",
    "href": "assignments/blog-posts/blog-post-penguins.html#your-challenge",
    "title": "Classifying Palmer Penguins",
    "section": "Your Challenge",
    "text": "Your Challenge\nWe are going to consider the problem of predicting the species of a penguin based on its measurements.\n\nExplore: Construct at least two interesting displayed figure (e.g. using seaborn) and at least one interesting displayed summary table (e.g. using pandas.groupby().aggregate). Make sure to include a helpful discussion of both the figure and the table. Don’t just show the result: explain what you learned about the data from these products.\nModel: Find three features of the data and a model trained on those features which achieves 100% testing accuracy. You must obtain your three features through a reproducible process. That is, you can’t just pick them: you need to code up some kind of search in order to obtain them.\n\nOne feature must be qualitative (like Island or Clutch Completion).\nThe other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\n\nEvaluate: Show the decision regions of your finished model, split out by the qualitative feature.\n\nI’ve supplied code to help you with several parts of this task."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#resources-and-hints",
    "href": "assignments/blog-posts/blog-post-penguins.html#resources-and-hints",
    "title": "Classifying Palmer Penguins",
    "section": "Resources and Hints",
    "text": "Resources and Hints\n\nThis Webpage Runs\nIf you run all the code on this assignment page in order, you’ll produce the result at the bottom of the page (possibly after installing some more packages in your ml-0451 Anaconda environment). So, one good way to approach this assignment is to take this code into a Jupyter notebook and start tweaking.\n\n\nData Preparation\nYou will need to prepare the qualitative columns in the data. Categorical feature columns like Sex and Island should be converted into so-called “one-hot encoded” 0-1 columns using the pd.get_dummies function. The label column Species should be coded differently, using a LabelEncoder. The following function handles this work for you.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nExplore\nPlease create two interesting visualizations of the data, and one summary table (e.g. compute the average or median value of some features, by group). Your visualizations and table should:\n\nInclude axis labels and legends.\nHelp you draw conclusions about what features you are going to try using for your model.\nBe accompanied by discussion of what you learned and how you will use it in modeling. Most figures and tables are worth at least one short paragraph of discussion.\n\n\n\nChoosing Features\nThis is where much of the work for this blog post lies. You need to choose 3 good features! One possibility is to use some of the tools described on this page. Another approach, which is ok to use on this data set, is exhaustive search of all the features contained in the data set. For this, the combinations function from the itertools package might be helpful.\nUSE CROSS-VALIDATION! This is your simplest way to guard against overfitting issues and get a good feeling for how your model might do on unseen data.\nIf you use the Island feature, you are allowed to use all of the columns that correspond to Island.\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n\n\nModel Choices\nThere are three species of penguin in the data. Most classifiers in scikit-learn will handle multi-way classification without issue. For example:\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.63671875\n\n\nEven though y_train contains three categories (labeled 0, 1, and 2), we’re able to fit a LogisticRegression() no problem.\nSince scikit-learn makes it so easy to experiment, this blog post is a great opportunity to explore some out-of-the-box models. I’d suggest:\n\nfrom sklearn.tree import DecisionTreeClassifier. This one has a max_depth parameter that controls the complexity of the model. Use cross-validation to find a good value of the parameter.\nfrom sklearn.ensemble import RandomForestClassifier. State-of-the-art before the rise of neural networks.\nfrom sklearn.svm import SVC. Another state-of-the-art algorithm before the rise of neural networks. Has a parameter gamma that controls the complexity of the model. Again, use cross-validation to select gamma. It’s important to let gamma cover a wide range of values, e.g. gamma = 10**np.arange(-5, 5).\n\nYou can find a more thorough listing of models on this page.\n\n\nTesting\nTo test your model, you should download the test data set and prepare it using the prepare_data function. You’ll need to make sure that you subset it using only the features you choose. Here’s code that does this:\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.7794117647058824\n\n\n\n\nPlotting Decision Regions\nNow we are going to plot a panel of decision regions for your classifier. You can use the plot_regions function defined below to plot your regions. This function assumes that your first two columns in X_train are quantitative and that the columns after that are one-hot qualitative columns.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nHere it is in action, visualized on the training data with a logistic regression classifier. For your blog post, I recommend visualizing decision regions on both the training and test sets, with the classifier of your choice.\n\nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nPlease show a confusion matrix for your model, evaluated on the test set. Comment on the confusion matrix. Are certain types of errors more likely than others?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#explore-1",
    "href": "assignments/blog-posts/blog-post-penguins.html#explore-1",
    "title": "Classifying Palmer Penguins",
    "section": "Explore!",
    "text": "Explore!\nPlease feel encouraged to be creative in your choices of data visualization, predictive model, and algorithm to compute your features. I also like pictures of penguins. =)"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#abstract-and-discussion",
    "href": "assignments/blog-posts/blog-post-penguins.html#abstract-and-discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Abstract and Discussion",
    "text": "Abstract and Discussion\nPlease add an introductory “abstract” section to your blog post describing the high-level aims of of your analysis and an overview of your findings. The abstract should be no more than one paragraph. Then, add a closing “discussion” section of your blog post in which you summarize your findings and describe what you learned from the process of completing this post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#useful-resources",
    "href": "assignments/blog-posts/blog-post-penguins.html#useful-resources",
    "title": "Classifying Palmer Penguins",
    "section": "Useful Resources",
    "text": "Useful Resources\n\nAn introduction to seaborn, a convenient package for data visualization with data frames.\nData Manipulation with Pandas from the Python Data Science Handbook\nYou might be interested in some of the explanations of how some other classifiers work, including decision trees and support vector machines, also from the Python Data Science Handbook."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-medical-bias.html",
    "href": "assignments/blog-posts/blog-post-medical-bias.html",
    "title": "Bias in a Medical Recommender System",
    "section": "",
    "text": "© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html",
    "href": "assignments/blog-posts/blog-post-optimization.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\nThe expectations for all blog posts apply!\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#introduction",
    "href": "assignments/blog-posts/blog-post-optimization.html#introduction",
    "title": "Implementing Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nRecently, we introduced the gradient descent algorithm for solving the empirical risk minimization problem. We also calculated the gradient of the loss function for logistic regression.\nIn this blog post you will:\n\nImplement gradient descent for logistic regression in an object-oriented paradigm.\nImplement a key variant of gradient descent with momentum in order to achieve faster convergence.\nPerform experiments to test your implementations."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#part-a-implement-logistic-regression",
    "href": "assignments/blog-posts/blog-post-optimization.html#part-a-implement-logistic-regression",
    "title": "Implementing Logistic Regression",
    "section": "Part A: Implement Logistic Regression",
    "text": "Part A: Implement Logistic Regression\n\nGetting Started\n\nPlease create a new blog post. In addition to the usual .ipynb file, please also create a script called logistic.py in the same directory. This is the file in which you will implement logistic regression.\nThen, in your .ipynb notebook file, place the following in a Python code block underneath your metadata block:\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nIf you complete the optional implementation of Newton’s Method, you’ll also need to import NewtonOptimizer.\n\n\nImplement LinearModel and LogisticRegression()\nIf you haven’t already, implement the methods of the LinearModel class as described in this warmup. Then, define a new class called LogisticRegression which inherits from LinearModel. This class should have two methods:\n\nLogisticRegression.loss(X, y) should compute the empirical risk \\(L(\\mathbf{w})\\) using the logistic loss function  \\[\n\\begin{aligned}\n  L(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left[-y_i \\log \\sigma(s_i) - (1-y_i)\\log (1-\\sigma(s_i))\\right]\n\\end{aligned}\n\\] The weight vector \\(\\mathbf{w}\\) used for this calculation should be stored as an instance variable of the class.\nLogisticRegression.grad(X, y) should compute the gradient of the empirical risk \\(L(\\mathbf{w})\\). You can use the formula for the gradient supplied in the lecture notes on gradient descent.\n\nAs usual, \\(s_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\) and \\(\\sigma(s) = \\frac{1}{1 + e^{-s}}\\).For an M, you can implement LogisticRegression.grad using a for-loop. For an E, your solution should involve no explicit loops. While working on a solution that avoids loops, you might find it useful to at some point convert a tensor v with shape (n,) into a tensor v_ with shape (n,1). The code v_ = v[:, None] will perform this conversion for you.\n\n\nImplement GradientDescentOptimizer\nNext, implement a GradientDescentOptimizer class. For this project, we are going to implement gradient descent with momentum, also known as Spicy Gradient Descent. Let \\(\\mathbf{w}_k\\) be the estimate of the weight vector at algorithmic step \\(k\\). Gradient descent with momentum performs the update  \\[\n\\begin{aligned}\n    \\mathbf{w}_{k+1} \\gets \\mathbf{w}_k - \\alpha \\nabla L(\\mathbf{w}_k) + \\beta(\\mathbf{w}_k - \\mathbf{w}_{k-1})\n\\end{aligned}\n\\tag{1}\\]This description of gradient descent with momentum is based on Hardt and Recht (2022).\nHere, \\(\\alpha\\) and \\(\\beta\\) are two learning rate parameters. When \\(\\beta = 0\\) we have “regular” gradient descent. In practice, a choice of \\(\\beta \\approx 0.9\\) or so is common. Implementing the momentum term isn’t too complex – you’ll just need to create a new instance variable to hold the previous value of \\(\\mathbf{w}\\) as well as the current one."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#part-b-experiments",
    "href": "assignments/blog-posts/blog-post-optimization.html#part-b-experiments",
    "title": "Implementing Logistic Regression",
    "section": "Part B: Experiments",
    "text": "Part B: Experiments\n\nExperimental Data\nHere is some code to generate data for a classification problem. You can control the number of points by adjusting n_points, the number of features by adjusting p_dims, and the difficulty of the classification problem by adjusting the noise (higher noise is a harder problem).\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\n\nHow to Train Your Model\nOnce you’ve correctly implemented logistic regression and gradient descent, you can do a gradient descent loop like this:\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(100):\n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\nExperiments\nPlease perform experiments, with careful written explanations, that demonstrate the following statements:\n\nVanilla gradient descent: When \\(p_dim = 2\\), when \\(\\alpha\\) is sufficiently small and \\(\\beta = 0\\), gradient descent for logistic regression converges to a weight vector \\(\\mathbf{w}\\) that looks visually correct (plot the decision boundary with the data). Furthermore, the loss decreases monotonically (plot the loss over iterations).\n\nThis is a good experiment to use to assess whether your implementation in Part A has bugs.\n\nBenefits of momentum: On the same data, gradient descent with momentum (e.g. \\(\\beta = 0.9\\)) can converge to the correct weight vector in fewer iterations than vanilla gradient descent (with \\(\\beta = 0\\)). Plot the loss over iterations for each method. You may need to experiment with the data and choice of \\(\\alpha\\) in order to observe speedups due to momentum.\nOverfitting: Generate some data where p_dim &gt; n_points. For example, p_dim = 100 and n_points = 50. Do this twice with the exact same parameters. Call the first dataset X_train, y_train and the second dataset X_test, y_test. Then, do an experiment in which you fit a logistic regression model to the data X_train, y_train and obtain 100% accuracy on this training data. What is the accuracy on the test data?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#part-c-writing",
    "href": "assignments/blog-posts/blog-post-optimization.html#part-c-writing",
    "title": "Implementing Logistic Regression",
    "section": "Part C: Writing",
    "text": "Part C: Writing\nPlease:\n\nPlease include informative comments throughout your source code and a thorough docstring for each method in LogisticRegression and GradientDescentOptimizer.\nPlease add careful expository writing throughout your blog post. You should describe each experiment and what it is intended to illustrate. You should also ensure that all your plots are legible and have appropriate axis labels and legends.\nAt the beginning of your blog post, please place a link to your source code logistic.py on GitHub. After this link, please write an abstract paragraph describing the topic of your post and giving a brief overview of the experiments you performed.\nAt the conclusion of your blog post, please write a discussion paragraph reminding the reader of what you did and what you learned while doing it."
  },
  {
    "objectID": "assignments/project/blog-post.html",
    "href": "assignments/project/blog-post.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "Wow! You completed an awesome project with your group. It’s time for you to tell the world (and especially me) about it on your blog.\nYour project blog post is the authoritative description of what you achieved and what you learned. It should describe all the achievement, effort, and learning that you want us to factor in to your final grade in the course. Your blog post should have eight sections, which are described below.\nYou can write the first seven sections of your blog post as a group and all post them on your separate blogs. If you take this path, you should just make sure to note it. The final section should be completed individually.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/project/blog-post.html#abstract",
    "href": "assignments/project/blog-post.html#abstract",
    "title": "Project Blog Post",
    "section": "1 Abstract",
    "text": "1 Abstract\nYour abstract is a one-paragraph summary of the problem you addressed, the approach(es) that you used to address it, and the big-picture results that you obtained.\nAt the end of your abstract, please include a link to the GitHub repository that houses your project code and other deliverables."
  },
  {
    "objectID": "assignments/project/blog-post.html#introduction",
    "href": "assignments/project/blog-post.html#introduction",
    "title": "Project Blog Post",
    "section": "2 Introduction",
    "text": "2 Introduction\nYou may be able to recycle some content from your project proposal for your introduction.\nYour introduction should describe the big-picture problem that you aimed to address in your project. What’s the problem to be solved, and why is it important? Who has tried solving this problem already, and what did they do? I would expect most introductions to reference no fewer than 2 scholarly studies that attempted similar tasks, although 5 is probably a better target. When citing scholarly studies in a blog post, please use Quarto citations. For a quick overview, see the appendix on references in Quarto."
  },
  {
    "objectID": "assignments/project/blog-post.html#values-statement",
    "href": "assignments/project/blog-post.html#values-statement",
    "title": "Project Blog Post",
    "section": "3 Values Statement",
    "text": "3 Values Statement\nYour values statement should be a few paragraphs that address the following set of questions:\n\nWho are the potential users of your project? Who, other than your users, might still be affected by your project?\nWho benefits from technology that solves the problem you address?\nWho could be harmed from technology that solves the problem you well address?\nWhat is your personal reason for working on this problem?\nBased on your reflection, would the world be a more equitable, just, joyful, peaceful, or sustainable place based on the technology that you implemented?"
  },
  {
    "objectID": "assignments/project/blog-post.html#materials-and-methods",
    "href": "assignments/project/blog-post.html#materials-and-methods",
    "title": "Project Blog Post",
    "section": "4 Materials and Methods",
    "text": "4 Materials and Methods\nYour Materials and Methods section should describe:\n\nYour Data\nInclude some discussion of where it came from, who collected it (include a citation), how it was collected, and what each row represents (a person, an environmental event, a body of text, etc) Please also include a discussion of potential limitations in the data: who or what is represented, and who or what isn’t?\nIn structuring your description of the data, I encourage you to address many of the questions outlined in (gebru2021datasheets?), although it is not necessary for you to write a complete data sheet for your data set.\n\n\nYour Approach\nThis is the primary section where you should describe what you did. Carefully describe:\n\nWhat features of your data you used as predictors for your models, and what features (if any) you used as targets.\nWhether you subset your data in any way, and for what reasons.\nWhat model(s) you used trained on your data, and how you chose them.\nHow you trained your models, and on what hardware.\nHow you evaluated your models (loss, accuracy, etc), and the size of your test set.\nIf you performed an audit for bias, how you approached this and what metrics you used."
  },
  {
    "objectID": "assignments/project/blog-post.html#results",
    "href": "assignments/project/blog-post.html#results",
    "title": "Project Blog Post",
    "section": "5 Results",
    "text": "5 Results\nThis is the section in which you describe the main findings or achievements of your model. You can report things like accuracies on train/test data, loss scores, comparisons to previous models, etc. To compare a small set of numbers, tables are fine, but more complex phenomena should be illustrated with figures. Both figures and tables should include appropriate captions, axis labels, legends, and another professional annotations. It’s fine for your figures to either be constructed manually or as computational outputs (e.g. from Pandas).\nPlease remember: your results do not speak for themselves. While figures and tables are highly effective forms of communication, your prose is necessary to tell your story."
  },
  {
    "objectID": "assignments/project/blog-post.html#concluding-discussion",
    "href": "assignments/project/blog-post.html#concluding-discussion",
    "title": "Project Blog Post",
    "section": "6 Concluding Discussion",
    "text": "6 Concluding Discussion\nYour conclusion is the right time to assess:\n\nIn what ways did our project work?\nDid we meet the goals that we set at the beginning of the project?\nHow do our results compare to the results of others who have also studied similar problems?\nIf we had more time, data, or computational resources, what might we do differently in order to improve further?"
  },
  {
    "objectID": "assignments/project/blog-post.html#group-contributions-statement",
    "href": "assignments/project/blog-post.html#group-contributions-statement",
    "title": "Project Blog Post",
    "section": "7 Group Contributions Statement",
    "text": "7 Group Contributions Statement\nWhen writing your group contributions statement, please keep in mind that everyone’s contributions are visible in the commit history of your GitHub repository.\nIn your group contributions statement, please include a short paragraph for each group member describing how they contributed to the project:\n\nWho worked on which parts of the source code?\nWho performed or visualized which experiments?\nWho led the writing of which parts of the blog post?\nEtc."
  },
  {
    "objectID": "assignments/project/blog-post.html#personal-reflection",
    "href": "assignments/project/blog-post.html#personal-reflection",
    "title": "Project Blog Post",
    "section": "8 Personal Reflection",
    "text": "8 Personal Reflection\nThis is the only section that you are required to write individually and not with your project group.\nAt the very end of your blog post, in a few paragraphs, respond to the following questions:\n\nWhat did you learn from the process of researching, implementing, and communicating about your project?\nHow do you feel about what you achieved? Did meet your initial goals? Did you exceed them or fall short? In what ways?\nIn what ways will you carry the experience of working on this project into your next courses, career stages, or personal life?\n\n\nAppendix: References in Quarto\nTo manage references in Quarto, you need to create a .bib file (you can call it refs.bib). This file should live in the same directory as your blog post. Your .bib file is essentially a database of document information. Here’s an example of a a refs.bib file:\n@article{hardt2021patterns,\n  title  = {Patterns, predictions, and actions: A story about machine learning},\n  author = {Hardt, Moritz and Recht, Benjamin},\n  journal= {arXiv preprint arXiv:2102.05242},\n  year   = {2021}\n}\n\n@article{kearns1994toward,\n  title     = {Toward Efficient Agnostic Learning},\n  author    = {Kearns, Michael J and Schapire, Robert E and Sellie, Linda M},\n  journal   = {Machine Learning},\n  volume    = {17},\n  pages     = {115--141},\n  year      = {1994},\n  publisher = {Citeseer}\n}\n\n@misc{narayanan2022limits,\n  author       = {Narayanan, Arvind},\n  howpublished = {Speech},\n  title        = {The limits of the quantitative approach to discrimination},\n  year         = {2022}\n}\nThe simplest way to get entries for your references is to look them up on Google Scholar.\n\nSearch for the document you want.\nClick the “Cite” link underneath and choose “Bibtex” from the options at the bottom.\nCopy and paste the contents of the new page to your refs.bib file.\n\nOnce you’ve assembled your references, add the following line to your document metadata (the stuff in the top cell of your Jupyter notebook)\nbibliography: refs.bib\nOnce you’ve followed these steps, you’re ready to cite! You can reference your documents using the @ symbol and their bibliographic key, which is the first entry for each document in the refs.bib file. For example, typing\n@hardt2021patterns\nresults in the reference\n(hardt2021patterns?)\nas well as an entry in the “References” section at the end of your blog post.\nFor more on how to handle citations in Quarto, check the Quarto documentation."
  },
  {
    "objectID": "assignments/project/proposal.html",
    "href": "assignments/project/proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "The project proposal is your first formal step toward completing your project for this course. In this assignment, you’ll (a) create the shared GitHub repository that will hold your project files and (b) collaboratively create a written description of what you will do, how you will judge its success, and what you intend to learn from the process.\nPlease note that, in order to complete the proposal, you will need to be in active communication with your project partners. Here’s my suggestion:\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/project/proposal.html#create-the-project-repository",
    "href": "assignments/project/proposal.html#create-the-project-repository",
    "title": "Project Proposal",
    "section": "1 Create the Project Repository",
    "text": "1 Create the Project Repository\nOne member of the group should go on GitHub.com and create the project repository. This is the central location that will house your project files. After creating the project repository, this group member should add all other group members as collaborators on the project (under Settings –&gt; Manage Access).\nAll group members should now clone the repository, using either GitHub Desktop or the command line.\nDo not fork the project repository, as this will lead to everyone having their own private version. Not very collaborative!\nBy the end of this step, you now have a shared repository, under version control, in which you can all collaborate. This is where your project files, including code and data, will live."
  },
  {
    "objectID": "assignments/project/proposal.html#write-your-project-proposal",
    "href": "assignments/project/proposal.html#write-your-project-proposal",
    "title": "Project Proposal",
    "section": "2 Write Your Project Proposal",
    "text": "2 Write Your Project Proposal\nWrite your project proposal in your project repository. For now, you can just use the file README.md to hold your proposal; this has the benefit that GitHub.com will automatically render it for you. Specs for your proposal are below. Here’s what I’m looking for from your proposal:\n\nExpected Sections\nYou should include sections in your proposal that address the following topics. Feel free to include additional sections as needed. Remember that you can create Markdown sections using the # character.\n\nAbstract\nIn 3-4 sentences, concisely describe:\n\nWhat problem your project addresses.\nThe overall approach you will use to solve the problem.\nHow you propose to evaluate your success against your stated goals.\n\n\n\nMotivation and Question\nDescribe your motivation for your project idea. Some (shortened) examples of good types of motivations:\n\nWe have a scientific data set for which predictive or expoloratory models would help us generate hypotheses.\nWe have user information for which predictive models would help us give users better experiences.\nWe have performance data (e.g. from sports teams) for which predictive models could help us make better decisions.\nAlgorithmic bias is an increasingly urgent challenge as machine learning products proliferate, and we want to explore it more deeply.\n\nYou should be more specific than these: describe your specific data set (if applicable); your scientific questions; the type of decisions your model could inform; etc.\n\n\nPlanned Deliverables\nConcisely state what you are aiming to create and what capabilities it will have. For most projects, I would expect the deliverable to include:\n\nA Python package containing all code used for algorithms and analysis, including documentation.\nAt least one Jupyter notebook illustrating the use of the package to analyze data.\n\nHowever, your specific idea might imply different deliverables (e.g. an essay). Consult with me if you’re not sure.\nYou should describe what your deliverable will be able to do and how you will evaluate its effectiveness. Please consider two scenarios:\n\n“Full success.” What will your deliverable be if everything works out for you exactly as you plan?\n“Partial success.” What useful deliverable will you be able to offer even if things don’t 100% work out? For example, maybe you aren’t able to get that webapp together, but you can still create a code repository that showcases the machine learning pipeline needed to use to support the app. Have a contingency plan!\n\n\nWritten Deliverables\nYou’ll also write a blog post on your project; you don’t have to discuss this post in your proposal though.\n\n\n\nResources Required\nWhat resources do you need in order to complete your project? Data? Computing power? An account with a specific service?\nPlease pay special attention to the question of data. If your project idea involves data, include at least one link to a data set you can use. If you can’t find data for your original idea, that’s ok! Think of something related to your group’s interests for which you can find data.\nMost projects should involve data in some way, but certain projects may not require data. Ask me if you’re not sure.\n\n\nWhat You Will Learn\nEach group member should return to their stated goals from the reflective goal-setting assignment at the beginning of the course. Then, in this section, please state what each group member intends to learn through working on the project, relating your intentions to your stated goals. You might be thinking of certain algorithms, software packages, version control, project management, effective teamwork, etc.\n\n\nRisk Statement\nWhat are two things that could potentially stop you from achieving the full deliverable above? Maybe it turns out that the pattern you thought would be present in the data just doesn’t exist? Or maybe your idea requires more computational power than is available to you? What particular risks might be applicable for your project?\n\n\nEthics Statement\nAll projects we undertake involve decisions about whose interests matter; which problems are important; and which tradeoffs are considered acceptable. Take some time to reflect on the potential impacts of your project on its prospective users and the broader world. Address the following questions:\n\nWhat groups of people have the potential to benefit from our project?\nWhat groups of people have the potential to be excluded from benefit or even harmed from our project?\nWill the world become an overall better place because we made our project? Describe at least 2 assumptions behind your answer. For example, if your project aims to make it easier to predict crime, your assumptions might include:\n\nCriminal activity is predictable based on other features of a person or location.\nThe world is a better place when police are able to perform their roles more efficiently.\n\n\nIf your project involves making decisions or recommendations, then you will also need to consider possible forms of algorithmic bias in your work. Here are some relevant examples:\n\nA recipe recommendation app can privilege the cuisines of some locales over others. Will your user search recipes by ingredients? Peanut butter and tomato might seem an odd combination in the context of European cuisine, but is common in many traditional dishes of the African diaspora. A similar set of questions applies to recommendation systems related to style or beauty.\nA sentiment analyzer must be trained on specific languages. What languages will be included? Will diverse dialects be included, or only the “standard” version of the target language? Who would be excluded by such a choice, and how will you communicate about your limitations?\n\n\n\nTentative Timeline\nWe will have a checkpoint for the project in Week 9 or 10, and then final presentations in Week 12. With this in mind, please describe what you expect to achieve after three and six. Your goal by the three-week check-in should be to have “something that works.” For example, maybe in three weeks you’ll ready to demonstrate the data acquisition pipeline and show some exploratory analysis, and in the last couple weeks you’ll actually implement your machine learning models. The “something that works” idea is related to the common concept of “minimum viable products” in software development, and is visually illustrated here:"
  },
  {
    "objectID": "assignments/project/proposal.html#general-expectations",
    "href": "assignments/project/proposal.html#general-expectations",
    "title": "Project Proposal",
    "section": "3 General Expectations",
    "text": "3 General Expectations\nYour proposal is acceptably complete if:\n\nThe proposal is hosted on GitHub as the top-level README.md file in a repository hosted by one of the group members.\nEach team member has made at least two commits to this file, which in total demonstrate substantial commitments to the writing of the proposal.\nThe proposal contains thoughtful discussion in each of the required sections, which addresses all of the relevant questions posed in each one.\nThe proposal is written in clear English prose. Within reason, grammatical mistakes are not a problem.\n\nYou have submitted a URL to your GitHub repository on Canvas.\n\n\nLength\nThere is no specifically required length for the proposal. Generally speaking, I would expect a thoughtful proposal to require around 600-900 words (roughly the length of 2-3 double-spaced pages). However, any length is acceptable provided that it provides thoughtful discussion of each of the required components."
  },
  {
    "objectID": "assignments/project/proposal.html#optional-practice-collaborative-workflows-in-git",
    "href": "assignments/project/proposal.html#optional-practice-collaborative-workflows-in-git",
    "title": "Project Proposal",
    "section": "4 Optional Practice: Collaborative Workflows in Git",
    "text": "4 Optional Practice: Collaborative Workflows in Git\nYour proposal should be written on GitHub and contain commits by multiple team members. This is the same workflow that you’ll use for collaborating on your project itself. If members of your team are not familiar with collaborative workflows in Git and GitHub, then you should complete this activity with your team in order to get up to speed.\nWorking through this mini activity with your group is optional but strongly recommended.\n\nMake a Grid\nIt’s possible that your repository already has a top-level file named README.md. If not, a new group member (not the one who created the repo) should create one. Then, this group member should create a code block in the file README.md containing a 3x3 grid of dots, like the below:\n. . . \n. . .\n. . .\nSave, commit, and push. All other group members should now pull, so that they have the grid of dots as well.\n\n\nPlay Tic-Tac-Toe\nIf you have more than two people, separate into two teams – it’s ok if they are not the same size. Play a few games of Tic-Tac-Toe by replacing the dots in the grid you made with the symbol X or O. Here’s how to make a move:\n\nA member of Team X deletes a dot and replaces it with an X.\nThis member commits and pushes their change.\nAll other team members pull, so that the move is reflected in their file.\nA member of Team O deletes a dot and replaces it with an O…\n\nBy playing some Tic-Tac-Toe, you practice the fundamental pull-commit-push workflow of collaboration. Make sure that every group member gets a chance to make a move, commit their move, and push at least twice.\n\n\nMerging\nCreate a new, blank Tic-Tac-Toe game. Imagine that Team X and Team O miscommunicated about who would go first, so they both make moves simultaneously. Test out the following scenarios.\nScenario 1:\nTeam X makes the following move:\n. . . \n. . .\n. X .\nTeam O makes the following move:\n. O . \n. . .\n. . .\nBoth teams should now attempt to commit and push. One team will be prompted to pull prior to pushing. This pull will prompt a merge, since two changes were made to the same file. Observe what happens, commit, and then push. Pull as needed so that both teams have both moves in their file.\nScenario 2:\nTeam X makes the following move:\n. . . \n. X .\n. . .\nTeam O makes the following move:\n. . . \n. O .\n. . .\nThe current representative of each team should commit these respective moves, and attempt to push.\nWhoops! One team will be prompted to pull, and after pulling will be informed that there is a merge conflict in the repository. Inspect the file. Notice that the relevant part of the file now looks very weird. If you look closely, you can find lines corresponding both to the X move and to the O move. Pick one (arbitrarily), and commit/push the result.\nThis is an example of the process used to handle merge conflicts, which occur when separate team members have modified the same file in conflicting ways.\nIt’s recommended, but not required, that your team members take some time on their own to do a little reading on how merge conflicts work. This page gives a good explanation, and also covers the (optional but highly useful) topic of branching."
  },
  {
    "objectID": "assignments/process/mid-course.html",
    "href": "assignments/process/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice (I recommend either VSCode or JupyterLab).\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Canvas.\nIn the The Data section, replace the blanks with brief responses.\nIn the What You Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, review the soundbytes describing letter grades.\nTake some time to reflect on your responses so far. When you’re ready, propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451 so far.\nOptionally, respond to the last prompt with some thoughts on how the semester is going and what we might do to help you meet your goals for the course.\nSubmit the notebook as a PDF on Canvas.\n\nWe’ll discuss your reflection and your proposed letter grade during our end-of-semester conference.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf mid-course.ipynb\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/process/mid-course.html#the-data",
    "href": "assignments/process/mid-course.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\nPresence in Class\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often have you taken notes on the core readings ahead of the class period? ____\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times have you actually presented the daily warm-up to your team? ____\nHow many times have you asked your team for help while presenting the daily warm-up? ____\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? ____\nHow often have you helped a teammate during the daily warm-up presentation? ____\n\n\n\nPresence Outside of Class\n\nHow often have you attended Student Hours or Peer Help? ____\nHow often have you asked for or received help from your fellow students? ____\nHave you been regularly participating in a study group outside class? ____\nHow often have you posted questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts have you submitted? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: ____\nM: Revisions useful: ____\nR: Revisions encouraged: ____\nN: Incomplete: ____\n\nRoughly how many hours per week have you spent on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/mid-course.html#what-youve-learned",
    "href": "assignments/process/mid-course.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#reflecting-on-goals",
    "href": "assignments/process/mid-course.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\n[your response here]\n\n\nUpdating Your Goals\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#grade-and-goals",
    "href": "assignments/process/mid-course.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "Grade and Goals",
    "text": "Grade and Goals\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\nWhat a Grade Sounds Like\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of ____\n\n\nA way in which I resonate with the soundbytes for that grade above is…"
  },
  {
    "objectID": "assignments/process/mid-course.html#optional-how-to-improve",
    "href": "assignments/process/mid-course.html#optional-how-to-improve",
    "title": "Mid-Course Reflection",
    "section": "(Optional:) How to Improve?",
    "text": "(Optional:) How to Improve?\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\n[your response here]"
  },
  {
    "objectID": "assignments/process/goal-setting.html",
    "href": "assignments/process/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice (I recommend either VSCode or JupyterLab).\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nIn each of the spaces provided, write down some goals describing what you believe success will look like for you in CSCI 0451. I’ve offered some ideas to help you get started in case you’re not sure, but you shouldn’t feel constrained by these.\nYou may want to look at the end-of-course reflection activity in which you’ll look back on your goals and propose a letter grade that reflects your learning, participation, and achievement in the course. You might especially want to look at the data that I’ll ask you to log and what a grade sounds like.\nSubmit the notebook as a PDF on Canvas.\n\nI’ll respond to your submission with feedback on your goals. I may ask you to display more or less ambition in some of your goals; in that case, I’ll ask you to revise and resubmit.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf goal-setting.ipynb\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-learn",
    "href": "assignments/process/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\n[your response here]"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-achieve",
    "href": "assignments/process/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n[your response here]\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n[your response here]\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n[your response here]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is an advanced elective on the topic of algorithms that learn patterns from data. Artificial intelligence, predictive analytics, computational science, pattern recognition, signal processing, and data science are all disciplines that draw heavily on techniques from machine learning. The course focuses almost entirely on predictive models, and places special emphasis on questions of fairness in automated decision-making.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "index.html#what-will-class-time-look-like",
    "href": "index.html#what-will-class-time-look-like",
    "title": "Syllabus",
    "section": "What Will Class Time Look Like?",
    "text": "What Will Class Time Look Like?\nMost class time will involve the following components:\n\n10-20 minutes of a warmup activity that addresses recent lectures or readings.\n40-50 minutes of lecture, which will often focus on math, theory, and programming examples. My aim is that we usually won’t spend the entire lecture period on theory, and that a major part of most lectures will be hands-on live-coding.\n\n\nThe Warmup Activity\nOn most days, we’ll have a warmup activity. The warmup activity will usually ask you to engage with the readings and complete some work ahead of class time. This could be a short piece of writing, a math problem, or an implementation of a Python function.\nEach day, a few students will be randomly selected to present their work to a small group of peers. It’s ok to ask for help or even pass if you’re not feeling confident in your solution, but you should plan to at least make a good attempt at the warmup before every class period. Your participation on the warmup activity is an important aspect of presence in the course, and I’ll ask you to reflect on it when proposing your course grade."
  },
  {
    "objectID": "index.html#collaborative-grading",
    "href": "index.html#collaborative-grading",
    "title": "Syllabus",
    "section": "Collaborative Grading",
    "text": "Collaborative Grading\nThis course is collaboratively graded.  In a nutshell, this means:You may have also heard the term ungrading to refer to a similar approach.\n\nThere are no points or scores attached to any assignment. When you turn in assignments, you’ll get feedback on how to revise/resubmit, improve or otherwise proceed in the course, but you won’t get “graded.”\nThere also aren’t any firm due dates, although I will give you suggestions on how to maintain a good pace. \nPeriodically throughout the semester, you will complete reflection activities to help you take stock of your learning and achievement in the course. In your final activity at the end of the semester, you’ll make a proposal for your letter grade in the course, and support it with evidence of your learning. You and I will then meet to discuss how the course went for you, using your reflection activity and proposal as a starting point. In this conversation, you and I will agree on your final letter grade for the course, which I will then submit to the registrar.\n\nAll work you wish to be considered toward your achievement in the course needs to be submitted by the end of Finals Week.Reflection activities:\n\nReflective goal-setting.\nMid-course reflection.\nEnd-of-course reflection and grade proposal."
  },
  {
    "objectID": "index.html#why-collaborative-grading",
    "href": "index.html#why-collaborative-grading",
    "title": "Syllabus",
    "section": "Why Collaborative Grading?",
    "text": "Why Collaborative Grading?\nBecause grading is broken! Traditional points-based grading is ineffective at both (a) accurately measuring student learning and (b) motivating students to learn. I broadly agree with Jesse Stommel when he writes:\n\nAgency, dialogue, self-actualization, and social justice are not possible in a hierarchical system that pits teachers against students and encourages competition by ranking students against one another. Grades (and institutional rankings) are currency for a capitalist system that reduces teaching and learning to a mere transaction. Grading is a massive co-ordinated effort to take humans out of the educational process.\n\nFor a course like this, I’d prefer to just not give you grades at all. But, Middlebury says I have to, and so my aim is to instead put the process of grading under your control to the greatest extent that I reasonably can."
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nThere are three kinds of assessed assignments in this course, plus a mysterious “Other” category.\n\n\n\nBlog Posts\n\n\nBlog posts are the primary way in which you will demonstrate your understanding of course content. Blog posts usually involve: written explanation of some relevant theory; implementation one or more algorithms according to written specifications; performing experiments to test the performance of the implementations; and communicating findings in a professional way. Some blog posts will be more like short essays than problem sets or programming assignments. Your blog posts will be hosted on your own public website (which you will create). This website will serve as your portfolio for the course.\n\n\n\n\n\nProject\n\n\nYour project is a large-scale undertaking that you will design and complete, usually in a group of 2 or 3, over the course of the semester. Your project should usually involve some combination of data collection, implementation, research of related work, experimentation, deployment, or theory work (but not necessarily all components). Projects are expected to demonstrate deep engagement with both the course content and the problem selected.\n\n\n\n\nProcess Reflections\n\n\nAt the beginning of the course, you’ll write a process reflection describing your aspirations for the course—what you want to learn and achieve, and how you’d like to be assessed against your goals. We’ll have a second process reflection mid-way through the course that will allow you to reflect on your progress toward your objectives and consider changing direction if needed. At the end of the course, you’ll write a summary reflection on your learning, accomplishment, and engagement with the class. This is also the place where you’ll propose your final letter grade.   I’ll usually give you written feedback on your process reflections. We’ll also meet at the end of the course to discuss your final reflection and agree on your letter grade for the course.\n\n\n\n\nOther…?\n\n\nYou may have some topic or idea that especially interests you and which you want to explore. If you’d like to work on this topic and use it to demonstrate your learning in the course, you can propose it to me. I may have suggestions or requested modifications before I agree to count the work in your course portfolio."
  },
  {
    "objectID": "index.html#best-by-dates",
    "href": "index.html#best-by-dates",
    "title": "Syllabus",
    "section": "Best-By Dates",
    "text": "Best-By Dates\nWhile we don’t have formal due dates, there is a benefit to keeping yourself on a schedule. It’s best to complete assignments close to the time when we covered the corresponding content in class, and it’s important for your wellbeing not to let work pile up. I’ll provide “best-by” dates for all assignments. These are my recommendations for when you should submit the first versions of these assignments to me for feedback.\n\n\n Image credit: Dr. Spencer Bagley"
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\n\nBlog Posts\nYou will receive feedback and “grades” on your blog posts. Your feedback will describe what we in the course team thought was especially successful and where you have room to improve. Your “grade” will be one of the following:\n\nE: Excellent, I do not suggest that you spend any more time on this assignment.\nM: Meets expectations, I think that you may be able to learn more by revising this assignment in response to feedback.\nR: Revision suggested, I believe that you need to spend more time with this assignment in order to learn what it has to teach you.\nN: Not Complete, You have more work to do in order to learn what this assignment has to teach you.\n\nThere is no formula by which Es, Ms, Rs, and Ns are factored into a final grade. Instead, it will be up to you to set goals around these and hold yourself accountable to them. Historically, ambitious students in this course have reached Es or above on at least 4 blog posts and Ms on at least 2-3 more.\nIt is relatively rare to earn an E on a first assignment submission. That’s ok! It’s an opportunity for you to revise and continue to learn. Revising in response to feedback is one of the single most effective ways for you to deepen your learning.\n\n\nProject\nYour project does not receive any formal grades. As one of your final blog posts, you will reflect on your contributions to your project and write about the extent to which you met your goals.\n\n\nProcess Reflections\nAfter you submit process reflections, I will write back to you with my own thoughts on your goals and your progress against them. Process reflections are not otherwise graded, you are required to complete them in order to receive a grade for the course."
  },
  {
    "objectID": "index.html#what-work-do-you-need-to-do",
    "href": "index.html#what-work-do-you-need-to-do",
    "title": "Syllabus",
    "section": "What Work Do You Need To Do?",
    "text": "What Work Do You Need To Do?\nAt the beginning of the semester, you’ll write a process reflection that will outline what you’d like to learn and achieve in the course. It’s ok if you don’t meet all your aspirations by the end of the course, and there will be an opportunity to revise mid-way. That said, you are still accountable to yourself and to me, in the following way:\n\nFor an A, a student should spend at least 10 productive hours of learning per week of work outside of class.\n\nHow you spend those 10 hours is largely up to you and your goals. Some examples:\n\nYou complete many of the assignments that I offer you, to a high degree of quality.\nYou complete fewer of the assignments that I offer you, but also propose several different assignments to demonstrate your learning in response to your interests.\nYou complete fewer of the assignments that I offer you, but also organize a regular study group or review session.\nYou complete fewer of the assignments that I offer you, but complete a final project that goes far beyond the standard project expectations.\n\nYou complete fewer of the assignments that I offer you, but demonstrate considerable outside learning in relevant math, the theory of fairness in machine learning, or some other endeavor.\n\nNotes:\n\nThere are lots of opportunities to practice applied data science workflows via things like Kaggle competitions. This can be a worthwhile use of time, but it’s not guaranteed to help you learn new ideas. With that in mind, these kinds of applied projects are not recommended as part of how you’ll spend your time for this class.\nData science projects that you do for other classes, consulting, internships, etc. are cool and I would love to hear about them! They don’t count as effort towards this course unless you can describe how the content from this course is informing your projects and how you are spending extra effort on those projects as part of your experience of this course."
  },
  {
    "objectID": "index.html#directing-your-learning",
    "href": "index.html#directing-your-learning",
    "title": "Syllabus",
    "section": "Directing Your Learning",
    "text": "Directing Your Learning\nThis course asks you to set your own goals and motivate yourself to achieve them. Neither of these tasks are easy. It’s ok to mess up every now and then – we all do! The real question is whether you’re going to look at mistakes and make time to reflect on what to do next time."
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "Syllabus",
    "section": "Programming",
    "text": "Programming\n\nYou can write moderately-complex, object-oriented software.\nYou are comfortable reading software documentation and researching how to perform a task that you haven’t seen before.\nYou know what a terminal is and how to perform simple operations at the command line.\nYou have experience debugging your code and you are ready to do it a lot more."
  },
  {
    "objectID": "index.html#math",
    "href": "index.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nI am assuming that you remember most of MATH 0200 and CSCI 0200. It’s ok if you haven’t memorized every single fact. What I need is for you to be ready to rapidly look up what you need so that you won’t be slowed down by math along the way.\n\nMatrix multiplication and inner products\nEverything about \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\).\n\nVisualizing linear spaces.\nEigenvalues, eigenvectors, positive-definite matrices.\nDerivatives, critical points of functions.\n\nI have prepared a mathematics pre-assessment for you to try. This assessment is ungraded and is primarily for you to reflect on your comfort with some of the skills we will need. It’s ok if you are not confident on every problem (especially the problems related to multivariable differentiation). You’ll just want to devote some of your learning effort during the course towards getting more comfortable.\n\nReviews/Diagnostics\n\nThis resource from Stanford’s CS246 contains most of the linear algebra that you’ll need for the course. The only big topic that’s missing is treatment of the existence of solutions of the linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) in terms of the rank of \\(\\mathbf{A}\\). You don’t need to have memorized everything here, but most of it should look familiar.\nProbability is not a formal requirement for CSCI 0451, but some probability can certainly be useful. To brush up on some basics, I suggest Chapter 2 of Introduction to Probability for Data Science by Stanley Chan. This treatment may be a little more advanced than what you learned in CSCI 0200, but you should recognize many of the main ideas."
  },
  {
    "objectID": "index.html#laptops",
    "href": "index.html#laptops",
    "title": "Syllabus",
    "section": "Laptops",
    "text": "Laptops\nPlease bring a laptop, and make sure that it has at least 75 minutes of charge.\n\n\n\n\n\n\nWe Can Help You Borrow a Laptop\n\n\n\n\n\nIf you ever find yourself temporarily in need of a laptop, the Computer Science department has 10 rotating Dell laptops available to our students. These come pre-installed with software for most of the courses in the major. They are available to be loaned out short-term or long-term based on your need (as determined by you). To request a laptop for short-term use (like a single class period), email me ahead of time.\nOn Long-Term Use: College policy has changed recently to include the expectation that every student have a laptop available. The college provides laptops to those who need them where “need” is based on Student Financial Services calculations. If you anticipate needing a laptop for the whole term, we encourage you to inquire with Student Financial Services and the library first due to our smaller pool of equipment. However, our department commits to meeting the needs of every student, so do not be afraid to reach out if you believe you need one of our laptops for any length of time."
  },
  {
    "objectID": "index.html#academic-integrity-and-collaboration",
    "href": "index.html#academic-integrity-and-collaboration",
    "title": "Syllabus",
    "section": "Academic Integrity and Collaboration",
    "text": "Academic Integrity and Collaboration\n\nAcademic Integrity\nBriefly, academic integrity means that you assume responsibility for ensuring that the work you submit demonstrates your learning and understanding.\nTo be frank, it’s pretty easy to act without integrity (i.e. cheat) in this course. First, there’s a lot of solution code for machine learning tasks in Python online. Second, I’m literally asking you all to post your assignments publicly online. So, there are lots of opportunities to turn in assignments without actually doing the learning that those assignments are designed to offer you.\nI assume that both of us want you to learn some cool stuff. Cheating stops you from doing that, and ultimately wastes both your time and mine. I won’t be vigorously hunting for academic integrity violations, but I may ask you to discuss code or theory with me in class or in our meetings. If I notice you struggling to explain code that you submitted for feedback, I may have questions.\nTrust me. Neither of us want this.\n\n\nCollaboration\nI love it! Please collaborate in ways that allow you and your collaboration partners to fully learn from and engage with the content. Sharing small snippets of code or math is often helpful to get someone unstuck, but sharing complete function implementations or mathematical arguments is usually counterproductive.\nHere are some general guidelines for how I think about collaboration.\n\n\nLarge Language Models\nLarge language models (LLMs) like ChatGPT aim to produce helpful, human-like text with a combination of two major mechanisms:\n\nLLMs use next-token prediction to predict the next entries in a sequence of text by using the previous entries. This enables them to mimic human-produced text. So, if you saw the sequence “I love machine _____”, you might guess that the next word might be “learning” and that it’s probably not “pineapples.”\nLLMs are trained using reinforcement learning with human feedback (RLHF) to produce sentences that are not just realistic but also helpful, nonoffensive, or accurate. They do this using a multistage training process that involves humans rating the quality of candidate texts.\n\nChatGPT and similar LLMs are shaping many conversations in education at Middlebury and beyond.\nWhile LLMs may have many benefits, they are also the subject of routine deception by powerful, motivated actors. They are powered by massive quantities of low-paying, traumatizing labor. LLMs and automation more generally are furthering labor instability, contributing to reduced wages, and are concentrating the power of a small number of large tech companies.\n\n\nIn This Class\n\nWork that you submit in this class must demonstrate your learning and understanding. Copy/paste from LLMs like ChatGPT does not do this, and is consequently a violation of academic integrity.\nIn particular, English writing in blog posts and project writeups must always be your own words. Using ChatGPT to generate ideas is permissible, but using it to generate text for your blog is a violation of academic integrity.\nUse of ChatGPT or GitHub Copilot to generate solution code for blog posts is likely to detract from your learning and is therefore strongly discouraged.\n\nIf you choose to use an LLM in any way in your blog posts or projects, you must:\n\nCite your source.\nDescribe the contribution of the source to your submitted writeup.\nDescribe what steps you have taken to independently verify that your learning from the LLM is indeed correct.\nRemember that you are always personally responsible for the accuracy, quality, and integrity of your work."
  },
  {
    "objectID": "index.html#general-advice",
    "href": "index.html#general-advice",
    "title": "Syllabus",
    "section": "General Advice",
    "text": "General Advice\nI am always happy to talk with you about your future plans, including internships, research opportunities, and graduate school applications. Because I am a creature of the academy, I am less knowledgeable about industry jobs, although you are welcome to ask about those too. You can drop in during Student Hours or email me to make an appointment."
  },
  {
    "objectID": "index.html#letters-of-recommendation",
    "href": "index.html#letters-of-recommendation",
    "title": "Syllabus",
    "section": "Letters of Recommendation",
    "text": "Letters of Recommendation\nWriting letters of recommendation for students is a fundamental part of my job and something that I am usually very happy to do. Here’s how to ask me for a letter."
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "Collaboration And Academic Honesty",
    "section": "",
    "text": "This is a page of general principles and guidelines that apply in courses I (Phil Chodrow) teach at Middlebury College. It is lightly adapted from the handout “Collaborating on Mathematics” by the Harvey Mudd Department of Mathematics, which I discovered in a Tweet by Francis Su.\nIn any case in which the guidelines and principles on this page conflict with the policies of a specific course, the policies of the specific course should be followed. For example, if the course syllabus says that collaboration is not permitted on homeworks, then collaboration is not permitted on homeworks, regardless of anything written here.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "collaboration.html#why-collaborate",
    "href": "collaboration.html#why-collaborate",
    "title": "Collaboration And Academic Honesty",
    "section": "Why Collaborate?",
    "text": "Why Collaborate?\nMost scientists and engineers don’t work on their own; they work with colleagues and students while doing and publishing research. Increasingly, open problems in science and engineering require multiple skill sets and areas of expertise. Because of this, the need to collaborate will only increase in the future. This is why several of CS@Midd’s learning goals are explicitly focused on communication and collaboration. We want our students to have strong professional and communication skills, to be able to function well as part of a team, and to be able to work and communicate with diverse groups of people."
  },
  {
    "objectID": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "href": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Homework and Other Individually-Assessed Assignments",
    "text": "Collaborating on Homework and Other Individually-Assessed Assignments\n\n(COLLABORATION IS A LIFE SKILL) Understand that working with others and asking for assistance are not signs of weakness or deficiency; rather, they are essential life skills important for making progress in any discipline, including computer science. Our department wants you to develop these skills. If you’re too shy to come to my office hours or to join a group of people working on their homework, ask a friend to come with you.\n(COLLABORATIONS BENEFIT FROM DIVERSITY) Open yourself up to working with people whom you don’t know (yet). You might find someone you work really well with and who doesn’t think exactly like you do. A wide range of experiecnes and backgrounds is beneficial in problem solving, although it may be helpful to find folks who can work on assignments during the same time of day and at roughly the same pace. If you’re having trouble finding people to work with, I can help!\n(COLLABORATIONS ARE INCLUSIVE) Believe that everyone has something meaningful to contribute (you included), and that you have something to learn from each person. This can be a difficult state of mind to achieve, but critical for healthy, effective collaboration. Here are some practical consequences:\n\nIn any group setting, listen carefully for everyone’s contributions. Don’t dismiss or ignore what someone says, and don’t move on until you’ve considered it carefully. If what is said doesn’t make sense to you, that doesn’t necessarily mean it’s incorrect–the person might just have a way of approaching the problem that is different and not yet clear to you. Furthermore, even ideas that ultimately turn out to be incomplete or incorrect are often still useful building blocks towards a successful approach.\nFind ways to verbally validate the ideas of others. For example: “One really neat feature of Zenith’s approach to part (b) is that it also works with a small modification for part (c).”\nIf someone in the group hasn’t spoken for a while, ask for their ideas or opinions. Conversely, if you find yourself talking a lot, take a step back and allow someone else to contribute to the discussion.\n\n(COLLABORATIONS REQUIRE PREPARATION) Don’t seek help from others on a probem before you’ve had time to think about it yourself, try at least one approach, and formulate the obstacle as clearly as you can. But at the same time, if you find yourself frustrated with a problem and you’re not making progress, don’t wait too long before you look for help from your classmates, your tutors, or me.\n(COLLABORATIONS GENERATE DEEPER UNDERSTANDING) Don’t be satisfied with only producing the correct final result; use your collaboration to push each other to understand:\n\nWhy does this approach work?\nWhat alternative approaches would also have worked?\nWhat are some of the merits and drawbacks of these different approaches?\n\n(COLLABORATIONS ARE EMPOWERING) Good collaborations empower people towards further growth.\n\nWhen you’re working on a problem with others and you find a path before everyone else, avoid ruining the experience of discovery for others. Conversely, if you haven’t figured out something yet and want to enjoy the discovery for yourself, don’t let someone else ruin your joy.\nIf someone asks you for help, don’t just tell them the answer or start showing them a solution method. Listen carefully to their question. Ask for more information if they aren’t being specific enough. If they say “I don’t know where to start,” ask them to tell you about their understanding of what the question is asking and which parts of it seem most puzzling. Ask guiding questions to help them discover ideas for themselves. In these situations, you have the opportunity to learn how to help others learn – this is an invaluable life skill.\n\n(COLLABORATIONS ACKNOWLEDGE CONTRIBUTORS) Whenever you’ve received help on a homework assignment from a classmate, a friend, a tutor, or me, acknowledge the support and briefly describe how it helped you in your assignment.\n\nThe reason I ask you to acknowledge tutors and myself is actually different from the reason I ask you to acknowledge classmates and friends. For classmates and friends, it’s about cultivating transparency and integrity. The primary reason I want you to acknowledge tutors and myself is that the exercise of explicitly remembering and reflecting on your learning journey is part of metacognition, a valuable set of practices that will help you succeed in this class, in college, and in your long-term career."
  },
  {
    "objectID": "collaboration.html#collaborating-on-group-projects",
    "href": "collaboration.html#collaborating-on-group-projects",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Group Projects",
    "text": "Collaborating on Group Projects\n\n(COLLABORATIONS SET GOOD EXPECTATIONS) Establish clear expectations and ways of communicating with each other to avoid misunderstandings. When, where, and how often will you meet? How can you reach each other in case of an emergency?\n(COLLABORATION IS NOT DIVISION OF LABOR) Collaboration is not the same as splitting up a problem into pieces and then slapping the completed pieces together.\n\nIdentify the parts of the problem that need to be completed together and the parts that can be completed individually.\nWork toward a final product that everyone is happy with and that represents the contributions of everyone on the group.\nDon’t just divide up the work based on who might have the most experience or skill with each part of the problem. Let those who want to develop their skills also have a chance to work on pieces that are unfamiliar to them\n\n(COLLABORATIONS ARE EQUITABLE) Aim for each person to contribute a fair and equitable amount of effort and/or time to the group’s deliverables.\n(COLLABORATIONS RESOLVE CONFLICT QUICKLY) Resolve any misunderstands between the team members quickly. Don’t let those misunderstandings fester into distrust, resentment, or anger. Don’t be afraid to ask your professor for help in resolving interpersonal conflict in your team. While this can feel uncomfortable, often these kinds of situations are important opportunities for everyone to learn more about how to coexist as collaborative, whole humans."
  },
  {
    "objectID": "collaboration.html#collaboration-is-a-skill",
    "href": "collaboration.html#collaboration-is-a-skill",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration is a Skill",
    "text": "Collaboration is a Skill\nYou might imagine that you already know whether you need to collaborate and how to do it. And indeed, there’s a lot you know already! But collaboration is a skill, and like other skills it rewards practice and growth. Effective collaboration involves perspective-taking, empathy, respect, and clear communication. We hope that you will find that the benefits of collaboration far outweigh its challenges."
  },
  {
    "objectID": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "href": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration and the Middlebury Honor Code",
    "text": "Collaboration and the Middlebury Honor Code\nThe Middlebury Honor Code’s preamble states that:\n\nThe students of Middlebury College believe that individual undergraduates must assume responsibility for their own integrity on all assigned academic work…The Middlebury student body, then, declares its commitment to an honor system that fosters moral growth and to a code that will not tolerate academic dishonesty in the College community.\n\nIn any assignment in which you receive a grade individually (homeworks, exams), the purpose of the grade is to measure your learning and achievement. When you turn in such an assignment, you implicitly represent that work as work that you are able to complete yourself under the stated conditions (which may include getting help or working with others). If you cannot complete some work under the stated collaboration conditions, it is dishonest to turn in that work.\nWhen working individually, it is your responsibility to uphold the Code’s standards of integrity and academic honesty. When working in a group, it is additionally your responsibility to ensure that your group as a whole upholds these standards.\nIf you have a question about whether some form of collaboration is permitted, just ask!\n\nWhat Happens if I Observe an Honor Violation?\n\nWe all fail to uphold our highest moral aspirations at times. If you show lack of integrity or academic honesty, that doesn’t mean you’re a bad person. It means that you’re under pressure and chose the course of action that looked like the most workable one to you at the time.\nThat said, if you show lack of integrity or academic honesty, that’s an indicator that you have an opportunity for some very important growth.\nIt is part of my job to help you achieve that growth. I take this part of my job very seriously. In order to help you on your journey, I will connect both of us with the Middlebury Community Standards Office. Office leadership will help us all find a path that helps you grow toward integrity and honesty.\nThis is an awkward and uncomfortable process for everyone involved. You don’t want this."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software and Setup",
    "section": "",
    "text": "Bell Luo ’24 contributed to these notes.\nAfter following this set of instructions, you will be all ready to go for participation in CSCI 0451.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "software.html#clone-your-blog",
    "href": "software.html#clone-your-blog",
    "title": "Software and Setup",
    "section": "Clone your blog",
    "text": "Clone your blog\nFinally, clone your blog repository to your local computer in a place where you’ll be able to find it in the future. You can do this using the big green “Clone” button on GitHub. You can clone either using VSCode, GitHub Desktop, or at the command line with git or gh: all options are good!"
  },
  {
    "objectID": "software.html#test-drive-quarto",
    "href": "software.html#test-drive-quarto",
    "title": "Software and Setup",
    "section": "Test Drive Quarto",
    "text": "Test Drive Quarto\nChange modify the About page of your blog by modifying the file about.qmd. You can do things like change text or change the profile picture (it doesn’t have to be of yourself). Once you’ve made these changes, open a terminal in the location of your cloned blog and type the command\nquarto preview\nAfter a few moments, a web browser window should pop up with a preview of your blog. If you navigate over to the About tab, you should see your changes."
  },
  {
    "objectID": "software.html#finalize-and-publish",
    "href": "software.html#finalize-and-publish",
    "title": "Software and Setup",
    "section": "Finalize and Publish",
    "text": "Finalize and Publish\nIn the terminal, use Ctrl + C to stop the preview process. Then type the command\nquarto render\nThis time you won’t see a preview, but that’s ok! Quarto rendered our code into files under the docs/ folder. So now, all we have to do is publish that on GitHub.\nOver in git, or GitHub Desktop, or inside VSCode, you should be able to see all the new and modified files that have been generated.\n\n\n\n\n\n\nThe Internet Remembers\n\n\n\nYou are about to publish to the Internet.\nOnce you publish something, it is virtually impossible to un-publish.\nSo pause for a moment, think, and check that you are not accidentally uploading your password, or your embarrassing childhood photo.\n\n\nStage these files (git add), add a short commit message, and commit them to the main branch.\nThen, push your commit to GitHub.\nThis sends your files back to GitHub.com (the “remote”), where it will immediately be published online.\nAfter a minute or two, navigate back over to the URL housing your website and check that your changes have been made."
  },
  {
    "objectID": "software.html#optional-the-vscode-quarto-extension",
    "href": "software.html#optional-the-vscode-quarto-extension",
    "title": "Software and Setup",
    "section": "Optional: The VSCode Quarto Extension",
    "text": "Optional: The VSCode Quarto Extension\nThere is a VSCode extension for working with Quarto documents which I highly recommend. Find it here. Here is a demo using this extension."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective.\n© Phil Chodrow, 2024"
  },
  {
    "objectID": "project.html#project-description",
    "href": "project.html#project-description",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective."
  },
  {
    "objectID": "project.html#what-makes-a-good-project",
    "href": "project.html#what-makes-a-good-project",
    "title": "Course Project",
    "section": "What Makes a Good Project?",
    "text": "What Makes a Good Project?\n\nBig Picture\nThere’s a lot more detail on this topic below, but there are two simple questions that you should ask yourselves when envisioning your project:\n\nWill I learn something by completing this project?  Will I be proud of this project once it’s done?\n\nIf the answer to both questions is “yes,” then your overall project idea is likely good. Feel free to approach me early if you want to talk over whether your project idea is suitable for the course.\n\n\nBoring Projects\nThere is a kind of machine learning project that I feel is very boring and doesn’t really teach all that much. I’ll call these “Kaggle-style” projects. A “Kaggle-style” project is a project that starts with a convenient, clean data set and ends with a test score.  KS projects don’t clean or explore the data; don’t implement new algorithms; and don’t think carefully about why one algorithm might be better than another for the data in question. Instead, they simply try a bunch of things and assess them with a validation or test score.  I will probably not be impressed with a Kaggle-style project, partly because these kinds of projects really only address the “Navigation” learning objective.The website Kaggle is famous for hosting machine learning competitions in which the goal is to train a model that achieves the best prediction score on test data.I am being a little unfair; some Kaggle submissions are of exceptionally high quality.\nIt’s fine (indeed, encouraged!) for you to find some data that interests you and apply machine learning methods to it in order to make predictions or understand the structure of the data. To deepen your project beyond “Kaggle style,” you can incorporate some or all of the following:\n\nWork with data that is messy and needs significant processing before it can be used for ML tasks.\nDesign a custom vectorization scheme (i.e. way of representing each data points as a vector), or experiment with several pre-implemented schemes.\nConstruct multiple visualizations of your data that highlight patterns you wish to model or questions that you wish to explore.\nConduct a careful audit of your model to understand whether it performs better in some situations than others.\n\n\n\nCritical Discussion\nOne thing that should be incorporated into both your proposal and your project writeup is a critical discussion of incentives and impacts in your model.\nIncorporate a critical discussion of incentives and impacts in your work.\n\nIf someone were paying you to develop this model, who would be paying and why? Why might someone want this model to be built? Are you comfortable with that?\nWho are the users of your work? Who could be affected by your work? Are these populations the same?\nAre there risks of substantial bias or harm associated with the work you produce?"
  },
  {
    "objectID": "project.html#ideas",
    "href": "project.html#ideas",
    "title": "Course Project",
    "section": "Ideas",
    "text": "Ideas\nHere are some suggestions for choosing project directions. It’s fine for your project to be something entirely different—indeed, I encourage it! The primary benefit of projects that I come up with is that they are more likely to “work.” The primary drawback is that they may not be what you’re interested in! Even projects that don’t fully meet their stated objectives can still be successful experiences that demonstrate learning for the course.\n\nTheory and Implementation\nIf you enjoy thinking about math and how to translate math into performant numerical code, then you may wish to consider implementing a machine learning algorithm that we haven’t implemented in blog posts. There are many candidates, and it can be partially up to you to decide. Since the project is bigger than a blog post…\n\n…Your implementation should likely be of a more complex algorithm than ones we implemented in blog posts already. Alternatively, you could implement several related algorithms.\n…You will likely need to perform more complicated experiments in order to demonstrate the performance of your implementation.\n\nBecause these projects involve theoretical content that we haven’t covered in class, it is a good idea to talk with me before committing to one of them.\n\nSupport Vector Machine\nSupport vector machines (SVMs) were among the state-of-the-art binary classifiers before the rise of deep neural networks. Support vector machines are convex linear classifiers like logistic regression, but have some special mathematical properties that enable them to make much faster predictions on new data by leveraging sparsity. A good SVM project would likely involve most or all of the following:\n\nImplementing SVM using a version of stochastic gradient descent called PEGASOS (Fig. 2 in (shalev2007pegasos?)).\nImplementing kernel SVM, which enables nonlinear decision boundaries, using a quadratic solver.\nTesting your results on several real and synthetic data sets for:\n\nRuntime of training.\nRuntime of prediction.\nAccuracy of prediction.\n\n\n\n\nFaster Gradient Descent\nGradient descent and its relatives apply to a wide range of machine learning algorithms. Gradient descent comes in many flavors:\n\nStochastic\nMomentum\nAccelerated\nPrimal-dual methods\nModified gradient methods for nonconvex problems\nNewton methods\n\netc. etc. One good project could be to implement several of these methods for one or more ML algorithms, comparing the runtime of the training step on a variety of data sets.\n\n\nOther\nI can throw you lots of other theoretical/implementation problems if you’re interested—come chat and we can discuss."
  },
  {
    "objectID": "project.html#applied-analysis-projects",
    "href": "project.html#applied-analysis-projects",
    "title": "Course Project",
    "section": "Applied Analysis Projects",
    "text": "Applied Analysis Projects\nYou may wish to take machine learning methods that we discussed in class and apply them to data about some topic that you care about. Some great project interests that I’ve heard include remote sensing, sports analytics, genome analysis, and text analysis/language modeling."
  },
  {
    "objectID": "project.html#audits-and-algorithmic-bias",
    "href": "project.html#audits-and-algorithmic-bias",
    "title": "Course Project",
    "section": "Audits and Algorithmic Bias",
    "text": "Audits and Algorithmic Bias\nMany of you may have an interest in further exploring topics related to fairness and bias in machine learning algorithms. This is a great topic! Some possibilities include:\n\nThoroughly reproducing the findings of papers that diagnose algorithmic bias with an available data set or algorithm, such as (obermeyer2019dissecting?).\nWrite a critical review essay on the topic of algorithmic bias in a specific area. This should be a polished essay with lots of references and some specific, quantitative examples."
  }
]