---
title: "Optimization with Adam"
type: "Blog Post"
date: 2023-04-19
description: |
    In this blog post, you'll implement the Adam optimizer and perform some simple experiments. 
objectives: 
  - Theory
  - Experimentation
jupyter: conda-env-ml-0451-py
number-sections: true
number-depth: 2
publish: "true"
bibliography: ../../refs.bib
---


::: {.callout-note}

This is a relatively theoretical blog post on which I am intentionally giving you relatively little guidance. The purpose is to offer a challenge for students who have already done most of the previous blog posts. If you have already completed most of the previous posts, then I encourage you to give this one a try! Otherwise, I suggest that you instead focus on completing some of the prior posts. 

:::

The Adam optimization algorithm is a mainstay of modern deep learning. You can think of Adam as *fancy gradient descent*. It still uses gradient information, but processes that information in a more complex way that often produces state-of-the-art performance in modern large-scale tasks. 

Adam was introduced by @kingma2014adam, in a paper which has been cited over 140,000 times. 

## What You Should Do

### Read the Adam Paper

Start by reading [the paper that introduced Adam](https://arxiv.org/pdf/1412.6980.pdf%5D). You can focus on sections 1, 2, 3, and 6.1, skipping the others. It's ok for your first reading to be relatively quick; you'll want to go back through the sections multiple times as you do your implementation and experiments. 

### Implement Adam for Logistic Regression

Expand your [implementation of logistic regression](blog-post-optimization.qmd) to include a version of the `fit` method that uses the Adam algorithm (Algorithm 1). If you implemented `gradient` as a separate function, you can just use that function within your implementation of Adam. 

The authors of the paper frame their algorithm as minimizing a *stochastic* (random) *objective* function $f$. This is a fancy mathematical way to talk about *stochastic batch gradient descent*. When they talk about evaluating/differentiating the "stochastic function" $f$, you can instead think of "evaluating/differentiating $f$ on a subset of the data," just like we did in gradient descent for logistic regression. So, you can think of $g_t$ in their algorithm as the gradient evaluated on a batch of the data. You can still follow the standard structure of stochastic gradient descent in which you loop through the entire data set in one epoch, reshuffle the batches, and do it again. 

Your implementation should allow the user to pass five arguments: 

1. `batch_size`, the batch size for computing gradients. This works in the same way as it did in our initial implementation of stochastic gradient descent, and you can use much of the same code. 
2. `alpha`, the step-size. 
3. `beta_1` and `beta_2`, the moment estimate decay rates. 
4. `w_0`, the initial guess for the weight vector. Personally I would suggest giving this argument a default `None` value and, in the case that the user does not pass `w_0`, initialize it randomly with the correct dimensions. 

### Perform a Basic Experiment

Redo some of the simple experiments from [implementation of logistic regression](blog-post-optimization.qmd). Compare Adam optimization to standard stochastic gradient descent with a few different parameter choices. Please measure **both** the number of epochs and the actual amount of time required to achieve convergence. 

### Perform a Digits Experiment

[We saw an example of loading and manipulating the digits data set in the [reading on k-means]((https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)).]{.aside}

Load the `digits` dataset from `scikit-learn`. Then, do one of two things: 

1. Filter the digits data set so that it only contains data with two class labels (e.g. 4s vs. 8s). 
2. **Optional challenge**: extend your implementation of logistic regression to handle multiple class labels. *Doing this by hand is quite challenging and I only recommend it if you are feeling very ambitious. You might be able to get some inspiration from [this implementation](https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372), though please don't copy code and acknowledge the author if you find her example helpful*. 

Then, do a version of the experiment in Section 6.1 of the Adam paper in which you again compare the efficiency of your Adam implementation against standard stochastic gradient descent in terms of both epochs and elapsed time. 

### Perform One More Experiment

Find a data set (any data set that interests you) on which you can perform  classification with your Adam model. I recommend that the number of features not be *very* large (maybe no larger than 100). Then, again compare the performance of Adam to standard stochastic gradient descent on the data you found. 

### Write Your Post

In your written blog post, please: 

1. Show your Adam implementation. Include comments corresponding to the comments in Alg. 1 of the paper so that the reader understands which lines of code correspond to which lines of math. 
2. Describe and discuss the findings from your experiments. 
3. Don't forget to label your axes!! 
4. Include an introductory section describing the purpose of the blog post and a summary section reflecting on your findings. In your summary paragraph, please include answers to the following questions: 
    - In the authors' assessment, what aspects of the Adam algorithm help it to converge so efficiently? (You don't need to go deep into the math but you should be able to identify a qualitative features from their paper). 
    - Did you observe highly efficient performance in your experiments when compared to other methods? If not, can you offer a hypothesis about why? 
    - How did the experience of implementing this algorithm compare to the experience of implementing standard stochastic gradient descent? 

Finally, don't forget to label your axes!! 



