---
title: "Kernel Logistic Regression"
type: "Blog Post"
date: 2023-03-08
description: |
    In this blog post, you'll implement kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries. 
objectives: 
  - Theory
  - Implementation
  - Navigation
  - Experimentation
jupyter: conda-env-ml-0451-py
number-sections: true
number-depth: 2
publish: "false"
---

::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vkappa}{\boldsymbol{\kappa}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

:::

::: {.callout-important}

This is one of two *suggested options* for a blog post this week. You might want to pick this option if some of the following bullet points describe you: 

1. You enjoy working with matrices and vectors in `numpy`. 
2. You like math and theoretical aspects of machine learning algorithms. 
3. You are willing to read a little extra theory before starting on your blog post. 

[The alternative](blog-post-penguins.qmd) has a more applied flavor. 

:::


# Introduction

In this blog post you'll implement and test *kernel logistic regression* for binary classification. Kernel logistic regression is one of many *kernelized linear classifiers.*  

In regular logistic regression, we aim to solve the empirical risk minimization problem 

$$
\hat{\vw} = \argmin_{\vw} \; L(\vw)\;, 
$$
where 
$$
L(\vw) = \frac{1}{n} \sum_{i = 1}^n \ell(\bracket{\vw, \vx_i}, y_i)\; 
$$
is the empirical risk and 
$$
\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;,
$$
is the logistic loss. Logistic regression is an outstanding algorithm for linear classification, but it can only handle *linear decision boundaries*. Here's an example of a data set that has a clear *nonlinear* pattern that we'd like to learn: 

```{python}
from sklearn.datasets import make_moons, make_circles
from matplotlib import pyplot as plt
import numpy as np
np.seterr(all="ignore")


X, y = make_moons(200, shuffle = True, noise = 0.2)
plt.scatter(X[:,0], X[:,1], c = y)
labels = plt.gca().set(xlabel = "Feature 1", ylabel = "Feature 2")
```

[Follow the instructions [here](http://rasbt.github.io/mlxtend/installation/) to install the `mlxtend` package.]{.aside}

A linear separator wouldn't do great on this data set. To see the best we can do, let's use pre-implemented versions of logistic regression and a visualization tool: 


```{python}
from sklearn.linear_model import LogisticRegression
from mlxtend.plotting import plot_decision_regions

LR = LogisticRegression()

LR.fit(X, y)

plot_decision_regions(X, y, clf = LR)

title = plt.gca().set(title = f"Accuracy = {(LR.predict(X) == y).mean()}",
                      xlabel = "Feature 1", 
                      ylabel = "Feature 2")
```

Our classifier does better than random chance, but it looks like we could do significantly better if we were able to learn the "curvy shape" of the data. Here's an example using *kernel logistic regression*, which you will implement in this assignment. 

```{python}
#| code-fold: false

from solutions.kernel_logistic import KernelLogisticRegression
from sklearn.metrics.pairwise import rbf_kernel

KLR = KernelLogisticRegression(rbf_kernel, gamma = .1)
KLR.fit(X, y)
plot_decision_regions(X, y, clf = KLR)
title = plt.gca().set(title = f"Accuracy = {(KLR.predict(X) == y).mean()}",
                      xlabel = "Feature 1", 
                      ylabel = "Feature 2")
```

## Kernel Logistic Regression

In the kernel logistic regression problem, we instead solve empirical risk minimization with modified features. The empirical risk is now

$$
L_k(\vv) = \frac{1}{n} \sum_{i = 1}^n \ell(\bracket{\vv, \vkappa(\vx_i)}, y_i)\;, 
$${#eq-empirical-risk-kernelized}

where $\vv \in \R^n$ (**not ** $\R^p$). The modified feature vector $\vkappa(\vx_i)$ has entries 

$$
\vkappa(\vx_i) = \left( \begin{matrix}
    k(\vx_1, \vx_i) \\ 
    k(\vx_2, \vx_i) \\ 
    \vdots \\ 
    k(\vx_n, \vx_i)
\end{matrix}\right)\;. 
$$

Here, $k:\R^2 \rightarrow \R$ is the *kernel function*. Kernel functions need to satisfy some special mathematical properties. We're not going to code them up; instead we're going to use some built-in functions from `scikit-learn` to handle the kernel functions for us. 

Once the model has been trained and an optimal $\hat{\vv}$ has been obtained, one can then make a prediction using the formula 

$$
\hat{y} = \bracket{\hat{\vv}, \vkappa (\vx)}\;.
$$

If it is desired to return a 0-1 label instead of a real number, one can return $\mathbb{1}[\hat{y} > 0]$. 

# What You Should Do

## Implement Kernel Logistic Regression

Implement a Python class called `KernelLogisticRegression`. You'll be able to use it like the example in the previous section. Your class should implement the following methods: 

[If you're not sure how to use `**kwargs` in Python functions and methods, you might want to check [this resource](https://realpython.com/python-kwargs-and-args/#using-the-python-kwargs-variable-in-function-definitions).]{.aside}



- `__init__(self, kernel, **kernel_kwargs)` should accept a `kernel` function and a set of named keyword arguments called `kernel_kwargs`. All the `__init__()` method should do is to save these items as instance variables called 
    - `self.kernel`
    - `self.kernel_kwargs`
- `fit(self, X, y)` will again be the method that learns the optimal parameters $\hat{v}$. The fit method is going to look a little different this time: 
    - First, `pad` `X` to make sure that `X` contains a column of `1`s.  Here's a function to do this: 
    ```python
        def pad(X):
            return np.append(X, np.ones((X.shape[0], 1)), 1)
    ```
    - ***Save*** `X` as an instance variable called `self.X_train`. 
    - Compute the *kernel matrix* of `X` with itself. If you implemented `__init__()` correct, this can be done with the call 
    ```python
    km = self.kernel(X_, X_, **self.kernel_kwargs)
    ```
    - Minimize the empirical risk @eq-empirical-risk-kernelized. You might find it useful to define a separate function for computing the empirical risk. Note that the predictor is *still* an inner product, just with a different parameter vector $\vv$ and a different matrix column $\vkappa(\vx_i)$. This means that, if you're careful, you can compute the entire empirical risk using just one matrix multiplication! 
        - However you find it, save the resulting optimal value of $\vv$ as `self.v`. 
        - You should still use the logistic loss for $\ell$. 
        - You will probably need to choose a random initial $\vv$. Don't forget that $\vv$ should have *length equal to the number of data points*, *not the number of features*. 
    - If you've already implemented gradient descent for logistic regression in [this blog post](blog-post-optimization.qmd), then it's not too hard to adapt your method to kernel logistic regression. However, it's also fine to use the function `scipy.optimize.minimize()` as demonstrated in [this lecture](../../lecture-notes/convex-linear-models.ipynb). 
- `predict(self, X)` should accept a new feature matrix and return binary labels $\{0,1\}$. For each row of $\mX$, the prediction is obtained using the formula $\mathbb{1}[\bracket{\hat{\vv}, \vkappa(\vx)}]$. To do this: 
    - Compute the kernel matrix between `self.X_train` and the new feature input `X`. Each column of this matrix is $\vkappa(\vx_j)$ for some $j$. 
    - Compute inner products of the form $\bracket{\vv, \vkappa(\vx_j)}$. If the user supplies a matrix `X` with multiple columns, you should be able to compute all the predictions at once. This can be done efficiently using matrix multiplication. 
    - Finally, return a binary vector $\hat{\vy}$ whose $j$th entry is $\hat{y}_j = \mathbb{1}[\bracket{\vv, \vx_j} > 0]$. 
- `score(self, X, y)` computes the accuracy of the model predictions on the feature matrix `X` with labels `y`. 

You can assume that the user will always only call `predict` and `score` after calling `fit`. If you'd like, you're welcome to add warnings or handle other cases in which the user may be less cooperative and attempt to call one of those methods first. 

My complete implementation of kernel logistic regression was about 50 lines of code, excluding comments. 

**Docstrings are not expected** for this blog post. 

## Experiments

### Basic Check

Once you're done, you'll be able to import and and use your function like this. 

```python
from kernel_logistic import KernelLogisticRegression # your source code
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.datasets import make_moons, make_circles

X, y = make_moons(200, shuffle = True, noise = 0.2)
KLR = KernelLogisticRegression(rbf_kernel, gamma = .1)
KLR.fit(X, y)
print(KLR.score(X, y))
```

Here, the `rbf_kernel` is the kernel function and `gamma` is a parameter to that kernel function that says how "wiggly" the decision boundary should be. Larger `gamma` means a more wiggly decision boundary. 

Your implementation is likely correct when you can generate new synthetic versions of the data set above (just call `make_moons` again) and achieve accuracy consistently at or above 90%. To check that, you can just run the code block above a few times. 

### Choosing `gamma`

When we choose a very large value of `gamma`, we can achieve a very wiggly decision boundary with very good accuracy on the training data. For example:  

```{python}
KLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)
KLR.fit(X, y)
print(KLR.score(X, y))
plot_decision_regions(X, y, clf = KLR)
t = title = plt.gca().set(title = f"Accuracy = {KLR.score(X, y)}",
                      xlabel = "Feature 1", 
                      ylabel = "Feature 2")
```

Here, our classifier draws a little orange blob around each orange data point: points very nearby are classified as orange while other points are classified as blue. This is sufficient to achieve 100% accuracy on the training data. But this doesn't *generalize*: generate some new data and we'll see much worse performance: 

```{python}
# new data with the same rough pattern
X, y = make_moons(200, shuffle = True, noise = 0.2)
plot_decision_regions(X, y, clf = KLR)
title = plt.gca().set(title = f"Accuracy = {KLR.score(X, y)}",
                      xlabel = "Feature 1", 
                      ylabel = "Feature 2")
```

Whoops! Not so good. We say that the *validation* or *testing* accuracy of the classifier is quite low. Cases in which the validation accuracy is low even though the training accuracy is high are classic instances of overfitting. 

[My suggestion is to choose `gamma in 10**np.arange(-5, 6)`]{.aside}
Design an experiment in which you fit your model for several different values of `gamma`. Show accuracy on both training data (the data on which the model was `fit`) and testing data (data generated from the same settings but which the model has never seen before). Please show your findings in the form of an attractive visualization with clear labels and a clear message. 

### Vary the Noise

Repeat your experiment with at least two other values of the `noise` parameter to `make_moons`. The noise determines how spread out the two crescents of points are. Do your findings suggest that the best value of `gamma` depends much on the amount of noise? 

### Try Other Problem Geometries

Use the [`make_circles`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles) function to generate some concentric circles instead of crescents. Show a few examples with varying amounts of noise. Can you find some values of `gamma` that look like they lead to good learning performance for this data set? Here's an example of a fairly successful classifier: both the points and the accuracy are computed on unseen test data. 

```{python}
#| echo: false
X, y = make_circles(200, noise = 0.05)
KLR = KernelLogisticRegression(rbf_kernel, gamma = .1)
KLR.fit(X, y)
print(KLR.score(X, y))
X, y = make_circles(200, noise = 0.05)
plot_decision_regions(X, y, clf = KLR)
t = title = plt.gca().set(title = f"Accuracy = {KLR.score(X, y)}",
                      xlabel = "Feature 1", 
                      ylabel = "Feature 2")
```

## Blog Post

Your blog post should describe your approach to your code and written descriptions of your experiments. 

- **Please include a walk-through for your user of how you computed the empirical loss.** 
- Please make sure that your figures are appropriately labeled and described. 
- Please make sure to include a link to the GitHub page containing your source code at the very beginning of the blog post. 

In case you're curious, it's possible to add [formal captions](https://quarto.org/docs/authoring/figures.html) to your figures in Quarto. This makes things look a little fancier, but is not required! 

Once you're happy with how things look, render your blog, push it to GitHub, and submit a link to the URL of your blog post on Canvas. 
