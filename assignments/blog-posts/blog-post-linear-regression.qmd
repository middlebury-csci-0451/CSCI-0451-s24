---
title: "Implementing Linear Regression"
type: "Blog Post"
date: 2023-03-15
description: |
    In this blog post, you'll implement least-squares linear regression, and experiment with LASSO regularization for overparameterized problems. 
objectives: 
  - Theory
  - Implementation
  - Experimentation
  - Navigation
jupyter: conda-env-ml-0451-py
number-sections: true
number-depth: 2
publish: "false"
---

::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mP}{\mathbf{P}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\vkappa}{\boldsymbol{\kappa}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

::: 

## Implement Linear Regression Two Ways

To start this blog post, please implement least-squares linear regression in *two ways*. 

1. First, use the analytical formula for the optimal weight vector $\hat{\vw}$ from [the lecture notes](../../lecture-notes/regression.qmd). This formula requires matrix inversion and several matrix multiplications. 
2. Next, use the formula for the *gradient* of the loss function to implement gradient descent for linear regression. You can still pass in `max_iter` and `alpha` (the learning rate) as parameters to the `fit` method. [Implementing *stochastic* gradient descent would be nice thing to do but not required. Only if you want to go above and beyond!]{.aside}

In addition to the `fit` method, your implementation should include a `score` method (see below) and a `predict` method (just return $\mX\vw$). 

It's fine for you to either define separate methods like `fit_analytic` and `fit_gradient` for these methods. It's also fine to define a single `fit` method with a `method` argument to determine which algorithm is used. 

As usual, place your implementation in a source file where you will be able to implement it. 

### The Score

Let $\bar{y} = \frac{1}{n} \sum_{i = 1}^ny_i$. Then, the *score* we'll use is the so-called *coefficient of determination*, which is 

$$
c = 1 - \frac{\sum_{i = 1}^n (\hat{y}_i - y_i)^2}{\sum_{i = 1}^n (\bar{y} - y_i)^2}\;.
$$


The coefficient of determination is always no larger than 1, with a higher value indicating better predictive performance. It can be arbitrarily negative for very bad models. Note that the numerator in the fraction is just $L(\vw)$, so making the loss small makes the coefficient of determination large. 

### Efficient Gradient Descent

For gradient descent, please implement a `score_history` so that you can visualize the value of the `score` over epochs. 

The formula for the gradient is 
$$
\nabla L(\vw) = 2\mX^T(\mX\vw - \vy)\;. 
$$
However, you should resist the urge to compute this formula "from scratch" at every iteration. The reason is that the matrix multiplication $\mX^T\mX$ has time-complexity $O(np^2)$, where $n$ is the number of data points and $p$ is the number of features. Similarly, the matrix-vector product $\mX^T\vy$ has time-complexity $O(np)$. Both of these can be pretty expensive if you have a lot of data points! Fortunately, *they don't depend on the current value of $w$*, so you can actually just precompute them: 

1. **Once** during the `fit` method, compute $\mP = \mX^T \mX$ and $\vq = \mX^T \vy$. 
2. The gradient is then $\nabla L(\vw) = 2(\mP\vw - \vq)$.

Computing $\mP \vw$ requires only $O(p^2)$ steps. In other words, precomputing $\mP$ and $\vq$ eliminates the dependence of the runtime on the number of data points -- not bad! 

## Demo

The following function will create both testing and validation data that you can use to test your implementation: 

```{python}
import numpy as np
from matplotlib import pyplot as plt

def pad(X):
    return np.append(X, np.ones((X.shape[0], 1)), 1)

def LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):
    if w is None: 
        w = np.random.rand(p_features + 1) + .2
    
    X_train = np.random.rand(n_train, p_features)
    y_train = pad(X_train)@w + noise*np.random.randn(n_train)

    X_val = np.random.rand(n_val, p_features)
    y_val = pad(X_val)@w + noise*np.random.randn(n_val)
    
    return X_train, y_train, X_val, y_val
```

Here's an example of how to use the function to generate data. Unfortunately, it's only possible to easily visualize this problem when `p_features = 1`. 

```{python}
n_train = 100
n_val = 100
p_features = 1
noise = 0.2

# create some data
X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)

# plot it
fig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)
axarr[0].scatter(X_train, y_train)
axarr[1].scatter(X_val, y_val)
labs = axarr[0].set(title = "Training", xlabel = "x", ylabel = "y")
labs = axarr[1].set(title = "Validation", xlabel = "x")
plt.tight_layout()
```

Once you've impmlemented your solution, you should be able to use it like this: 

```{python}
from solutions.linear_regression import LinearRegression

LR = LinearRegression()
LR.fit(X_train, y_train) # I used the analytical formula as my default fit method

print(f"Training score = {LR.score(X_train, y_train).round(4)}")
print(f"Validation score = {LR.score(X_val, y_val).round(4)}")
```

The estimated weight vector $\vw$ is 

```{python}
LR.w
```

I can get the same value for $\vw$ using gradient descent (it would be even closer if we allowed more iterations).  

```{python}
LR2 = LinearRegression()

LR2.fit(X_train, y_train, method = "gradient", alpha = 0.01, max_iter = 1e2)
LR2.w
```

I can also see how the score changed over time. Because we're not using stochastic gradient descent, the score should increase monotonically in each iteration.  

```{python}
plt.plot(LR2.score_history)
labels = plt.gca().set(xlabel = "Iteration", ylabel = "Score")
```

Your implementation is likely correct when you are able to reproduce results that are similar to these (although small differences are no problem). 

## Experiments

Once you've demonstrated the behavior above, perform an experiment in which you allow `p_features`, the number of features used, to increase, while holding `n_train`, the number of training points, constant. Try increasing `p_features` all the way to `n_train - 1`. What happens to the *training score*? What happens to the *validation* score? I'd suggest showing these results on a nice plot in which the horizontal axis is the number of features, the vertical axis is the score, and the training/validation scores are shown in different colors. 

[Optional: Relate your findings when `p_features = n_train-1` to the existence of a solution of the equation $\mX\vw = \vy$. What do you know about the rank of $\mX$? Remember that the number of columns in $\mX$ is actually `p_features + 1`, since it's still necessary to pad with a column of 1s.]{.aside}

When discussing your findings, make sure to connect them to the idea of overfitting. 

## LASSO Regularization

The LASSO algorithm uses a modified loss function with a regularization term: 

$$
L(\vw) = \norm{\mX \vw - \vy}_2^2 + \alpha \norm{\vw'}_1\;.
$$

Here, $\vw'$ is the vector composed of all the entries of $\vw$ *excluding the very last entry*. The 1-norm is defined as 

$$
\norm{\vw'}_1 = \sum_{j = 1}^{p-1} \abs{w_j}\;.
$$

The effect of the regularizing term is to make the entries of the weight vector $\vw$ small. In fact, LASSO has a nice property: it tends to force entries of the weight vector to be *exactly zero*. [The reason we exclude the final entry of $\vw$ is that it is not desirable to penalize the weight corresponding to the constant feature in $\mX$.]{.aside} This is very desirable in so-called *overparameterized* problems, when the number of features $p$ is larger than the number of data points $n$. 

Implementing LASSO involves some more complicated mathematical optimization than we will discuss in this course, so instead we'll use the implementation in `scikit-learn`. You can import it like this: 

```{python}
from sklearn.linear_model import Lasso
L = Lasso(alpha = 0.001)
```

Here, `alpha` controls the strength of the regularization (it's not related to the learning rate in gradient descent). Let's fit this model on some data and check the coefficients: 

```{python}
p_features = n_train - 1
X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)
L.fit(X_train, y_train)
```

The score on the validation set is high, which might be different from what you found with pure linear regression. 
```{python}
L.score(X_val, y_val)
```

### What You Should Do

Replicate the same experiment you did with linear regression, increasing the number of features up to or even *past* `n_train - 1`, using LASSO instead of linear regression. You might want to experiment with a few values of the regularization strength `alpha`. Comment on how your validation score compares to standard linear regression when the number of features used is large. 

## Optional: Bikeshare Data Set

The following code will download and save a data set related to the Capital Bikeshare system in Washington DC. We use the aggregated version graciously provided by the authors of the following paper:

> Fanaee-T, Hadi, and Gama, Joao, "Event labeling combining ensemble detectors and background knowledge", Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.

This data set includes information about the season and time of year; the weather; and the count of bicycle users on each day for two years (year 0 is 2011, year 1 is 2012). This level of information gives us considerable ability to model phenomena in the data.

[For more on what the entries in each column means, you can consult the data [dictionary here](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) (under "Attribute Information").]{.aside}

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
bikeshare = pd.read_csv("https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv")

bikeshare.head()
```


Our aim for this case study is to plot daily usage by *casual* users (as opposed to registered users). The total number of casual users each day is given by the `casual` column, Let's plot this over time: 

```{python}
# import datetime
fig, ax = plt.subplots(1, figsize = (7, 3))
ax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])
ax.set(xlabel = "Day", ylabel = "# of casual users")
l = plt.tight_layout()
```

For this prediction task, it's handy to work with a smaller subset of the columns, and to transform the `mnth` column into dummy variables. 

```{python}
cols = ["casual", 
        "mnth", 
        "weathersit", 
        "workingday",
        "yr",
        "temp", 
        "hum", 
        "windspeed",
        "holiday"]

bikeshare = bikeshare[cols]

bikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = "if_binary")
bikeshare
```

Now we can do a train-test split. 
```{python}
train, test = train_test_split(bikeshare, test_size = .2, shuffle = False)

X_train = train.drop(["casual"], axis = 1)
y_train = train["casual"]

X_test = test.drop(["casual"], axis = 1)
y_test = test["casual"]
```

Train an instance of your `LinearRegression` class on the bikeshare training data. Then: 

- Score your model on the test set. 
- Compute the predictions for each day and visualize them in comparison to the actual ridership on the test set. 
- Compare the entries `w` of your model to the corresponding entry of `X_train.columns` in order to see which features your model found to contribute to ridership. Positive coefficients suggest that the corresponding feature contributes to ridership. Can you find effects corresponding to nice weather? Summer months? Holidays? Weekends? 

